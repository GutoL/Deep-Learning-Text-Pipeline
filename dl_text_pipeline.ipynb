{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6a9a7f",
   "metadata": {
    "heading_collapsed": true,
    "id": "ec6a9a7f"
   },
   "source": [
    "# Important Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11341e77",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "11341e77"
   },
   "source": [
    "### Install Pytorch on windows:\n",
    "https://saturncloud.io/blog/how-to-install-pytorch-on-windows-using-conda/\n",
    "\n",
    "### Preprocessing:\n",
    "https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "\n",
    "### XAI:\n",
    "https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/bert-explainable-ai/\n",
    "https://towardsdatascience.com/introducing-transformers-interpret-explainable-ai-for-transformers-890a403a9470\n",
    "https://github.com/cdpierse/transformers-interpret\n",
    "https://levelup.gitconnected.com/huggingface-transformers-interpretability-with-captum-28e4ff4df234\n",
    "https://silviatulli.com/2021/11/02/explaining-the-outputs-of-transformers-models-a-working-example/\n",
    "https://brainsteam.co.uk/2022/03/14/painless-explainability-for-text-models-with-eli5/#eli5-and-transformershuggingface\n",
    "\n",
    "### SBERT — Sentence-BERT:\n",
    "https://towardsdatascience.com/sbert-deb3d4aef8a4\n",
    "https://www.sbert.net/docs/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffcd69",
   "metadata": {
    "id": "02ffcd69"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mEhD3fW_35X_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:11:14.971414Z",
     "start_time": "2023-10-16T14:11:14.959828Z"
    },
    "id": "mEhD3fW_35X_"
   },
   "outputs": [],
   "source": [
    "# # install libraries\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate -U\n",
    "# !pip install -U xformers\n",
    "# !pip install datasets evaluate\n",
    "# !!pip install emoji\n",
    "# !pip install scikit-learn scipy matplotlib\n",
    "# !pip install openpyxl --upgrade\n",
    "# !pip install wordcloud\n",
    "# !pip install nltk\n",
    "# !pip install tweet-preprocessor\n",
    "# !pip install captum\n",
    "# !pip install transformers-interpret\n",
    "# !pip install eli5\n",
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd917d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:30:04.693488Z",
     "start_time": "2023-10-20T09:30:04.592484Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37096,
     "status": "ok",
     "timestamp": 1700128094997,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "81dd917d",
    "outputId": "b22b7784-84d8-406e-fec1-998cce71905f"
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# General\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import os\n",
    "from numpy.random import seed\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Modeling\n",
    "# import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, TextClassificationPipeline\n",
    "from transformers.pipelines import TextClassificationPipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "torch.set_flush_denormal(True)\n",
    "\n",
    "# XAI\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "from transformers_interpret import SequenceClassificationExplainer, MultiLabelClassificationExplainer\n",
    "# from eli5.lime import TextExplainer\n",
    "import eli5\n",
    "\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model performance evaluation\n",
    "import evaluate\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "print(plt.style.available)\n",
    "# plt.style.use(\"_classic_test_patch\")\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "# plt.style.use('seaborn-v0_8-talk')\n",
    "# plt.style.use(\"fivethirtyeight\")\n",
    "# plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3685f1b",
   "metadata": {
    "id": "a3685f1b"
   },
   "source": [
    "# Reading dataset\n",
    "Here you have to create a code to read your dataset as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4a852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:40:36.059199Z",
     "start_time": "2023-10-19T11:40:35.717007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 2943,
     "status": "ok",
     "timestamp": 1700128589083,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "e3a4a852",
    "outputId": "0557fced-4f09-4717-eb8e-edefc8432ab0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_type = 'racism'\n",
    "\n",
    "path = 'dataset/' # Local\n",
    "# path = 'drive/My Drive/hate_speech/datasets/manually_coded/' # Colab\n",
    "\n",
    "original_text_column = 'data_text'\n",
    "label_column = 'label'\n",
    "\n",
    "if dataset_type == 'racism':\n",
    "    dataset_file = 'Racism.xlsx'\n",
    "\n",
    "elif dataset_type == 'homophobic':\n",
    "    dataset_file = 'Homophobic.xlsx'\n",
    "\n",
    "elif dataset_type == 'feminine_slurs':\n",
    "    dataset_file = 'Feminine_Slurs.xlsx'\n",
    "\n",
    "elif dataset_type == 'sexist':\n",
    "    dataset_file = 'sexist.xlsx'\n",
    "\n",
    "elif dataset_type == 'ableist':\n",
    "    dataset_file = 'Ableist.xlsx'\n",
    "\n",
    "original_text_column = 'data_text'\n",
    "label_column = 'label'\n",
    "\n",
    "df_hate_speech = pd.read_excel(path+dataset_file)[original_text_column]\n",
    "\n",
    "df_non_hate_speech = pd.read_csv(path+'non_hate_speech.csv')[original_text_column]\n",
    "\n",
    "df_non_hate_speech = shuffle(df_non_hate_speech, random_state=42)\n",
    "df_non_hate_speech = df_non_hate_speech.head(df_hate_speech.shape[0])\n",
    "\n",
    "label = [1]*df_non_hate_speech.shape[0] + [0]*df_non_hate_speech.shape[0]\n",
    "\n",
    "hate_speech_df = pd.concat([df_hate_speech, df_non_hate_speech]).to_frame()\n",
    "\n",
    "hate_speech_df.columns = [original_text_column]\n",
    "\n",
    "text_column = original_text_column\n",
    "\n",
    "hate_speech_df[label_column] = label\n",
    "\n",
    "hate_speech_df.dropna(subset=[original_text_column], inplace=True)\n",
    "\n",
    "hate_speech_df = shuffle(hate_speech_df, random_state=42)\n",
    "\n",
    "hate_speech_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hate_speech_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27710b6",
   "metadata": {
    "id": "e27710b6"
   },
   "source": [
    "# Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7e776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:40:41.688746Z",
     "start_time": "2023-10-19T11:40:41.527999Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1700130729880,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "95d7e776",
    "outputId": "b13d06a9-d3e4-4d54-c3a0-7cd0ba295b56"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Create a set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, df, text_column, label_column, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.processed_text_column = None\n",
    "        self.label_column = label_column\n",
    "        self.number_of_labels = len(df[label_column].value_counts())\n",
    "\n",
    "\n",
    "    def __demojize_text(self, text):\n",
    "        return emoji.demojize(text)\n",
    "\n",
    "    def __remove_words_with_euro(self, input_string):\n",
    "        # Define a regular expression pattern to match words containing 'euro'\n",
    "        pattern = r'\\b\\w*#?euro\\w*\\b'\n",
    "        # Use re.sub to replace matching words with an empty string\n",
    "        result = re.sub(pattern, '', input_string)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __remove_stop_words(self, sentence):\n",
    "        # Split the sentence into individual words\n",
    "        words = sentence.split()\n",
    "        # Use a list comprehension to remove stop words\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        # Join the filtered words back into a sentence\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def __preprocess_sentence(self, text, setup):\n",
    "\n",
    "        if setup['lower_case']:\n",
    "            text = text.lower()\n",
    "\n",
    "        if setup['remove_emojis']:\n",
    "            text = self.__demojize_text(text)\n",
    "\n",
    "        if setup['remove_stop_words']:\n",
    "            text = self.__remove_stop_words(text)\n",
    "\n",
    "        if setup['remove_numbers']:\n",
    "            text = text.replace('\\d+', '') # Removing numbers\n",
    "\n",
    "        # text = p.clean(text) #heavy cleaning\n",
    "\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            # t = remove_words_with_euro(t)\n",
    "\n",
    "            if setup['remove_users']:\n",
    "                t = '' if t.startswith('@') and len(t) > 1 else t\n",
    "                # t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            if setup['remove_urls']:\n",
    "                t = '' if t.startswith('http') else t\n",
    "                # t = 'http' if t.startswith('http') else t\n",
    "\n",
    "            new_text.append(t)\n",
    "\n",
    "        new_text = \" \".join(new_text)\n",
    "\n",
    "        if setup['lemmatize']:\n",
    "            wnl = WordNetLemmatizer()\n",
    "            list2 = nltk.word_tokenize(new_text)\n",
    "            new_text = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    def get_text_column_name(self):\n",
    "        if self.processed_text_column:\n",
    "            return self.processed_text_column\n",
    "        else:\n",
    "            return self.text_column\n",
    "\n",
    "    def get_top_words(self, n=100):\n",
    "\n",
    "        temp_text_column = self.get_text_column_name()\n",
    "\n",
    "        # Combine all tweets into a single string\n",
    "        all_tweets = \" \".join(self.df[temp_text_column])\n",
    "\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(all_tweets)\n",
    "\n",
    "        # Remove stopwords and non-alphabetic words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "        # Calculate word frequencies\n",
    "        word_freq = Counter(words)\n",
    "\n",
    "        # Get the top n words\n",
    "        top_words_and_count = word_freq.most_common(n)\n",
    "        top_words = [word for word, counter in top_words_and_count]\n",
    "        counters = [counter for word, counter in top_words_and_count]\n",
    "\n",
    "        return {'words':top_words, 'counters':counters}\n",
    "\n",
    "\n",
    "    def get_top_words_tfidf(self, n):\n",
    "\n",
    "        temp_text_column = self.get_text_column_name()\n",
    "\n",
    "        # Create a TF-IDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=n, stop_words='english')\n",
    "\n",
    "        # Fit and transform the text data\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.df[temp_text_column])\n",
    "\n",
    "        # Get feature names (words)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Sum the TF-IDF scores for each word across all tweets\n",
    "        word_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "        # Sort words by their TF-IDF scores\n",
    "        top_indices = word_scores.argsort()[0, ::-1][:n]\n",
    "\n",
    "        # Get the top n words and their TF-IDF scores\n",
    "        top_words = [(feature_names[i], word_scores[0, i]) for i in top_indices]\n",
    "\n",
    "        return top_words[0][0][0]\n",
    "\n",
    "    def preprocess(self, setup):\n",
    "\n",
    "        self.df.dropna(subset=[self.text_column], inplace=True)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.processed_text_column = 'processed_'+self.text_column\n",
    "        self.df[self.processed_text_column] = self.df.apply(lambda x: self.__preprocess_sentence(x[self.text_column], setup), axis=1)\n",
    "\n",
    "        if setup['remove_non_text_characters']:\n",
    "            pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "            self.df[self.processed_text_column] = self.df.apply(lambda x: pattern.sub('', x[self.processed_text_column]), axis=1)\n",
    "\n",
    "        self.df[self.label_column] = self.df[self.label_column].astype(int)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def unsample(self):\n",
    "\n",
    "        # temp_text_column = self.get_text_column_name()\n",
    "        # columns = [temp_text_column, self.label_column]\n",
    "\n",
    "        columns = [self.text_column, self.processed_text_column, self.label_column]\n",
    "\n",
    "        processed_df_grouped = self.df[columns].groupby(self.label_column)\n",
    "        processed_df_grouped.groups.values()\n",
    "\n",
    "        frames_of_groups = [x.sample(processed_df_grouped.size().min(), random_state=self.random_state) for y, x in processed_df_grouped]\n",
    "        self.df = pd.concat(frames_of_groups)\n",
    "\n",
    "        self.df = shuffle(self.df, random_state=self.random_state)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def split_train_test_dataset(self, train_size=0.8):\n",
    "        # Training dataset\n",
    "        train_data = self.df[[self.get_text_column_name(), self.label_column]].sample(frac=train_size, random_state=self.random_state)\n",
    "\n",
    "        # Testing dataset\n",
    "        test_data = self.df[[self.get_text_column_name(), self.label_column]].drop(train_data.index)\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': False,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "\n",
    "data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "# print(data_handler.get_top_words(100))\n",
    "# print(data_handler.get_top_words_tfidf(100))\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset()\n",
    "# data_handler.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9981e",
   "metadata": {
    "id": "1da9981e"
   },
   "source": [
    "# Language Model Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642105b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:53:36.036873Z",
     "start_time": "2023-10-19T11:45:33.099759Z"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1700140203284,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "d642105b"
   },
   "outputs": [],
   "source": [
    "class LanguageModelHandler():\n",
    "    def __init__(self, model_name, dataset_type, text_column, label_column, text_size_limit=512):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.pipeline = None\n",
    "        self.zero_shot_pipeline = None\n",
    "        self.num_labels = 0\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.dataset_type = dataset_type\n",
    "        self.text_size_limit = text_size_limit\n",
    "        self.create_tokenizer()\n",
    "\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "\n",
    "    def test_gpu(self):\n",
    "        print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        # Storing ID of current CUDA device\n",
    "        cuda_id = torch.cuda.current_device()\n",
    "        print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def __tokenize_dataset(self, data):\n",
    "        return self.tokenizer(data[self.text_column], max_length= 32,\n",
    "                              truncation=True,\n",
    "                              padding=\"max_length\")\n",
    "\n",
    "    def add_new_tokens_to_tokenizer(self, new_tokens):\n",
    "        if self.tokenizer is not None:\n",
    "            number_of_tokens_added = self.tokenizer.add_tokens(new_tokens=specific_words)\n",
    "\n",
    "            if self.model is not None:\n",
    "                print('### Resizing the model embeddings layer...')\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "            return number_of_tokens_added\n",
    "\n",
    "    def prepare_training_testing_datasets(self, train_data, test_data):\n",
    "        self.hg_train_data = Dataset.from_pandas(train_data)\n",
    "        self.hg_test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "        self.num_labels =len(train_data[self.label_column].value_counts())\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        self.tokenized_dataset_train = self.hg_train_data.map(self.__tokenize_dataset)\n",
    "        self.tokenized_dataset_test = self.hg_test_data.map(self.__tokenize_dataset)\n",
    "\n",
    "        return self.tokenized_dataset_train, self.tokenized_dataset_test\n",
    "\n",
    "    def create_model(self):\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            id2label={0: 'non-'+self.dataset_type, 1:self.dataset_type})\n",
    "        except:\n",
    "            print('Error to import the model, ignore mismatched sizes')\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            ignore_mismatched_sizes=True,\n",
    "                                                                            id2label={0: 'non-'+self.dataset_type, 1:self.dataset_type})\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Function to compute the metric\n",
    "    def __compute_metrics(self, eval_pred):\n",
    "        metric_accuracy = evaluate.load(\"accuracy\")\n",
    "        metric_precision = evaluate.load(\"precision\")\n",
    "        metric_recall = evaluate.load(\"recall\")\n",
    "        metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "        logits, labels = eval_pred\n",
    "        # probabilities = tf.nn.softmax(logits)\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "        results = {\n",
    "            'accuracy': metric_accuracy.compute(predictions=predictions, references=labels),\n",
    "            'precision': metric_precision.compute(predictions=predictions, references=labels),\n",
    "            'recall': metric_recall.compute(predictions=predictions, references=labels),\n",
    "            'f1': metric_f1.compute(predictions=predictions, references=labels)\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def train_evaluate_model(self, training_args, early_stopping_patience, iterations):\n",
    "\n",
    "        results_summary = {}\n",
    "        detailed_metrics = ['eval_accuracy', 'eval_precision', 'eval_recall',  'eval_f1']\n",
    "\n",
    "        model = deepcopy(self.model)\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            train_dataset = self.tokenized_dataset_train,\n",
    "            eval_dataset = self.tokenized_dataset_test,\n",
    "            compute_metrics = self.__compute_metrics\n",
    "        )\n",
    "\n",
    "        if early_stopping_patience:\n",
    "            self.trainer.callbacks = [EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "        results = self.trainer.evaluate(self.tokenized_dataset_test)\n",
    "\n",
    "        for metric in results:\n",
    "            if metric not in results_summary:\n",
    "                if metric in detailed_metrics:\n",
    "                    results_summary[metric] = [results[metric][\"\".join(metric.split('eval_'))]]\n",
    "                else:\n",
    "                    results_summary[metric] = [results[metric]]\n",
    "            else:\n",
    "                if metric in detailed_metrics:\n",
    "                    results_summary[metric].append(results[metric][\"\".join(metric.split('eval_'))])\n",
    "                else:\n",
    "                    results_summary[metric].append(results[metric])\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        return results_summary, self.trainer\n",
    "\n",
    "    def __create_classification_column(self, df, classification_column='classification'):\n",
    "        # Add a new column 'classification' with 0 if 'non-sexist' has higher probability, else 1\n",
    "        df[classification_column] = df.apply(lambda row: 0 if row['non-'+self.dataset_type] > row[self.dataset_type] else 1, axis=1)\n",
    "        return df\n",
    "\n",
    "    def __data_loader(self, dataframe, column=1):\n",
    "        for row in dataframe.values:\n",
    "            text = row[column] # Getting the text of the tweet\n",
    "\n",
    "            if len(text.split()) > self.text_size_limit:\n",
    "                yield text.split()[:self.text_size_limit]\n",
    "            else:\n",
    "                yield text\n",
    "\n",
    "    def classify_unlabaled_datasets(self, dataset_name_file, result_file_name, batch_size_to_save, column_index=1):\n",
    "\n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = pipeline('text-classification', model=self.model,\n",
    "                                     tokenizer=self.tokenizer, device=self.device)\n",
    "\n",
    "        df = pd.read_csv(dataset_name_file)#.head(4000)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        if os.path.isfile(result_file_name): # if the results file exists\n",
    "            df_results = pd.read_csv(result_file_name)\n",
    "            df = df.tail(df.shape[0] - df_results.shape[0])\n",
    "        else:\n",
    "            df_results = pd.DataFrame(columns=list(self.model.config.id2label.values()))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for prediction in tqdm(self.pipeline(self.__data_loader(df, column_index), batch_size=32, return_all_scores=True), total=df.shape[0]):\n",
    "            result = {\n",
    "                text_column: [df.iloc[i]['text']],\n",
    "                prediction[0]['label']: [prediction[0]['score']],\n",
    "                prediction[1]['label']: [prediction[1]['score']]\n",
    "            }\n",
    "\n",
    "            df_results = pd.concat([df_results, pd.DataFrame.from_dict(result)])\n",
    "\n",
    "            if i % batch_size_to_save == 0 and i > 0:\n",
    "                self.__create_classification_column(df_results, self.dataset_type).to_csv(result_file_name, index=False)\n",
    "            i += 1\n",
    "\n",
    "        self.__create_classification_column(df_results, self.dataset_type).to_csv(result_file_name, index=False)#['label_match'].value_counts()\n",
    "\n",
    "    def predict_proba(self, texts_array):\n",
    "\n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = pipeline('text-classification', model=self.model,\n",
    "                                     tokenizer=self.tokenizer, device=self.device)\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for predictions in tqdm(self.pipeline(self.__data_loader(pd.DataFrame(texts_array), column=0),\n",
    "                                              batch_size=32, return_all_scores=True),\n",
    "                                total=len(texts_array)):\n",
    "        #for predictions in [{'label': 'non-racism', 'score': 0.44055721163749695}, {'label': 'racism', 'score': 0.5594428181648254}]:\n",
    "            all_results.append([prediction['score'] for prediction in predictions])\n",
    "\n",
    "        return np.array(all_results)\n",
    "\n",
    "    def zero_shot_classification(self, sentence, labels, model_name=None):\n",
    "        if self.zero_shot_pipeline is None:\n",
    "\n",
    "            if model_name is None:\n",
    "                model_name = self.model_name\n",
    "\n",
    "            self.zero_shot_pipeline = pipeline(\"zero-shot-classification\", model=model_name, device=self.device)\n",
    "\n",
    "        return self.zero_shot_pipeline(sentence, labels)\n",
    "\n",
    "    def zero_shot_classification_dataframe(self, dataframe, labels, model_name=None, results_file_name=None,\n",
    "                                           batch_size=1000, column_index=1):\n",
    "\n",
    "        if results_file_name and os.path.isfile(results_file_name):\n",
    "            results_df = pd.read_csv(results_file_name)\n",
    "            dataframe = dataframe.tail(dataframe.shape[0]-results_df.shape[0]) # getting the last rows that were not collected yet\n",
    "\n",
    "        else:\n",
    "            results_df = pd.DataFrame(columns=['text']+labels)\n",
    "\n",
    "        print('Classifying', dataframe.shape[0], 'tweets')\n",
    "\n",
    "        if self.zero_shot_pipeline is None:\n",
    "            if model_name is None:\n",
    "                model_name = self.model_name\n",
    "            self.zero_shot_pipeline = pipeline(\"zero-shot-classification\", model=model_name, device=self.device)\n",
    "\n",
    "        i = results_df.shape[0]\n",
    "\n",
    "        for prediction in tqdm(self.zero_shot_pipeline(self.__data_loader(dataframe, column_index), labels,\n",
    "                                                       batch_size=32, return_all_scores=True),\n",
    "                                                       total=dataframe.shape[0]):\n",
    "            pred = {} # {'text': prediction['sequence']}\n",
    "\n",
    "            for j in range(len(prediction['labels'])):\n",
    "                pred[prediction['labels'][j]] = prediction['scores'][j]\n",
    "\n",
    "            results_df = pd.concat([results_df, pd.DataFrame([pred])], ignore_index=True)\n",
    "\n",
    "            if i % batch_size == 0 and results_file_name is not None:\n",
    "                results_df.to_csv(results_file_name, index=False)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        results_df['text'] = dataframe.iloc[:, column_index]\n",
    "\n",
    "        if results_file_name:\n",
    "            results_df.to_csv(results_file_name, index=False)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    def sentences_to_embedding_standard(self, sentences, model_names=None):\n",
    "\n",
    "        embeddings_results = {}\n",
    "\n",
    "        if model_names is None:\n",
    "            model_names = [self.model_name]\n",
    "\n",
    "        for model_name in model_names:\n",
    "\n",
    "            model = SentenceTransformer(model_name_or_path=model_name) #  device='gpu'\n",
    "\n",
    "            embeddings = model.encode(sentences)\n",
    "            embeddings_results[model_name] = embeddings\n",
    "\n",
    "        return embeddings_results\n",
    "\n",
    "    def sentences_to_embedding_fine_tuning(self, sentences, model_name_list, model_list, tokenizer_list):\n",
    "        # tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        # model = RobertaModel.from_pretrained('roberta-base')\n",
    "        # tweets = [\"Replace me by any text you'd like in this sentence.\",\n",
    "        #           \"Replace me by any text you'd like in this sentence2.\"]\n",
    "\n",
    "        embeddings_results = {}\n",
    "\n",
    "        for model_name, model, tokenizer in zip(model_name_list, model_list, tokenizer_list):\n",
    "            encoded_inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_inputs)\n",
    "                embeddings = output.logits # pooler_output\n",
    "                embeddings_results[model_name] = (embeddings)\n",
    "\n",
    "        return embeddings_results\n",
    "\n",
    "    def plot_embeddings(self, embeddings_results, labels, algorithm='PCA', all_together=False):\n",
    "\n",
    "        embeddings_df = pd.DataFrame()\n",
    "\n",
    "        n_components = 2\n",
    "\n",
    "        if algorithm == 'TSNE':\n",
    "            dim_reduction_obj = TSNE(n_components=n_components, verbose=0, perplexity=40, n_iter=300)\n",
    "\n",
    "        elif algorithm == 'PCA':\n",
    "            dim_reduction_obj = PCA(n_components=n_components)\n",
    "\n",
    "        elif algorithm == 'MDS':\n",
    "            dim_reduction_obj = MDS(n_components=n_components, metric=True, random_state=42)\n",
    "\n",
    "\n",
    "        for model_name in embeddings_results:\n",
    "            X = embeddings_results[model_name]\n",
    "\n",
    "            reduced_data = dim_reduction_obj.fit_transform(X)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['model'] = [model_name]*X.shape[0]\n",
    "            df['model'] = df['model'].apply(lambda i: str(i))\n",
    "\n",
    "            df['label'] = labels\n",
    "\n",
    "            df['first_dimension'] = reduced_data[:,0]\n",
    "            df['second_dimension'] = reduced_data[:,1]\n",
    "\n",
    "            embeddings_df = pd.concat([embeddings_df, df])\n",
    "\n",
    "        if all_together:\n",
    "            self.plot_embbedings_together(embeddings_df)\n",
    "        else:\n",
    "            self.plot_embbedings_separated(embeddings_df)\n",
    "\n",
    "    def plot_embbedings_together(self, embeddings_df):\n",
    "        plt.figure(figsize=(16,10))\n",
    "\n",
    "        # Automatically assign colors and shapes\n",
    "        unique_models = embeddings_df['model'].unique()\n",
    "        unique_labels = embeddings_df['label'].unique()\n",
    "\n",
    "        color_dict = {model: plt.cm.tab10(i) for i, model in enumerate(unique_models)}\n",
    "        symbols = ['x', 'o'] #['v', '^', 's', 'D', 'o', '<', '>', 'p', '*']\n",
    "        shape_dict = {label: marker for label, marker in zip(unique_labels, symbols)}\n",
    "\n",
    "        # Scatter plot\n",
    "        for model, group_model in embeddings_df.groupby('model'):\n",
    "            for label, group_label in group_model.groupby('label'):\n",
    "                plt.scatter(group_label['first_dimension'], group_label['second_dimension'],\n",
    "                            label=f'{model} - {label}',\n",
    "                            color=color_dict.get(model, 'black'),\n",
    "                            marker=shape_dict.get(label, 'o'),\n",
    "                            alpha=0.3)\n",
    "\n",
    "        # Customize the plot\n",
    "        # plt.title('Scatter Plot with Models and Labels')\n",
    "        plt.xlabel('First Dimension')\n",
    "        plt.ylabel('Second Dimension')\n",
    "        plt.legend()\n",
    "        plt.grid(False)\n",
    "\n",
    "        # disabling xticks by Setting xticks to an empty list\n",
    "        plt.xticks([])  \n",
    "         \n",
    "        # disabling yticks by setting yticks to an empty list\n",
    "        plt.yticks([]) \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_embbedings_separated(self, embeddings_df):\n",
    "        # Automatically assign colors and shapes\n",
    "        unique_models = embeddings_df['model'].unique()\n",
    "        unique_labels = embeddings_df['label'].unique()\n",
    "\n",
    "        # color_dict = {model: plt.cm.tab10(i) for i, model in enumerate(unique_models)}\n",
    "        color_dict = {label: sns.color_palette(\"husl\", n_colors=len(unique_labels))[i] for i, label in enumerate(unique_labels)}\n",
    "        shape_dict = {label: marker for label, marker in zip(unique_labels, ['x', 'o', 's', 'D', 'v', '<', '>', 'p', '*'])}\n",
    "\n",
    "        # Set the size of each subplot\n",
    "        fig, axes = plt.subplots(1, len(embeddings_df['model'].unique()), figsize=(15, 5))  # Adjust the figsize as needed\n",
    "\n",
    "        # Create subplots for each model\n",
    "        for ax, model in zip(axes, embeddings_df['model'].unique()):\n",
    "            ax.set_title(model)\n",
    "\n",
    "            for label, group_label in embeddings_df[embeddings_df['model'] == model].groupby('label'):\n",
    "                ax.scatter(group_label['first_dimension'], group_label['second_dimension'],\n",
    "                          label=label,\n",
    "                          color=color_dict.get(label, 'black'),\n",
    "                          marker=shape_dict.get(label, 'o'))\n",
    "\n",
    "            ax.set_xlabel('First Dimension')\n",
    "            ax.set_ylabel('Second Dimension')\n",
    "            ax.legend()\n",
    "            ax.grid(False)\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # disabling xticks by Setting xticks to an empty list\n",
    "        plt.xticks([])  \n",
    "         \n",
    "        # disabling yticks by setting yticks to an empty list\n",
    "        plt.yticks([]) \n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, path, name_file):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(path+name_file)\n",
    "        # Save model\n",
    "        self.trainer.save_model(path+name_file)\n",
    "\n",
    "    def load_model(self, path, name_file):\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path+name_file)\n",
    "        # Load Model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path+name_file)\n",
    "\n",
    "        return self.tokenizer, self.model\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# language_model_manager = LanguageModelHandler(model_name= 'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "#                                               dataset_type=dataset_type,\n",
    "#                                               text_column=data_handler.get_text_column_name(),\n",
    "#                                               label_column=data_handler.label_column)\n",
    "\n",
    "# language_model_manager = LanguageModelHandler(model_name= 'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "#                                               dataset_type='text',\n",
    "#                                               text_column='text',\n",
    "#                                               label_column='label')\n",
    "\n",
    "# sentences = ['Impact of words for the sentence: If it weren’t for the “niggers” England wouldn’t of got out of group stages. You lot are shite, be grateful \\#eng \\#Euro2020Final \\#euro2020',\n",
    "#              'Im in tears right now, we are in the final \\#EURO2020 \\#ENGDEN \\#ENG']\n",
    "\n",
    "# sentences = test_data[data_handler.get_text_column_name()].to_list()\n",
    "\n",
    "# embeddings = language_model_manager.sentences_to_embedding(sentences=sentences, model_name=None, pre_trained_model=None)\n",
    "\n",
    "# print(embeddings)\n",
    "\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=len(sentences)-1, n_iter=300)\n",
    "# tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "# print(tsne_results)\n",
    "\n",
    "# tsne_results_df = pd.DataFrame()\n",
    "\n",
    "# tsne_results_df['comp-1'] = tsne_results[:,0]\n",
    "# tsne_results_df['comp-2'] = tsne_results[:,1]\n",
    "\n",
    "# tsne_results_df['class'] = ['1', '2']\n",
    "\n",
    "# sns.scatterplot(x=\"comp-1\", y=\"comp-2\",\n",
    "#                 hue=tsne_results_df['class'].tolist(),\n",
    "#                 palette=sns.color_palette(\"hls\", 10),\n",
    "#                 data=tsne_results_df).set(title=\"Data T-SNE projection\")\n",
    "\n",
    "\n",
    "# # language_model_manager.zero_shot_classification(sentence='fuck off!', labels=['offensive', 'non-offensive'],\n",
    "# #                                                 model_name='facebook/bart-large-mnli')\n",
    "\n",
    "# language_model_manager.zero_shot_classification_dataframe(dataframe=data_handler.df.head(),\n",
    "#                                                           labels=['offensive', 'non-offensive'],\n",
    "#                                                           model_name='facebook/bart-large-mnli',\n",
    "#                                                           results_file_name='',\n",
    "#                                                           batch_size=1000)\n",
    "\n",
    "\n",
    "# print(language_model_manager.test_gpu())\n",
    "\n",
    "# language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "# language_model_manager.create_model()\n",
    "\n",
    "# # Set up training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "#     logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "#     logging_strategy='epoch',\n",
    "#     logging_steps=100,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     learning_rate=5e-6,\n",
    "#     save_strategy='epoch',\n",
    "#     save_steps=100,\n",
    "#     evaluation_strategy='epoch',\n",
    "#     eval_steps=100,\n",
    "#     load_best_model_at_end=True,\n",
    "#     num_train_epochs=10,\n",
    "#     # seed=42\n",
    "# )\n",
    "\n",
    "# results, trainer = language_model_manager.train_evaluate_model(training_args=training_args,\n",
    "#                                                                early_stopping_patience=2,\n",
    "#                                                                iterations=1) # '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641886a2",
   "metadata": {
    "id": "641886a2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d90a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T12:33:11.182035Z",
     "start_time": "2023-10-19T12:11:59.362544Z"
    },
    "id": "61d90a9a"
   },
   "outputs": [],
   "source": [
    "class ExplainableTransformerPipeline():\n",
    "    \"\"\"Wrapper for Captum framework usage with Huggingface Pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, pipeline_name='text-classification'):\n",
    "\n",
    "        if 'Roberta' in model.__class__.__name__:\n",
    "            self.__name = 'roberta'\n",
    "        elif 'Bert' in model.__class__.__name__:\n",
    "            self.__name = 'bert'\n",
    "\n",
    "        self.__pipeline = pipeline(pipeline_name, model=model, tokenizer=tokenizer, device=device)\n",
    "        self.__cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "        self.__device = device\n",
    "\n",
    "    def forward_func(self, inputs, position = 0):\n",
    "        \"\"\"\n",
    "            Wrapper around prediction method of pipeline\n",
    "        \"\"\"\n",
    "        pred = self.__pipeline.model(inputs, attention_mask=torch.ones_like(inputs))\n",
    "        return pred[position]\n",
    "\n",
    "    def visualize_word_importance_in_sentence(self, text:str):\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text)\n",
    "\n",
    "        print('Prediction:', self.__cls_explainer.predicted_class_name)\n",
    "        print('Words importance:', word_attributions)\n",
    "\n",
    "        self.__cls_explainer.visualize()\n",
    "\n",
    "\n",
    "    def visualize_word_importance(self, inputs: list, attributes: list, prediction:str):\n",
    "        \"\"\"\n",
    "            Visualization method.\n",
    "            Takes list of inputs and correspondent attributs for them to visualize in a barplot\n",
    "        \"\"\"\n",
    "        attr_sum = attributes.sum(-1)\n",
    "\n",
    "        attr = attr_sum / torch.norm(attr_sum)\n",
    "\n",
    "        word_importance = pd.Series(attr.cpu().numpy()[0],\n",
    "                         index = self.__pipeline.tokenizer.convert_ids_to_tokens(inputs.detach().cpu().numpy()[0],skip_special_tokens=False))\n",
    "\n",
    "        print(word_importance)\n",
    "\n",
    "        plt.title(prediction)\n",
    "        plt.show(word_importance.plot.barh(figsize=(10,20)))\n",
    "\n",
    "        return word_importance\n",
    "\n",
    "    def __generate_inputs(self, text: str):\n",
    "        \"\"\"\n",
    "            Convenience method for generation of input ids as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.__pipeline.tokenizer.encode(text, add_special_tokens=False),\n",
    "                            device = self.__device).unsqueeze(0)\n",
    "\n",
    "    def generate_baseline(self, sequence_len: int):\n",
    "        \"\"\"\n",
    "            Convenience method for generation of baseline vector as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor([self.__pipeline.tokenizer.cls_token_id] + [self.__pipeline.tokenizer.pad_token_id] * (sequence_len - 2) + [self.__pipeline.tokenizer.sep_token_id], device = self.__device).unsqueeze(0)\n",
    "\n",
    "    def __clean_text_for_explanation(self, text):\n",
    "        text = re.sub(r'(?<=:)\\s+|\\s+(?=:)', '', text)\n",
    "        text = emoji.emojize(text)\n",
    "\n",
    "        regular_punct = list(string.punctuation) # python punctuations\n",
    "        special_punct = ['©', '^', '®',' ','¾', '¡','!'] # user defined special characters to remove\n",
    "\n",
    "        for punc in regular_punct:\n",
    "            if punc in text:\n",
    "                text = text.replace(punc, ' ')\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    ## LIME\n",
    "    def model_adapter(self, texts):\n",
    "\n",
    "        all_scores = []\n",
    "        batch_size = 64\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "\n",
    "            batch = texts[i:i+batch_size]\n",
    "\n",
    "            # use bert encoder to tokenize text\n",
    "            encoded_input = self.__pipeline.tokenizer(batch,\n",
    "                              return_tensors='pt',\n",
    "                              padding=True,\n",
    "                              truncation=True,\n",
    "                              max_length=self.__pipeline.model.config.max_position_embeddings-2)\n",
    "\n",
    "            for key in encoded_input:\n",
    "                encoded_input[key] = encoded_input[key].to(self.__device)\n",
    "\n",
    "            output = self.__pipeline.model(**encoded_input)\n",
    "            # by default this model gives raw logits rather\n",
    "            # than a nice smooth softmax so we apply it ourselves here\n",
    "\n",
    "            scores = output[0].softmax(1).detach().cpu().numpy()\n",
    "\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "        return np.array(all_scores)\n",
    "\n",
    "\n",
    "    def get_most_impactful_words_lime(self, text, keyword, word_importance_results):\n",
    "\n",
    "        prediction = self.__pipeline(text)[0]['label']\n",
    "\n",
    "        if prediction == keyword:\n",
    "            print(text)\n",
    "            te = TextExplainer(n_samples=500, random_state=42)\n",
    "            te.fit(text, self.model_adapter)\n",
    "\n",
    "            graphic_explanation = te.explain_prediction(target_names=list(self.__pipeline.model.config.id2label.values()))\n",
    "\n",
    "            print(graphic_explanation.targets)\n",
    "\n",
    "            for element in graphic_explanation.targets:\n",
    "                for f in element.feature_weights.pos:\n",
    "                    for word in f.feature.split():\n",
    "                        if word in word_importance_results:\n",
    "                            word_importance_results[word] += f.weight\n",
    "                        else:\n",
    "                            word_importance_results[word] = f.weight\n",
    "                return word_importance_results, graphic_explanation\n",
    "        else:\n",
    "            return word_importance_results, None\n",
    "\n",
    "\n",
    "\n",
    "    ## INTEGRATED GRADIENTS\n",
    "    def explain(self, text: str):\n",
    "        \"\"\"\n",
    "            Main entry method. Passes text through series of transformations and through the model.\n",
    "            Calls visualization method.\n",
    "        \"\"\"\n",
    "        prediction = self.__pipeline.predict(text)\n",
    "        inputs = self.__generate_inputs(text)\n",
    "        baseline = self.generate_baseline(sequence_len = inputs.shape[1])\n",
    "\n",
    "        print('inputs', len(inputs[0]))\n",
    "        # print('se liga:', self.__pipeline.model.config.label2id)\n",
    "\n",
    "        lig = LayerIntegratedGradients(self.forward_func,\n",
    "                                       getattr(self.__pipeline.model, self.__name).embeddings)\n",
    "\n",
    "        # For some reason we need to swap the label dictionary\n",
    "        labels_swaped = {v: k for k, v in self.__pipeline.model.config.id2label.items()}\n",
    "\n",
    "        attributes, delta = lig.attribute(inputs=inputs,\n",
    "                                  baselines=baseline,\n",
    "                                  target=labels_swaped[prediction[0]['label']],\n",
    "                                  return_convergence_delta=True)\n",
    "\n",
    "        self.visualize_word_importance(inputs, attributes, prediction)\n",
    "\n",
    "\n",
    "    def join_tokens_into_words(self, token_tuples):\n",
    "        self.tokens_to_exclude = ['[CLS]', '[SEP]']\n",
    "        tokens_list = []\n",
    "        scores_list = []\n",
    "\n",
    "        current_tokens_list = []\n",
    "        current_scores_list = []\n",
    "\n",
    "        for i, (token, score) in enumerate(token_tuples):\n",
    "          if token in self.tokens_to_exclude:\n",
    "              continue\n",
    "\n",
    "          if i < len(token_tuples)-1:\n",
    "            next_token = token_tuples[i+1][0]\n",
    "\n",
    "            if '##' not in next_token and len(current_tokens_list) > 0:\n",
    "              current_tokens_list.append(token)\n",
    "              current_scores_list.append(score)\n",
    "\n",
    "              tokens_list.append(current_tokens_list)\n",
    "              scores_list.append(current_scores_list)\n",
    "\n",
    "              current_tokens_list = []\n",
    "              current_scores_list = []\n",
    "\n",
    "            elif '##' not in next_token and len(current_tokens_list) == 0:\n",
    "              tokens_list.append([token])\n",
    "              scores_list.append([score])\n",
    "\n",
    "            elif '##' in next_token:\n",
    "              current_tokens_list.append(token)\n",
    "              current_scores_list.append(score)\n",
    "\n",
    "        last_token = token_tuples[-1][0]\n",
    "        last_score = token_tuples[-1][1]\n",
    "\n",
    "        if '##' in last_token and last_token not in self.tokens_to_exclude:\n",
    "          tokens_list.append(current_tokens_list+[last_token])\n",
    "          scores_list.append(current_scores_list+[last_score])\n",
    "\n",
    "        elif '##' not in last_token and last_token not in self.tokens_to_exclude:\n",
    "          tokens_list.append([last_token])\n",
    "          scores_list.append([last_score])\n",
    "\n",
    "        return tokens_list, scores_list\n",
    "\n",
    "\n",
    "    def __get_most_impactful_words_integrated_gradients(self, text_to_evaluate, threshold, keyword, results):\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text=text_to_evaluate)\n",
    "        # print(self.__cls_explainer.predicted_class_name)\n",
    "        tokens_list, scores_list = self.join_tokens_into_words(word_attributions)\n",
    "\n",
    "        new_word_attributions = []\n",
    "        for i, tokens in enumerate(tokens_list):\n",
    "            new_word_attributions.append((self.__pipeline.tokenizer.convert_tokens_to_string(tokens), np.mean(scores_list[i])))\n",
    "\n",
    "        if self.__cls_explainer.predicted_class_name == keyword:\n",
    "\n",
    "            for word in new_word_attributions:\n",
    "                if word[1] > threshold:\n",
    "                    if word[0] in results:\n",
    "                        results[word[0]] += 1\n",
    "                    else:\n",
    "                        results[word[0]] = 1\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_vertical_bar(self, text, word_scores):\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Create a vertical bar plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        bars = ax.bar(words, word_scores, color='#5B2C6F') # color='skyblue'\n",
    "\n",
    "        margin = 0.02\n",
    "\n",
    "        for word, score, bar in zip(words, word_scores, bars):\n",
    "            if score >= 0:\n",
    "                ax.text(word, score + margin, f'{score:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "            else:\n",
    "                ax.text(word, score - margin, f'{score:.2f}', ha='center', va='top', fontsize=10)\n",
    "\n",
    "        # Rotate word labels by 45 degrees for readability\n",
    "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Words')\n",
    "        ax.set_ylabel('Word Impact Scores')\n",
    "        # ax.set_title('Word Scores Vertical Bar Plot')\n",
    "\n",
    "        # Adjust the position of the y-axis labels\n",
    "        ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "        ax.set_ylim([np.min(word_scores)-0.1, np.max(word_scores)+0.1])\n",
    "\n",
    "        # plt.grid(True, color = \"grey\", linewidth = \"1\")\n",
    "        plt.axhline(y=0, color='black', linestyle='-', linewidth=\"0.5\")\n",
    "\n",
    "        # Display the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_colored_text(self, text, word_scores):\n",
    "\n",
    "        # Create a colormap based on the 'viridis' colormap\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        # cmap = plt.get_cmap('viridis')\n",
    "\n",
    "        # Normalize word scores to the range [0, 1]\n",
    "        norm = plt.Normalize(min(word_scores), max(word_scores))\n",
    "\n",
    "        # Create a color map using the normalized scores and the colormap\n",
    "        mappable = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        mappable.set_array([])\n",
    "\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Calculate the horizontal spacing between words\n",
    "        total_word_count = len(words)\n",
    "        spacing = 1.0 / total_word_count\n",
    "\n",
    "        # Create a figure and axis for the text\n",
    "        fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "        for i, (word, score) in enumerate(zip(words, word_scores)):\n",
    "            color = cmap(norm(score))\n",
    "            x_position = i * spacing\n",
    "            ax.text(x_position, 0, word, color=color, fontsize=12, ha='center', rotation=45)\n",
    "\n",
    "        # Add a color scale just above the text\n",
    "        colorbar = ColorbarBase(ax=fig.add_axes([0.2, 0.8, 0.6, 0.02]),\n",
    "                                cmap=cmap,\n",
    "                                norm=norm,\n",
    "                                orientation='horizontal')\n",
    "        colorbar.set_label('Word Impact Scores')\n",
    "\n",
    "        # Remove the axis and display the plot\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_word_importance(self, sentence, bar=True):\n",
    "\n",
    "        sentence = self.__clean_text_for_explanation(sentence)\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text=sentence)\n",
    "\n",
    "        print(word_attributions)\n",
    "\n",
    "        tokens_list, scores_list = self.join_tokens_into_words(word_attributions)\n",
    "\n",
    "        scores_list = [np.mean(scores) for scores in scores_list]\n",
    "\n",
    "        print(scores_list)\n",
    "\n",
    "        if bar:\n",
    "            print(len(sentence.split()), len(scores_list))\n",
    "            self.plot_vertical_bar(sentence, scores_list)\n",
    "        else:\n",
    "            self.plot_colored_text(sentence, scores_list)\n",
    "            # self.plot_sentence(sentence)\n",
    "\n",
    "\n",
    "    def get_most_impactful_words_for_dataset(self, dataset, column_text,\n",
    "                                             threshold, keyword, method, n=20):\n",
    "        results = {}\n",
    "\n",
    "        i = 0\n",
    "        for index, row in dataset.iterrows():\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Processing:', i)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            text = self.__clean_text_for_explanation(row[column_text])\n",
    "\n",
    "            if method == 'integrated_gradients':\n",
    "                results = self.__get_most_impactful_words_integrated_gradients(text_to_evaluate=text,\n",
    "                                                            threshold=threshold,\n",
    "                                                            results=results,\n",
    "                                                            keyword=keyword)\n",
    "            elif method == 'lime':\n",
    "                results, graphic_explanation = self.get_most_impactful_words_lime(\n",
    "                                                                    text=text,\n",
    "                                                                    keyword=keyword,\n",
    "                                                                    word_importance_results=results)\n",
    "\n",
    "            # if i > 5:\n",
    "            #     break\n",
    "\n",
    "        return pd.DataFrame([(key, value) for key, value in dict(sorted(results.items(), key=lambda item: item[1], reverse=True)).items()], columns=['word','frequency']).head(n)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------\n",
    "# exp_model = ExplainableTransformerPipeline(model=language_model_manager.trainer.model,\n",
    "#                                            tokenizer=language_model_manager.tokenizer,\n",
    "#                                            device=language_model_manager.device,\n",
    "#                                            pipeline_name='text-classification')\n",
    "\n",
    "\n",
    "# ####### LIME\n",
    "# # results_most_important_words = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "# #                                                                               column_text=data_handler.get_text_column_name(),\n",
    "# #                                                                               threshold=0, keyword='racism',\n",
    "# #                                                                               method='lime', n=100)\n",
    "\n",
    "\n",
    "# ## Using lime to plot the word importance for few samples\n",
    "# # samples = test_data[test_data[label_column]==1].sample(n=3, random_state=42)\n",
    "\n",
    "# # for sample in samples[data_handler.get_text_column_name()]:\n",
    "# #     print('*** Sample:',sample)\n",
    "# #     word_importance_results, graphic_explanation = exp_model.get_most_impactful_words_lime(sample, 'racism', {})\n",
    "# #     print(word_importance_results)\n",
    "# #     graphic_explanation\n",
    "\n",
    "\n",
    "# ####### INTEGRATED GRADIENTS\n",
    "\n",
    "# # Using integrated gradients to plot the word importance for few samples\n",
    "# samples = test_data[test_data[label_column]==1].sample(n=3,random_state=42)\n",
    "\n",
    "# for sample in samples[data_handler.get_text_column_name()]:\n",
    "# # for sample in samples[data_handler.text_column]:\n",
    "#     print(sample)\n",
    "#     # exp_model.explain(sample)\n",
    "#     # exp_model.visualize_word_importance_in_sentence(sample)\n",
    "#     exp_model.plot_word_importance(sample, bar=True)\n",
    "\n",
    "\n",
    "# results = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "#                                                column_text=data_handler.get_text_column_name(),\n",
    "#                                                threshold=0.1,\n",
    "#                                                keyword=dataset_type,\n",
    "#                                                method='integrated_gradients',\n",
    "#                                                n=50)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b0f97",
   "metadata": {
    "heading_collapsed": true,
    "id": "199b0f97",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf39ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:12:13.071533Z",
     "start_time": "2023-10-16T14:11:34.390225Z"
    },
    "hidden": true,
    "id": "0fcf39ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ExperimentManager():\n",
    "    def __init__(self, data_handler, dataset_type):\n",
    "        self.data_handler = data_handler\n",
    "        self.dataset_type = dataset_type\n",
    "        self.metrics = ('eval_accuracy','eval_precision','eval_recall', 'eval_f1')\n",
    "\n",
    "    def start_experiment(self, experiment_design, preprocessing_setup):\n",
    "        data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "        train_data, test_data = self.data_handler.split_train_test_dataset()\n",
    "\n",
    "        if experiment_design['unsample']:\n",
    "            data_handler.unsample()\n",
    "\n",
    "        experiment_results = {}\n",
    "\n",
    "        for model_name in experiment_design['model_list']:\n",
    "\n",
    "            print('----------------------------------------')\n",
    "            print('Training:', model_name)\n",
    "\n",
    "            language_model_manager = LanguageModelHandler(model_name=model_name,\n",
    "                                              dataset_type=self.dataset_type,\n",
    "                                              text_column=self.data_handler.get_text_column_name(),\n",
    "                                              label_column=self.data_handler.label_column)\n",
    "\n",
    "            language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "            language_model_manager.create_model()\n",
    "\n",
    "            results, trainer = language_model_manager.train_evaluate_model(training_args=experiment_design['training_args'],\n",
    "                                                               early_stopping_patience=experiment_design['early_stopping_patience'],\n",
    "                                                               iterations=experiment_design['iterations'])\n",
    "\n",
    "            df_results = pd.DataFrame()\n",
    "            df_results['Dataset'] = [self.dataset_type] * len(self.metrics)\n",
    "            df_results['Model'] = [model_name] * len(self.metrics)\n",
    "            df_results['Metric'] = [metric.replace('eval_', '').capitalize() for metric in self.metrics]\n",
    "            df_results['Value'] = [np.mean(results[k]) for k in self.metrics if k in results]\n",
    "\n",
    "            experiment_results[model_name] = {'results':df_results, 'model':language_model_manager}\n",
    "\n",
    "        return experiment_results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': False,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "# No preprocessing\n",
    "# preprocessing_setup = {key: False for key in preprocessing_setup}\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-6,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10,\n",
    "    # seed=42\n",
    ")\n",
    "\n",
    "experiment_design = {\n",
    "    'model_list': [\n",
    "        # 'bert-base-uncased',\n",
    "        # 'vinai/bertweet-base',\n",
    "        # 'cardiffnlp/twitter-roberta-base-offensive', # Offensive speech Roberta\n",
    "        # 'Hate-speech-CNERG/dehatebert-mono-english' # Hate speech Roberta\n",
    "\n",
    "        # 'Pablo94/racism-finetuned-detests-29-10-2022', ## Racism model\n",
    "        'bitsanlp/Homophobia-Transphobia-v2-mBERT-EDA' ## Homophobia model\n",
    "    ],\n",
    "    'unsample': True,\n",
    "    'early_stopping_patience': 2,\n",
    "    'training_args': training_args,\n",
    "    'iterations': 1\n",
    "}\n",
    "\n",
    "data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "experiment_manager = ExperimentManager(data_handler, dataset_type=dataset_type)\n",
    "results = experiment_manager.start_experiment(experiment_design, preprocessing_setup)\n",
    "\n",
    "results[experiment_design['model_list'][0]]['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21f6cc",
   "metadata": {
    "id": "6d21f6cc"
   },
   "source": [
    "# Starting the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbc5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T15:59:28.403915Z",
     "start_time": "2023-10-16T15:59:11.039468Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923,
     "referenced_widgets": [
      "e7914fa91c67430f97058fd5100b70d5",
      "9b17ff0391484da5afdf280c19fa98b7",
      "eec7851037614f6ebc703bb58af51622",
      "545f06ebfe884b69811c23284555f64c",
      "482c3d0da9f8439ba580a9a099c88371",
      "5677fa7431e84c7a941806cbbbc1a540",
      "c46fb41481114f238b68067d772e0848",
      "94095cc31948429d8ef6d4450fc9a6b2",
      "c60a95f8d3614d8a9731d001bb44c930",
      "e419d5cbb4e44b1d839ae663c213bafa",
      "9ab8460c49164a0391ca54c2b94c43fa",
      "a369868d59a346db8db2b64a76630eef",
      "c31dc00045fd49a9bc5e4d4096b4d8c7",
      "e1c7ac8be8ea4e2c9bd92e11c6680aaa",
      "aedbb97553524c94a4895361e5039369",
      "791d9505a60041a5836a911c34e3b81d",
      "1eae6dccb2ff420e8c29504cc14eb36f",
      "90ba4c84955640fe9d5ed37ba2b8532f",
      "f1e08ae4144d49229ec193dd1d1f5132",
      "8e70f844541d4ad3a4abdff94d0b66c9",
      "0ddf376336fb42acbcdba242de2f431f",
      "939ca1bf528849f9a0646ed7c4f9c59e"
     ]
    },
    "executionInfo": {
     "elapsed": 220217,
     "status": "error",
     "timestamp": 1700145654017,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "d6bbc5a0",
    "outputId": "dbc0ef4c-17a9-447b-879c-1b86373c88cb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Defining Data Handler object and processing the dataset\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': False,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "\n",
    "data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "# print(data_handler.get_top_words(100))\n",
    "# print(data_handler.get_top_words_tfidf(100))\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Defining the Languager Model Manager object and creating the model\n",
    "# 'vinai/bertweet-base' 'Hate-speech-CNERG/dehatebert-mono-english' 'bert-base-uncased' 'cardiffnlp/twitter-roberta-base-offensive'\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "language_model_manager = LanguageModelHandler(model_name=model_name,\n",
    "                                              dataset_type=dataset_type,\n",
    "                                              text_column=data_handler.get_text_column_name(),\n",
    "                                              label_column=data_handler.label_column)\n",
    "\n",
    "language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "language_model_manager.create_model()\n",
    "\n",
    "#### Training the model\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-6,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "results, trainer = language_model_manager.train_evaluate_model(training_args=training_args,\n",
    "                                                               early_stopping_patience=2,\n",
    "                                                               iterations=1) \n",
    "\n",
    "language_model_manager.save_model(path='saved_models/',name_file=model_name.replace('/','-')) # '''\n",
    "\n",
    "\n",
    "##### Generating the Embeddings\n",
    "sentences = test_data[data_handler.get_text_column_name()].to_list() #+train_data[data_handler.get_text_column_name()].to_list()\n",
    "labels = []\n",
    "\n",
    "for label in test_data[data_handler.label_column].to_list():\n",
    "    if label == 1:\n",
    "        labels.append('racism')\n",
    "    else:\n",
    "        labels.append('non racism')\n",
    "\n",
    "print('Calculating Embeddings...')\n",
    "\n",
    "models_to_test = [\n",
    "                  'bert-base-uncased',\n",
    "                  # 'vinai/bertweet-base',\n",
    "                  # 'Hate-speech-CNERG/dehatebert-mono-english',\n",
    "                  'cardiffnlp/twitter-roberta-base-offensive'\n",
    "                  ]\n",
    "\n",
    "tokenizer_list = [] # [language_model_manager.tokenizer]\n",
    "model_list = [] # [trainer.model]\n",
    "\n",
    "for model_name_to_load in models_to_test:\n",
    "    tokenizer, model = language_model_manager.load_model(path='saved_models/', name_file=model_name_to_load.replace('/','-'))\n",
    "    \n",
    "    tokenizer_list.append(tokenizer)\n",
    "    model_list.append(model)\n",
    "\n",
    "dimention_reduction_algorithm = 'TSNE'\n",
    "\n",
    "embeddings = language_model_manager.sentences_to_embedding_standard(sentences=sentences, model_names=models_to_test)\n",
    "\n",
    "language_model_manager.plot_embeddings(embeddings_results=embeddings, labels=labels, \n",
    "                                       algorithm=dimention_reduction_algorithm, all_together=False)\n",
    "\n",
    "embeddings = language_model_manager.sentences_to_embedding_fine_tuning(sentences=sentences,\n",
    "                                                                       model_name_list=models_to_test, \n",
    "                                                                       tokenizer_list=tokenizer_list,\n",
    "                                                                       model_list=model_list)\n",
    "\n",
    "language_model_manager.plot_embeddings(embeddings_results=embeddings, labels=labels, \n",
    "                                       algorithm=dimention_reduction_algorithm, all_together=False) # '''\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "#### Classifying big unlabled datasets with the trained model\n",
    "\n",
    "# year = '2008'\n",
    "# dataset_path = 'dataset/euros_second/'\n",
    "# dataset_name_file = dataset_path+year+'.csv'\n",
    "# result_file_name = dataset_path+language_model_manager.dataset_type\n",
    "# result_file_name += '/'+year+'_'+language_model_manager.model_name.split('/')[-1]+'.csv'\n",
    "\n",
    "# language_model_manager.classify_unlabaled_datasets(dataset_name_file=dataset_name_file,\n",
    "#                                                    result_file_name=result_file_name,\n",
    "#                                                    batch_size_to_save=100)\n",
    "\n",
    "#### Defining the Explainable AI object\n",
    "exp_model = ExplainableTransformerPipeline(model=language_model_manager.trainer.model,\n",
    "                                           tokenizer=language_model_manager.tokenizer,\n",
    "                                           device=language_model_manager.device,\n",
    "                                           pipeline_name='text-classification')\n",
    "\n",
    "\n",
    "####### LIME\n",
    "# results_most_important_words = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "#                                                                               column_text=data_handler.get_text_column_name(),\n",
    "#                                                                               threshold=0, keyword='racism',\n",
    "#                                                                               method='lime', n=100)\n",
    "\n",
    "\n",
    "## Using lime to plot the word importance for few samples\n",
    "# samples = test_data[test_data[label_column]==1].sample(n=3, random_state=42)\n",
    "\n",
    "# for sample in samples[data_handler.get_text_column_name()]:\n",
    "#     print('*** Sample:',sample)\n",
    "#     word_importance_results, graphic_explanation = exp_model.get_most_impactful_words_lime(sample, 'racism', {})\n",
    "#     print(word_importance_results)\n",
    "#     graphic_explanation\n",
    "\n",
    "\n",
    "####### INTEGRATED GRADIENTS\n",
    "\n",
    "# Using integrated gradients to plot the word importance for few samples\n",
    "# samples = test_data[test_data[label_column]==0].sample(n=3,random_state=42)\n",
    "\n",
    "# print(data_handler.df.iloc[samples.index][data_handler.text_column].values)\n",
    "\n",
    "# for sample in samples[data_handler.get_text_column_name()]:\n",
    "# # for sample in samples[data_handler.text_column]:\n",
    "#     print(sample)\n",
    "#     # exp_model.explain(sample)\n",
    "#     # exp_model.visualize_word_importance_in_sentence(sample)\n",
    "#     exp_model.plot_word_importance(sample, bar=True)\n",
    "\n",
    "\n",
    "# results = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "#                                                column_text=data_handler.get_text_column_name(),\n",
    "#                                                threshold=0.1,\n",
    "#                                                keyword=dataset_type,\n",
    "#                                                method='integrated_gradients',\n",
    "#                                                n=50)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efd365-020c-47f7-b0d2-80927f4751fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T16:45:30.048389Z",
     "start_time": "2023-10-16T16:45:30.035532Z"
    },
    "id": "29efd365-020c-47f7-b0d2-80927f4751fc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Using Pipeline for zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a2b72-1b90-4938-8348-22c46ee681a9",
   "metadata": {
    "id": "a78a2b72-1b90-4938-8348-22c46ee681a9"
   },
   "outputs": [],
   "source": [
    "language_model_manager = LanguageModelHandler(model_name= 'facebook/bart-large-mnli', #'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "                                              dataset_type=dataset_type,\n",
    "                                              text_column='text', #data_handler.get_text_column_name(),\n",
    "                                              label_column='label' #data_handler.label_column\n",
    "                                             )\n",
    "year = '2016'\n",
    "dataset_path = 'dataset/euros_second/'\n",
    "dataset_name_file = dataset_path+year+'.csv'\n",
    "result_file_name = dataset_path+'zero_shot/'\n",
    "result_file_name += year+'_'+language_model_manager.model_name.split('/')[-1]+'.csv'\n",
    "\n",
    "df_to_classify_zero_shot = pd.read_csv(dataset_name_file)\n",
    "\n",
    "# language_model_manager.zero_shot_classification(sentence='fuck off!', labels=['offensive', 'non-offensive'],\n",
    "#                                                 model_name='facebook/bart-large-mnli')\n",
    "\n",
    "language_model_manager.zero_shot_classification_dataframe(dataframe=df_to_classify_zero_shot,\n",
    "                                                          labels=['neutral', 'racism','homophobia', 'sexism'],\n",
    "                                                          results_file_name=result_file_name,\n",
    "                                                          column_index=7, # you have to check the text column index in your dataset\n",
    "                                                          batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e27710b6",
    "641886a2",
    "199b0f97",
    "29efd365-020c-47f7-b0d2-80927f4751fc"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ddf376336fb42acbcdba242de2f431f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1eae6dccb2ff420e8c29504cc14eb36f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "482c3d0da9f8439ba580a9a099c88371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "545f06ebfe884b69811c23284555f64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e419d5cbb4e44b1d839ae663c213bafa",
      "placeholder": "​",
      "style": "IPY_MODEL_9ab8460c49164a0391ca54c2b94c43fa",
      "value": " 1677/1677 [00:01&lt;00:00, 1466.87 examples/s]"
     }
    },
    "5677fa7431e84c7a941806cbbbc1a540": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "791d9505a60041a5836a911c34e3b81d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e70f844541d4ad3a4abdff94d0b66c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90ba4c84955640fe9d5ed37ba2b8532f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "939ca1bf528849f9a0646ed7c4f9c59e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94095cc31948429d8ef6d4450fc9a6b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ab8460c49164a0391ca54c2b94c43fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b17ff0391484da5afdf280c19fa98b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5677fa7431e84c7a941806cbbbc1a540",
      "placeholder": "​",
      "style": "IPY_MODEL_c46fb41481114f238b68067d772e0848",
      "value": "Map: 100%"
     }
    },
    "a369868d59a346db8db2b64a76630eef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c31dc00045fd49a9bc5e4d4096b4d8c7",
       "IPY_MODEL_e1c7ac8be8ea4e2c9bd92e11c6680aaa",
       "IPY_MODEL_aedbb97553524c94a4895361e5039369"
      ],
      "layout": "IPY_MODEL_791d9505a60041a5836a911c34e3b81d"
     }
    },
    "aedbb97553524c94a4895361e5039369": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ddf376336fb42acbcdba242de2f431f",
      "placeholder": "​",
      "style": "IPY_MODEL_939ca1bf528849f9a0646ed7c4f9c59e",
      "value": " 419/419 [00:00&lt;00:00, 633.22 examples/s]"
     }
    },
    "c31dc00045fd49a9bc5e4d4096b4d8c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1eae6dccb2ff420e8c29504cc14eb36f",
      "placeholder": "​",
      "style": "IPY_MODEL_90ba4c84955640fe9d5ed37ba2b8532f",
      "value": "Map: 100%"
     }
    },
    "c46fb41481114f238b68067d772e0848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c60a95f8d3614d8a9731d001bb44c930": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1c7ac8be8ea4e2c9bd92e11c6680aaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1e08ae4144d49229ec193dd1d1f5132",
      "max": 419,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e70f844541d4ad3a4abdff94d0b66c9",
      "value": 419
     }
    },
    "e419d5cbb4e44b1d839ae663c213bafa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7914fa91c67430f97058fd5100b70d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b17ff0391484da5afdf280c19fa98b7",
       "IPY_MODEL_eec7851037614f6ebc703bb58af51622",
       "IPY_MODEL_545f06ebfe884b69811c23284555f64c"
      ],
      "layout": "IPY_MODEL_482c3d0da9f8439ba580a9a099c88371"
     }
    },
    "eec7851037614f6ebc703bb58af51622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94095cc31948429d8ef6d4450fc9a6b2",
      "max": 1677,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c60a95f8d3614d8a9731d001bb44c930",
      "value": 1677
     }
    },
    "f1e08ae4144d49229ec193dd1d1f5132": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
