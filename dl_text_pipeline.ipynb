{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6a9a7f",
   "metadata": {
    "heading_collapsed": true,
    "id": "ec6a9a7f"
   },
   "source": [
    "# Important Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11341e77",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "11341e77"
   },
   "source": [
    "### Install Pytorch on windows:\n",
    "https://saturncloud.io/blog/how-to-install-pytorch-on-windows-using-conda/\n",
    "\n",
    "### Preprocessing:\n",
    "https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "\n",
    "### XAI:\n",
    "https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/bert-explainable-ai/\n",
    "https://towardsdatascience.com/introducing-transformers-interpret-explainable-ai-for-transformers-890a403a9470\n",
    "https://github.com/cdpierse/transformers-interpret\n",
    "https://levelup.gitconnected.com/huggingface-transformers-interpretability-with-captum-28e4ff4df234\n",
    "https://silviatulli.com/2021/11/02/explaining-the-outputs-of-transformers-models-a-working-example/\n",
    "https://brainsteam.co.uk/2022/03/14/painless-explainability-for-text-models-with-eli5/#eli5-and-transformershuggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffcd69",
   "metadata": {
    "id": "02ffcd69"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mEhD3fW_35X_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:11:14.971414Z",
     "start_time": "2023-10-16T14:11:14.959828Z"
    },
    "id": "mEhD3fW_35X_"
   },
   "outputs": [],
   "source": [
    "## install libraries\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate -U\n",
    "# !pip install -U xformers\n",
    "# !pip install datasets evaluate\n",
    "# !!pip install emoji\n",
    "# !pip install scikit-learn scipy matplotlib\n",
    "# !pip install openpyxl --upgrade\n",
    "# !pip install wordcloud\n",
    "# !pip install nltk\n",
    "# !pip install tweet-preprocessor\n",
    "# !pip install captum\n",
    "# !pip install transformers-interpret\n",
    "# !pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd917d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:30:04.693488Z",
     "start_time": "2023-10-20T09:30:04.592484Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1918,
     "status": "ok",
     "timestamp": 1697047339766,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": -60
    },
    "id": "81dd917d",
    "outputId": "0366e3ee-c535-4fd5-f7ef-a7931afef173",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# General\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import os\n",
    "from numpy.random import seed\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Modeling\n",
    "# import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, TextClassificationPipeline\n",
    "from transformers.pipelines import TextClassificationPipeline\n",
    "import torch\n",
    "torch.set_flush_denormal(True)\n",
    "\n",
    "# XAI\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "from transformers_interpret import SequenceClassificationExplainer, MultiLabelClassificationExplainer\n",
    "from eli5.lime import TextExplainer\n",
    "import eli5 \n",
    "\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model performance evaluation\n",
    "import evaluate\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "path = 'dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3685f1b",
   "metadata": {
    "id": "a3685f1b"
   },
   "source": [
    "# Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4a852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:40:36.059199Z",
     "start_time": "2023-10-19T11:40:35.717007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2767,
     "status": "ok",
     "timestamp": 1697047917324,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": -60
    },
    "id": "e3a4a852",
    "outputId": "1b91d96b-aaa1-4f03-e39f-c07083af61c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_type = 'racism'\n",
    "\n",
    "original_text_column = 'data_text'\n",
    "label_column = 'label'\n",
    "\n",
    "dataset_file = 'Racism.xlsx'\n",
    "\n",
    "df_hate_speech = pd.read_excel(path+dataset_file)[original_text_column]\n",
    "\n",
    "df_non_hate_speech = pd.read_csv(path+'non_hate_speech.csv')[original_text_column]\n",
    "\n",
    "df_non_hate_speech = shuffle(df_non_hate_speech, random_state=42)\n",
    "df_non_hate_speech = df_non_hate_speech.head(df_hate_speech.shape[0])\n",
    "\n",
    "label = [1]*df_non_hate_speech.shape[0] + [0]*df_non_hate_speech.shape[0]\n",
    "\n",
    "hate_speech_df = pd.concat([df_hate_speech, df_non_hate_speech]).to_frame()\n",
    "\n",
    "hate_speech_df.columns = [original_text_column]\n",
    "\n",
    "text_column = original_text_column\n",
    "\n",
    "hate_speech_df[label_column] = label\n",
    "\n",
    "hate_speech_df.dropna(subset=[original_text_column], inplace=True)\n",
    "\n",
    "hate_speech_df = shuffle(hate_speech_df, random_state=42)\n",
    "\n",
    "hate_speech_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hate_speech_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27710b6",
   "metadata": {
    "id": "e27710b6"
   },
   "source": [
    "# Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7e776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:40:41.688746Z",
     "start_time": "2023-10-19T11:40:41.527999Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1697047918566,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": -60
    },
    "id": "95d7e776",
    "outputId": "f646b2d8-ec75-4789-911a-d3349935377d"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Create a set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, df, text_column, label_column, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.processed_text_column = None\n",
    "        self.label_column = label_column\n",
    "        self.number_of_labels = len(df[label_column].value_counts())\n",
    "\n",
    "\n",
    "    def __demojize_text(self, text):\n",
    "        return emoji.demojize(text)\n",
    "\n",
    "    def __remove_words_with_euro(self, input_string):\n",
    "        # Define a regular expression pattern to match words containing 'euro'\n",
    "        pattern = r'\\b\\w*#?euro\\w*\\b'\n",
    "        # Use re.sub to replace matching words with an empty string\n",
    "        result = re.sub(pattern, '', input_string)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __remove_stop_words(self, sentence):\n",
    "        # Split the sentence into individual words\n",
    "        words = sentence.split()\n",
    "        # Use a list comprehension to remove stop words\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        # Join the filtered words back into a sentence\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def __preprocess_sentence(self, text, setup):\n",
    "\n",
    "        if setup['lower_case']:\n",
    "            text = text.lower()\n",
    "\n",
    "        if setup['remove_emojis']:\n",
    "            text = self.__demojize_text(text)\n",
    "\n",
    "        if setup['remove_stop_words']:\n",
    "            text = self.__remove_stop_words(text)\n",
    "\n",
    "        if setup['remove_numbers']:\n",
    "            text = text.replace('\\d+', '') # Removing numbers\n",
    "\n",
    "        # text = p.clean(text) #heavy cleaning\n",
    "        \n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            # t = remove_words_with_euro(t)\n",
    "\n",
    "            if setup['remove_users']:\n",
    "                t = '' if t.startswith('@') and len(t) > 1 else t\n",
    "                # t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            if setup['remove_urls']:\n",
    "                t = '' if t.startswith('http') else t\n",
    "                # t = 'http' if t.startswith('http') else t\n",
    "\n",
    "            new_text.append(t)\n",
    "\n",
    "        new_text = \" \".join(new_text)\n",
    "        \n",
    "        if setup['lemmatize']:\n",
    "            wnl = WordNetLemmatizer()\n",
    "            list2 = nltk.word_tokenize(new_text)\n",
    "            new_text = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    def get_text_column_name(self):\n",
    "        if self.processed_text_column:\n",
    "            return self.processed_text_column\n",
    "        else:\n",
    "            return self.text_column\n",
    "\n",
    "    def get_top_words(self, n=100):\n",
    "\n",
    "        temp_text_column = self.get_text_column_name()\n",
    "\n",
    "        # Combine all tweets into a single string\n",
    "        all_tweets = \" \".join(self.df[temp_text_column])\n",
    "\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(all_tweets)\n",
    "\n",
    "        # Remove stopwords and non-alphabetic words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "        # Calculate word frequencies\n",
    "        word_freq = Counter(words)\n",
    "\n",
    "        # Get the top n words\n",
    "        top_words_and_count = word_freq.most_common(n)\n",
    "        top_words = [word for word, counter in top_words_and_count]\n",
    "        counters = [counter for word, counter in top_words_and_count]\n",
    "\n",
    "        return {'words':top_words, 'counters':counters}\n",
    "\n",
    "\n",
    "    def get_top_words_tfidf(self, n):\n",
    "\n",
    "        temp_text_column = self.get_text_column_name()\n",
    "\n",
    "        # Create a TF-IDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=n, stop_words='english')\n",
    "\n",
    "        # Fit and transform the text data\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.df[temp_text_column])\n",
    "\n",
    "        # Get feature names (words)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Sum the TF-IDF scores for each word across all tweets\n",
    "        word_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "        # Sort words by their TF-IDF scores\n",
    "        top_indices = word_scores.argsort()[0, ::-1][:n]\n",
    "\n",
    "        # Get the top n words and their TF-IDF scores\n",
    "        top_words = [(feature_names[i], word_scores[0, i]) for i in top_indices]\n",
    "\n",
    "        return top_words[0][0][0]\n",
    "\n",
    "    def preprocess(self, setup):\n",
    "\n",
    "        self.df.dropna(subset=[self.text_column], inplace=True)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.processed_text_column = 'processed_'+self.text_column\n",
    "        self.df[self.processed_text_column] = self.df.apply(lambda x: self.__preprocess_sentence(x[self.text_column], setup), axis=1)\n",
    "\n",
    "        if setup['remove_non_text_characters']:\n",
    "            pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "            self.df[self.processed_text_column] = self.df.apply(lambda x: pattern.sub('', x[self.processed_text_column]), axis=1)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def unsample(self):\n",
    "\n",
    "        # temp_text_column = self.get_text_column_name()\n",
    "        # columns = [temp_text_column, self.label_column]\n",
    "\n",
    "        columns = [self.text_column, self.processed_text_column, self.label_column]\n",
    "\n",
    "        processed_df_grouped = self.df[columns].groupby(self.label_column)\n",
    "        processed_df_grouped.groups.values()\n",
    "\n",
    "        frames_of_groups = [x.sample(processed_df_grouped.size().min(), random_state=self.random_state) for y, x in processed_df_grouped]\n",
    "        self.df = pd.concat(frames_of_groups)\n",
    "\n",
    "        self.df = shuffle(self.df, random_state=self.random_state)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def split_train_test_dataset(self, train_size=0.8):\n",
    "        # Training dataset\n",
    "        train_data = self.df[[self.get_text_column_name(), self.label_column]].sample(frac=train_size, random_state=self.random_state)\n",
    "\n",
    "        # Testing dataset\n",
    "        test_data = self.df[[self.get_text_column_name(), self.label_column]].drop(train_data.index)\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': False,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "\n",
    "data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "# print(data_handler.get_top_words(100))\n",
    "# print(data_handler.get_top_words_tfidf(100))\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset()\n",
    "data_handler.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9981e",
   "metadata": {
    "id": "1da9981e"
   },
   "source": [
    "# Language Model Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642105b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:53:36.036873Z",
     "start_time": "2023-10-19T11:45:33.099759Z"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1697048886698,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": -60
    },
    "id": "d642105b"
   },
   "outputs": [],
   "source": [
    "class LanguageModelHandler():\n",
    "    def __init__(self, model_name, dataset_type, text_column, label_column):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.pipeline = None\n",
    "        self.num_labels = 0\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.dataset_type = dataset_type\n",
    "        self.create_tokenizer()\n",
    "\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "\n",
    "    def test_gpu(self):\n",
    "        print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        # Storing ID of current CUDA device\n",
    "        cuda_id = torch.cuda.current_device()\n",
    "        print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def __tokenize_dataset(self, data):\n",
    "        return self.tokenizer(data[self.text_column], max_length= 32,\n",
    "                              truncation=True,\n",
    "                              padding=\"max_length\")\n",
    "\n",
    "    def add_new_tokens_to_tokenizer(self, new_tokens):\n",
    "        if self.tokenizer is not None:\n",
    "            number_of_tokens_added = self.tokenizer.add_tokens(new_tokens=specific_words)\n",
    "\n",
    "            if self.model is not None:\n",
    "                print('### Resizing the model embeddings layer...')\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "            return number_of_tokens_added\n",
    "\n",
    "    def prepare_training_testing_datasets(self, train_data, test_data):\n",
    "        self.hg_train_data = Dataset.from_pandas(train_data)\n",
    "        self.hg_test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "        self.num_labels =len(train_data[self.label_column].value_counts())\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        self.tokenized_dataset_train = self.hg_train_data.map(self.__tokenize_dataset)\n",
    "        self.tokenized_dataset_test = self.hg_test_data.map(self.__tokenize_dataset)\n",
    "\n",
    "        return self.tokenized_dataset_train, self.tokenized_dataset_test\n",
    "\n",
    "    def create_model(self):\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            id2label={0: 'non-'+self.dataset_type, 1:self.dataset_type})\n",
    "        except:\n",
    "            print('Error to import the model, ignore mismatched sizes')\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            ignore_mismatched_sizes=True,\n",
    "                                                                            id2label={0: 'non-'+self.dataset_type, 1:self.dataset_type})\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Function to compute the metric\n",
    "    def __compute_metrics(self, eval_pred):\n",
    "        metric_accuracy = evaluate.load(\"accuracy\")\n",
    "        metric_precision = evaluate.load(\"precision\")\n",
    "        metric_recall = evaluate.load(\"recall\")\n",
    "        metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "        logits, labels = eval_pred\n",
    "        # probabilities = tf.nn.softmax(logits)\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "        results = {\n",
    "            'accuracy': metric_accuracy.compute(predictions=predictions, references=labels),\n",
    "            'precision': metric_precision.compute(predictions=predictions, references=labels),\n",
    "            'recall': metric_recall.compute(predictions=predictions, references=labels),\n",
    "            'f1': metric_f1.compute(predictions=predictions, references=labels)\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def train_evaluate_model(self, training_args, early_stopping_patience, iterations):\n",
    "\n",
    "        results_summary = {}\n",
    "        detailed_metrics = ['eval_accuracy', 'eval_precision', 'eval_recall',  'eval_f1']\n",
    "\n",
    "        model = deepcopy(self.model)\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            train_dataset = self.tokenized_dataset_train,\n",
    "            eval_dataset = self.tokenized_dataset_test,\n",
    "            compute_metrics = self.__compute_metrics\n",
    "        )\n",
    "\n",
    "        if early_stopping_patience:\n",
    "            self.trainer.callbacks = [EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "        results = self.trainer.evaluate(self.tokenized_dataset_test)\n",
    "\n",
    "        for metric in results:\n",
    "            if metric not in results_summary:\n",
    "                if metric in detailed_metrics:\n",
    "                    results_summary[metric] = [results[metric][\"\".join(metric.split('eval_'))]]\n",
    "                else:\n",
    "                    results_summary[metric] = [results[metric]]\n",
    "            else:\n",
    "                if metric in detailed_metrics:\n",
    "                    results_summary[metric].append(results[metric][\"\".join(metric.split('eval_'))])\n",
    "                else:\n",
    "                    results_summary[metric].append(results[metric])\n",
    "        \n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        return results_summary, self.trainer\n",
    "\n",
    "    def __create_classification_column(self, df, classification_column='classification'):\n",
    "        # Add a new column 'classification' with 0 if 'non-sexist' has higher probability, else 1\n",
    "        df[classification_column] = df.apply(lambda row: 0 if row['non-'+self.dataset_type] > row[self.dataset_type] else 1, axis=1)\n",
    "        return df\n",
    "\n",
    "    def __data_loader(self, dataframe, column=1):\n",
    "        for row in dataframe.values:\n",
    "            yield row[column] # Getting the text of the tweet\n",
    "\n",
    "    def classify_unlabaled_datasets(self, dataset_name_file, result_file_name, batch_size_to_save):\n",
    "        \n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = pipeline('text-classification', model=self.model,\n",
    "                                     tokenizer=self.tokenizer, device=self.device)\n",
    "\n",
    "        df = pd.read_csv(dataset_name_file)#.head(4000)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        if os.path.isfile(result_file_name): # if the results file exists\n",
    "            df_results = pd.read_csv(result_file_name)\n",
    "            df = df.tail(df.shape[0] - df_results.shape[0])\n",
    "        else:\n",
    "            df_results = pd.DataFrame(columns=list(self.model.config.id2label.values()))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for prediction in tqdm(self.pipeline(self.__data_loader(df), batch_size=32, return_all_scores=True), total=df.shape[0]):\n",
    "            result = {\n",
    "                text_column: [df.iloc[i]['text']],\n",
    "                prediction[0]['label']: [prediction[0]['score']],\n",
    "                prediction[1]['label']: [prediction[1]['score']]\n",
    "            }\n",
    "\n",
    "            df_results = pd.concat([df_results, pd.DataFrame.from_dict(result)])\n",
    "\n",
    "            if i % batch_size_to_save == 0 and i > 0:\n",
    "                self.__create_classification_column(df_results, self.dataset_type).to_csv(result_file_name, index=False)\n",
    "            i += 1\n",
    "\n",
    "        self.__create_classification_column(df_results, self.dataset_type).to_csv(result_file_name, index=False)#['label_match'].value_counts()\n",
    "    \n",
    "    def predict_proba(self, texts_array):\n",
    "        \n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = pipeline('text-classification', model=self.model,\n",
    "                                     tokenizer=self.tokenizer, device=self.device)\n",
    "            \n",
    "        all_results = []\n",
    "        \n",
    "        for predictions in tqdm(self.pipeline(self.__data_loader(pd.DataFrame(texts_array), column=0), \n",
    "                                              batch_size=32, return_all_scores=True), \n",
    "                                total=len(texts_array)):\n",
    "        #for predictions in [{'label': 'non-racism', 'score': 0.44055721163749695}, {'label': 'racism', 'score': 0.5594428181648254}]:\n",
    "            all_results.append([prediction['score'] for prediction in predictions])\n",
    "        \n",
    "        return np.array(all_results)\n",
    "                            \n",
    "    \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "language_model_manager = LanguageModelHandler(model_name= 'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "                                              dataset_type=dataset_type,\n",
    "                                              text_column=data_handler.get_text_column_name(),\n",
    "                                              label_column=data_handler.label_column)\n",
    "\n",
    "print(language_model_manager.test_gpu())\n",
    "\n",
    "language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "language_model_manager.create_model()\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-6,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10,\n",
    "    # seed=42\n",
    ")\n",
    "\n",
    "results, trainer = language_model_manager.train_evaluate_model(training_args=training_args,\n",
    "                                                               early_stopping_patience=2,\n",
    "                                                               iterations=1) # '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641886a2",
   "metadata": {
    "id": "641886a2"
   },
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d90a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T12:33:11.182035Z",
     "start_time": "2023-10-19T12:11:59.362544Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1697047920686,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": -60
    },
    "id": "61d90a9a"
   },
   "outputs": [],
   "source": [
    "class ExplainableTransformerPipeline():\n",
    "    \"\"\"Wrapper for Captum framework usage with Huggingface Pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, pipeline_name='text-classification'):\n",
    "        \n",
    "        if 'Roberta' in model.__class__.__name__:\n",
    "            self.__name = 'roberta'\n",
    "        elif 'Bert' in model.__class__.__name__:\n",
    "            self.__name = 'bert'\n",
    "            \n",
    "        self.__pipeline = pipeline(pipeline_name, model=model, tokenizer=tokenizer, device=device)\n",
    "        self.__cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "        self.__device = device\n",
    "\n",
    "    def forward_func(self, inputs, position = 0):\n",
    "        \"\"\"\n",
    "            Wrapper around prediction method of pipeline\n",
    "        \"\"\"\n",
    "        pred = self.__pipeline.model(inputs, attention_mask=torch.ones_like(inputs))\n",
    "        return pred[position]\n",
    "\n",
    "    def visualize_word_importance_in_sentence(self, text:str):\n",
    "        \n",
    "        word_attributions = self.__cls_explainer(text)\n",
    "\n",
    "        print('Prediction:', self.__cls_explainer.predicted_class_name)\n",
    "        print('Words importance:', word_attributions)\n",
    "\n",
    "        self.__cls_explainer.visualize()\n",
    "\n",
    "\n",
    "    def visualize_word_importance(self, inputs: list, attributes: list, prediction:str):\n",
    "        \"\"\"\n",
    "            Visualization method.\n",
    "            Takes list of inputs and correspondent attributs for them to visualize in a barplot\n",
    "        \"\"\"\n",
    "        attr_sum = attributes.sum(-1)\n",
    "\n",
    "        attr = attr_sum / torch.norm(attr_sum)\n",
    "\n",
    "        word_importance = pd.Series(attr.cpu().numpy()[0],\n",
    "                         index = self.__pipeline.tokenizer.convert_ids_to_tokens(inputs.detach().cpu().numpy()[0],skip_special_tokens=False))\n",
    "\n",
    "        print(word_importance)\n",
    "\n",
    "        plt.title(prediction)\n",
    "        plt.show(word_importance.plot.barh(figsize=(10,20)))\n",
    "\n",
    "        return word_importance\n",
    "    \n",
    "    def __generate_inputs(self, text: str):\n",
    "        \"\"\"\n",
    "            Convenience method for generation of input ids as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.__pipeline.tokenizer.encode(text, add_special_tokens=False), \n",
    "                            device = self.__device).unsqueeze(0)\n",
    "    \n",
    "    def generate_baseline(self, sequence_len: int):\n",
    "        \"\"\"\n",
    "            Convenience method for generation of baseline vector as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor([self.__pipeline.tokenizer.cls_token_id] + [self.__pipeline.tokenizer.pad_token_id] * (sequence_len - 2) + [self.__pipeline.tokenizer.sep_token_id], device = self.__device).unsqueeze(0)\n",
    "\n",
    "    def __clean_text_for_explanation(self, text):\n",
    "        text = re.sub(r'(?<=:)\\s+|\\s+(?=:)', '', text)\n",
    "        text = emoji.emojize(text)\n",
    "        \n",
    "        regular_punct = list(string.punctuation) # python punctuations \n",
    "        special_punct = ['©', '^', '®',' ','¾', '¡','!'] # user defined special characters to remove \n",
    "        \n",
    "        for punc in regular_punct:\n",
    "            if punc in text:\n",
    "                text = text.replace(punc, ' ')\n",
    "                \n",
    "        return text.strip()\n",
    "        \n",
    "    ## LIME\n",
    "    def model_adapter(self, texts):\n",
    "    \n",
    "        all_scores = []\n",
    "        batch_size = 64\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "\n",
    "            batch = texts[i:i+batch_size]\n",
    "            \n",
    "            # use bert encoder to tokenize text \n",
    "            encoded_input = self.__pipeline.tokenizer(batch, \n",
    "                              return_tensors='pt', \n",
    "                              padding=True, \n",
    "                              truncation=True, \n",
    "                              max_length=self.__pipeline.model.config.max_position_embeddings-2)\n",
    "            \n",
    "            for key in encoded_input:\n",
    "                encoded_input[key] = encoded_input[key].to(self.__device)\n",
    "                \n",
    "            output = self.__pipeline.model(**encoded_input)\n",
    "            # by default this model gives raw logits rather \n",
    "            # than a nice smooth softmax so we apply it ourselves here\n",
    "            \n",
    "            scores = output[0].softmax(1).detach().cpu().numpy()\n",
    "\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "        return np.array(all_scores)\n",
    "            \n",
    "    \n",
    "    def get_most_impactful_words_lime(self, text, keyword, word_importance_results):\n",
    "        \n",
    "        prediction = self.__pipeline(text)[0]['label']\n",
    "        \n",
    "        if prediction == keyword:\n",
    "            print(text)\n",
    "            te = TextExplainer(n_samples=500, random_state=42)\n",
    "            te.fit(text, self.model_adapter)\n",
    "            \n",
    "            graphic_explanation = te.explain_prediction(target_names=list(self.__pipeline.model.config.id2label.values()))\n",
    "\n",
    "            print(graphic_explanation.targets)\n",
    "\n",
    "            for element in graphic_explanation.targets:\n",
    "                for f in element.feature_weights.pos:\n",
    "                    for word in f.feature.split():\n",
    "                        if word in word_importance_results:\n",
    "                            word_importance_results[word] += f.weight\n",
    "                        else:\n",
    "                            word_importance_results[word] = f.weight\n",
    "                return word_importance_results, graphic_explanation\n",
    "        else:\n",
    "            return word_importance_results, None\n",
    "\n",
    "    \n",
    "    \n",
    "    ## INTEGRATED GRADIENTS\n",
    "    def explain(self, text: str):\n",
    "        \"\"\"\n",
    "            Main entry method. Passes text through series of transformations and through the model.\n",
    "            Calls visualization method.\n",
    "        \"\"\"\n",
    "        prediction = self.__pipeline.predict(text)\n",
    "        inputs = self.__generate_inputs(text)\n",
    "        baseline = self.generate_baseline(sequence_len = inputs.shape[1])\n",
    "        \n",
    "        print('inputs', len(inputs[0]))\n",
    "        # print('se liga:', self.__pipeline.model.config.label2id)\n",
    "\n",
    "        lig = LayerIntegratedGradients(self.forward_func, \n",
    "                                       getattr(self.__pipeline.model, self.__name).embeddings)\n",
    "\n",
    "        # For some reason we need to swap the label dictionary\n",
    "        labels_swaped = {v: k for k, v in self.__pipeline.model.config.id2label.items()}\n",
    "\n",
    "        attributes, delta = lig.attribute(inputs=inputs,\n",
    "                                  baselines=baseline,\n",
    "                                  target=labels_swaped[prediction[0]['label']],\n",
    "                                  return_convergence_delta=True)\n",
    "\n",
    "        self.visualize_word_importance(inputs, attributes, prediction)\n",
    "\n",
    "\n",
    "    def join_tokens_into_words(self, token_tuples):\n",
    "        self.tokens_to_exclude = ['[CLS]', '[SEP]']\n",
    "        tokens_list = []\n",
    "        scores_list = []\n",
    "        \n",
    "        current_tokens_list = []\n",
    "        current_scores_list = []\n",
    "        \n",
    "        for i, (token, score) in enumerate(token_tuples):\n",
    "          if token in self.tokens_to_exclude:\n",
    "              continue\n",
    "              \n",
    "          if i < len(token_tuples)-1:\n",
    "            next_token = token_tuples[i+1][0]\n",
    "        \n",
    "            if '##' not in next_token and len(current_tokens_list) > 0:\n",
    "              current_tokens_list.append(token)\n",
    "              current_scores_list.append(score)\n",
    "        \n",
    "              tokens_list.append(current_tokens_list)\n",
    "              scores_list.append(current_scores_list)\n",
    "        \n",
    "              current_tokens_list = []\n",
    "              current_scores_list = []\n",
    "        \n",
    "            elif '##' not in next_token and len(current_tokens_list) == 0:\n",
    "              tokens_list.append([token])\n",
    "              scores_list.append([score])\n",
    "        \n",
    "            elif '##' in next_token:\n",
    "              current_tokens_list.append(token)\n",
    "              current_scores_list.append(score)\n",
    "        \n",
    "        last_token = token_tuples[-1][0]\n",
    "        last_score = token_tuples[-1][1]\n",
    "        \n",
    "        if '##' in last_token and last_token not in self.tokens_to_exclude:\n",
    "          tokens_list.append(current_tokens_list+[last_token])\n",
    "          scores_list.append(current_scores_list+[last_score])\n",
    "        \n",
    "        elif '##' not in last_token and last_token not in self.tokens_to_exclude:\n",
    "          tokens_list.append([last_token])\n",
    "          scores_list.append([last_score])\n",
    "\n",
    "        return tokens_list, scores_list\n",
    "        \n",
    "    \n",
    "    def __get_most_impactful_words_integrated_gradients(self, text_to_evaluate, threshold, keyword, results):\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text=text_to_evaluate)\n",
    "        # print(self.__cls_explainer.predicted_class_name)\n",
    "        tokens_list, scores_list = self.join_tokens_into_words(word_attributions)\n",
    "\n",
    "        new_word_attributions = []\n",
    "        for i, tokens in enumerate(tokens_list):\n",
    "            new_word_attributions.append((self.__pipeline.tokenizer.convert_tokens_to_string(tokens), np.mean(scores_list[i])))\n",
    "            \n",
    "        if self.__cls_explainer.predicted_class_name == keyword:\n",
    "\n",
    "            for word in new_word_attributions:\n",
    "                if word[1] > threshold:\n",
    "                    if word[0] in results:\n",
    "                        results[word[0]] += 1\n",
    "                    else:\n",
    "                        results[word[0]] = 1\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_vertical_bar(self, text, word_scores):\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "    \n",
    "        # Create a vertical bar plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        bars = ax.bar(words, word_scores, color='skyblue')\n",
    "\n",
    "        margin = 0.02\n",
    "    \n",
    "        for word, score, bar in zip(words, word_scores, bars):\n",
    "            if score >= 0:\n",
    "                ax.text(word, score + margin, f'{score:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "            else:\n",
    "                ax.text(word, score - margin, f'{score:.2f}', ha='center', va='top', fontsize=10)\n",
    "    \n",
    "        # Rotate word labels by 45 degrees for readability\n",
    "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "    \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Words')\n",
    "        ax.set_ylabel('Word Impact Scores')\n",
    "        # ax.set_title('Word Scores Vertical Bar Plot')\n",
    "    \n",
    "        # Adjust the position of the y-axis labels\n",
    "        ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "        ax.set_ylim([np.min(word_scores)-0.1, np.max(word_scores)+0.1])\n",
    "    \n",
    "        # Display the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_colored_text(self, text, word_scores):\n",
    "\n",
    "        # Create a colormap based on the 'viridis' colormap\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        # cmap = plt.get_cmap('viridis')\n",
    "    \n",
    "        # Normalize word scores to the range [0, 1]\n",
    "        norm = plt.Normalize(min(word_scores), max(word_scores))\n",
    "    \n",
    "        # Create a color map using the normalized scores and the colormap\n",
    "        mappable = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        mappable.set_array([])\n",
    "    \n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "    \n",
    "        # Calculate the horizontal spacing between words\n",
    "        total_word_count = len(words)\n",
    "        spacing = 1.0 / total_word_count\n",
    "    \n",
    "        # Create a figure and axis for the text\n",
    "        fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    \n",
    "        for i, (word, score) in enumerate(zip(words, word_scores)):\n",
    "            color = cmap(norm(score))\n",
    "            x_position = i * spacing\n",
    "            ax.text(x_position, 0, word, color=color, fontsize=12, ha='center', rotation=45)\n",
    "    \n",
    "        # Add a color scale just above the text\n",
    "        colorbar = ColorbarBase(ax=fig.add_axes([0.2, 0.8, 0.6, 0.02]),\n",
    "                                cmap=cmap,\n",
    "                                norm=norm,\n",
    "                                orientation='horizontal')\n",
    "        colorbar.set_label('Word Impact Scores')\n",
    "    \n",
    "        # Remove the axis and display the plot\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    def plot_word_importance(self, sentence, bar=True):\n",
    "\n",
    "        sentence = self.__clean_text_for_explanation(sentence)\n",
    "        \n",
    "        word_attributions = self.__cls_explainer(text=sentence)\n",
    "        tokens_list, scores_list = self.join_tokens_into_words(word_attributions)\n",
    "\n",
    "        scores_list = [np.mean(scores) for scores in scores_list]\n",
    "        if bar:\n",
    "            self.plot_vertical_bar(sentence, scores_list)\n",
    "        else:\n",
    "            self.plot_colored_text(sentence, scores_list)\n",
    "            # self.plot_sentence(sentence)\n",
    "        \n",
    "        \n",
    "    def get_most_impactful_words_for_dataset(self, dataset, column_text, \n",
    "                                             threshold, keyword, method, n=20):\n",
    "        results = {}\n",
    "        \n",
    "        i = 0\n",
    "        for index, row in dataset.iterrows():\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print('Processing:', i)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "            text = self.__clean_text_for_explanation(row[column_text])\n",
    "            \n",
    "            if method == 'integrated_gradients':\n",
    "                results = self.__get_most_impactful_words_integrated_gradients(text_to_evaluate=text, \n",
    "                                                            threshold=threshold,\n",
    "                                                            results=results,\n",
    "                                                            keyword=keyword)\n",
    "            elif method == 'lime':\n",
    "                results, graphic_explanation = self.get_most_impactful_words_lime(\n",
    "                                                                    text=text, \n",
    "                                                                    keyword=keyword,\n",
    "                                                                    word_importance_results=results)\n",
    "\n",
    "            # if i > 5:\n",
    "            #     break\n",
    "        \n",
    "        return pd.DataFrame([(key, value) for key, value in dict(sorted(results.items(), key=lambda item: item[1], reverse=True)).items()], columns=['word','frequency']).head(n)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------\n",
    "exp_model = ExplainableTransformerPipeline(model=language_model_manager.trainer.model,\n",
    "                                           tokenizer=language_model_manager.tokenizer,\n",
    "                                           device=language_model_manager.device,\n",
    "                                           pipeline_name='text-classification')\n",
    "\n",
    "\n",
    "####### LIME\n",
    "# results_most_important_words = exp_model.get_most_impactful_words_for_dataset(dataset=test_data, \n",
    "#                                                                               column_text=data_handler.get_text_column_name(), \n",
    "#                                                                               threshold=0, keyword='racism', \n",
    "#                                                                               method='lime', n=100)\n",
    "\n",
    "\n",
    "## Using lime to plot the word importance for few samples\n",
    "# samples = test_data[test_data[label_column]==1].sample(n=3, random_state=42)\n",
    "\n",
    "# for sample in samples[data_handler.get_text_column_name()]:\n",
    "#     print('*** Sample:',sample)\n",
    "#     word_importance_results, graphic_explanation = exp_model.get_most_impactful_words_lime(sample, 'racism', {})\n",
    "#     print(word_importance_results)\n",
    "#     graphic_explanation\n",
    "\n",
    "\n",
    "####### INTEGRATED GRADIENTS\n",
    "\n",
    "# Using integrated gradients to plot the word importance for few samples\n",
    "samples = test_data[test_data[label_column]==1].sample(n=3,random_state=42)\n",
    "\n",
    "for sample in samples[data_handler.get_text_column_name()]:\n",
    "# for sample in samples[data_handler.text_column]:\n",
    "    print(sample)\n",
    "    # exp_model.explain(sample)\n",
    "    # exp_model.visualize_word_importance_in_sentence(sample)\n",
    "    exp_model.plot_word_importance(sample, bar=True)\n",
    "    \n",
    "\n",
    "# results = exp_model.get_most_impactful_words_for_dataset(dataset=test_data, \n",
    "#                                                column_text=data_handler.get_text_column_name(), \n",
    "#                                                threshold=0.1, \n",
    "#                                                keyword=dataset_type,\n",
    "#                                                method='integrated_gradients',\n",
    "#                                                n=50)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e846b-cb23-410d-a75e-a9bb6a6a5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of tuples\n",
    "token_tuples = [\n",
    "    ('I', 0.444),\n",
    "    ('am', 0.6533),\n",
    "    ('not', 0.2342),\n",
    "    ('anti', 0.333),\n",
    "    ('##va', 0.3674232),\n",
    "    ('##x', 0.145151)\n",
    "]\n",
    "\n",
    "token_tuples = [('euro', -0.04504043073408189), \n",
    "                ('##20', -0.0012444165141832979), \n",
    "                ('##20', 0.009583244537505661), \n",
    "                ('##fin', 0.049853830068387804), \n",
    "                ('##al', -0.019816686608616105),\n",
    "                ('I', 0.444),\n",
    "               ]\n",
    "\n",
    "\n",
    "\n",
    "word_list = []\n",
    "score_list = []\n",
    "\n",
    "current_word_list = None\n",
    "current_score_list = None\n",
    "\n",
    "for token, score in token_tuples:\n",
    "\n",
    "    if current_word_list == None: # first token\n",
    "        current_word_list = [token]\n",
    "        current_score_list = [score]\n",
    "    \n",
    "    elif '##' in token:\n",
    "    elif '##' not in token: \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "'''\n",
    "results_tokens = []  # To store the combined tokens\n",
    "results_scores = []  # To store the sum of scores for combined tokens\n",
    "\n",
    "# Initialize variables to track combined token and its score\n",
    "tokens_to_be_combined = []\n",
    "acculumated_score = 0\n",
    "previous_token = None\n",
    "previous_score = 0\n",
    "\n",
    "for token, score in token_tuples:\n",
    "\n",
    "    if previous_token is None: # first token\n",
    "        previous_score = score\n",
    "        previous_token = token\n",
    "        continue\n",
    "        \n",
    "    elif '##' in token:\n",
    "        tokens_to_be_combined.append(previous_token)\n",
    "        acculumated_score += previous_score\n",
    "        \n",
    "    elif '##' not in token and previous_token is not None:\n",
    "        if len(tokens_to_be_combined) == 0:\n",
    "            results_scores.append(previous_score)\n",
    "            \n",
    "        else:\n",
    "            results_scores.append(acculumated_score)\n",
    "            acculumated_score = 0\n",
    "            tokens_to_be_combined = []\n",
    "            \n",
    "    previous_score = score\n",
    "    previous_token = token\n",
    "    print('******', token, previous_score, acculumated_score)\n",
    "            \n",
    "            \n",
    "if '##' not in token:\n",
    "    results_scores.append(score)\n",
    "else:\n",
    "    results_scores.append(acculumated_score+score)\n",
    "    \n",
    "\n",
    "token_tuples = [token for token, score in token_tuples]\n",
    "combined_tokens = language_model_manager.tokenizer.convert_tokens_to_string(token_tuples)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b0f97",
   "metadata": {
    "heading_collapsed": true,
    "id": "199b0f97",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf39ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:12:13.071533Z",
     "start_time": "2023-10-16T14:11:34.390225Z"
    },
    "hidden": true,
    "id": "0fcf39ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ExperimentManager():\n",
    "    def __init__(self, data_handler, dataset_type):\n",
    "        self.data_handler = data_handler\n",
    "        self.dataset_type = dataset_type\n",
    "        self.metrics = ('eval_accuracy','eval_precision','eval_recall', 'eval_f1')\n",
    "\n",
    "    def start_experiment(self, experiment_design, preprocessing_setup):\n",
    "        data_handler.preprocess(setup=preprocessing_setup)\n",
    "        \n",
    "        train_data, test_data = self.data_handler.split_train_test_dataset()\n",
    "\n",
    "        if experiment_design['unsample']:\n",
    "            data_handler.unsample()\n",
    "        \n",
    "        experiment_results = {}\n",
    "        \n",
    "        for model_name in experiment_design['model_list']:\n",
    "            \n",
    "            print('----------------------------------------')\n",
    "            print('Training:', model_name)            \n",
    "\n",
    "            language_model_manager = LanguageModelHandler(model_name=model_name,\n",
    "                                              dataset_type=self.dataset_type,\n",
    "                                              text_column=self.data_handler.get_text_column_name(),\n",
    "                                              label_column=self.data_handler.label_column)\n",
    "\n",
    "            language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "            language_model_manager.create_model()\n",
    "\n",
    "            results, trainer = language_model_manager.train_evaluate_model(training_args=experiment_design['training_args'],\n",
    "                                                               early_stopping_patience=experiment_design['early_stopping_patience'],\n",
    "                                                               iterations=experiment_design['iterations'])\n",
    "            \n",
    "            df_results = pd.DataFrame()\n",
    "            df_results['Dataset'] = [self.dataset_type] * len(self.metrics)\n",
    "            df_results['Model'] = [model_name] * len(self.metrics)\n",
    "            df_results['Metric'] = [metric.replace('eval_', '').capitalize() for metric in self.metrics]\n",
    "            df_results['Value'] = [np.mean(results[k]) for k in self.metrics if k in results]\n",
    "            \n",
    "            experiment_results[model_name] = {'results':df_results, 'model':language_model_manager}\n",
    "            \n",
    "        return experiment_results\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': False,\n",
    "    'remove_emojis': False,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': False\n",
    "}\n",
    "\n",
    "# No preprocessing\n",
    "# preprocessing_setup = {key: False for key in preprocessing_setup}\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-6,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10,\n",
    "    # seed=42\n",
    ")\n",
    "\n",
    "experiment_design = {\n",
    "    'model_list': [\n",
    "#         'bert-base-uncased',\n",
    "#         'vinai/bertweet-base',\n",
    "        'cardiffnlp/twitter-roberta-base-offensive', # Offensive speech Roberta\n",
    "#         'Hate-speech-CNERG/dehatebert-mono-english' # Hate speech Roberta\n",
    "    ],\n",
    "    'unsample': True,\n",
    "    'early_stopping_patience': 2,\n",
    "    'training_args': training_args,\n",
    "    'iterations': 1\n",
    "\n",
    "}\n",
    "\n",
    "data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "experiment_manager = ExperimentManager(data_handler, dataset_type=dataset_type)\n",
    "results = experiment_manager.start_experiment(experiment_design, preprocessing_setup)\n",
    "\n",
    "results[experiment_design['model_list'][0]]['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21f6cc",
   "metadata": {
    "id": "6d21f6cc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Starting the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbc5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T15:59:28.403915Z",
     "start_time": "2023-10-16T15:59:11.039468Z"
    },
    "id": "d6bbc5a0",
    "outputId": "05da85e2-488a-4e5a-8264-125770a9fc29"
   },
   "outputs": [],
   "source": [
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': True,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True\n",
    "}\n",
    "\n",
    "'''\n",
    "data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "# print(data_handler.get_top_words(100))\n",
    "# print(data_handler.get_top_words_tfidf(100))\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset()\n",
    "\n",
    "language_model_manager = LanguageModelHandler(model_name='cardiffnlp/twitter-roberta-base-offensive',\n",
    "                                              dataset_type=dataset_type,\n",
    "                                              text_column=data_handler.get_text_column_name(),\n",
    "                                              label_column=data_handler.label_column)\n",
    "\n",
    "language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "language_model_manager.create_model()\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-6,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10,\n",
    "    # seed=42\n",
    ")\n",
    "\n",
    "results, trainer = language_model_manager.train_evaluate_model(training_args=training_args,\n",
    "                                                               early_stopping_patience=2,\n",
    "                                                               iterations=1) # '''\n",
    "\n",
    "# year = '2008'\n",
    "# dataset_path = 'dataset/euros_second/'\n",
    "# dataset_name_file = dataset_path+year+'.csv'\n",
    "# result_file_name = dataset_path+language_model_manager.dataset_type\n",
    "# result_file_name += '/'+year+'_'+language_model_manager.model_name.split('/')[-1]+'.csv'\n",
    "\n",
    "# language_model_manager.classify_unlabaled_datasets(dataset_name_file=dataset_name_file,\n",
    "#                                                    result_file_name=result_file_name,\n",
    "#                                                    batch_size_to_save=100)\n",
    "\n",
    "exp_model = ExplainableTransformerPipeline(model=language_model_manager.trainer.model,\n",
    "                                           tokenizer=language_model_manager.tokenizer,\n",
    "                                           device=language_model_manager.device,\n",
    "                                           pipeline_name='text-classification')\n",
    "\n",
    "samples = test_data[test_data[label_column]==1].sample(n=3,random_state=42)\n",
    "\n",
    "for sample in samples[data_handler.get_text_column_name()]:\n",
    "    print(sample)\n",
    "    exp_model.explain(sample)\n",
    "    exp_model.visualize_word_importance_in_sentence(sample)\n",
    "\n",
    "# results = exp_model.get_most_impactful_words_for_dataset(dataset=test_data, \n",
    "#                                                column_text=data_handler.get_text_column_name(), \n",
    "#                                                threshold=0.1, \n",
    "#                                                keyword=dataset_type,\n",
    "#                                                n=50)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3a872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T16:45:30.048389Z",
     "start_time": "2023-10-16T16:45:30.035532Z"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a3685f1b",
    "e27710b6",
    "1da9981e",
    "641886a2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
