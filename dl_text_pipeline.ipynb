{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6a9a7f",
   "metadata": {
    "heading_collapsed": true,
    "id": "ec6a9a7f"
   },
   "source": [
    "# Important Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11341e77",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "11341e77"
   },
   "source": [
    "### Install Pytorch on windows:\n",
    "https://saturncloud.io/blog/how-to-install-pytorch-on-windows-using-conda/\n",
    "\n",
    "### Preprocessing:\n",
    "https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "\n",
    "### XAI:\n",
    "https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/bert-explainable-ai/\n",
    "https://towardsdatascience.com/introducing-transformers-interpret-explainable-ai-for-transformers-890a403a9470\n",
    "https://github.com/cdpierse/transformers-interpret\n",
    "https://levelup.gitconnected.com/huggingface-transformers-interpretability-with-captum-28e4ff4df234\n",
    "https://silviatulli.com/2021/11/02/explaining-the-outputs-of-transformers-models-a-working-example/\n",
    "https://brainsteam.co.uk/2022/03/14/painless-explainability-for-text-models-with-eli5/#eli5-and-transformershuggingface\n",
    "\n",
    "### SBERT â€” Sentence-BERT:\n",
    "https://towardsdatascience.com/sbert-deb3d4aef8a4\n",
    "https://www.sbert.net/docs/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffcd69",
   "metadata": {
    "id": "02ffcd69"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mEhD3fW_35X_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:11:14.971414Z",
     "start_time": "2023-10-16T14:11:14.959828Z"
    },
    "id": "mEhD3fW_35X_"
   },
   "outputs": [],
   "source": [
    "# # install libraries\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "!pip install -U xformers\n",
    "!pip install datasets evaluate\n",
    "!pip install emoji\n",
    "!pip install scikit-learn scipy matplotlib\n",
    "!pip install openpyxl --upgrade\n",
    "!pip install wordcloud\n",
    "!pip install nltk\n",
    "!pip install tweet-preprocessor\n",
    "!pip install captum\n",
    "!pip install transformers-interpret\n",
    "# !pip install eli5\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_RJ49taRH8Ho",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30975,
     "status": "ok",
     "timestamp": 1706559487070,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "_RJ49taRH8Ho",
    "outputId": "d4f13840-a367-4e34-caa3-17e3517a9598"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# path = 'drive/My Drive/' # Colab\n",
    "path = '' # Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd917d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:30:04.693488Z",
     "start_time": "2023-10-20T09:30:04.592484Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27849,
     "status": "ok",
     "timestamp": 1706559803904,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "81dd917d",
    "outputId": "a8acf337-ab61-4f63-d9a8-631510c48aae"
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# General\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "from numpy.random import seed\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Modeling\n",
    "# import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline #, BertModel, BertTokenizer\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, TextClassificationPipeline\n",
    "from transformers.pipelines import TextClassificationPipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "torch.set_flush_denormal(True)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "# XAI\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "from transformers_interpret import SequenceClassificationExplainer, MultiLabelClassificationExplainer\n",
    "# from eli5.lime import TextExplainer\n",
    "# import eli5\n",
    "\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model performance evaluation\n",
    "import evaluate\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "print(plt.style.available)\n",
    "from wordcloud import WordCloud\n",
    "# plt.style.use(\"_classic_test_patch\")\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "# plt.style.use('seaborn-v0_8-talk')\n",
    "# plt.style.use(\"fivethirtyeight\")\n",
    "# plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glRaTvONHT-f",
   "metadata": {
    "id": "glRaTvONHT-f"
   },
   "source": [
    "# Update me =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r5nqU5lnHWQq",
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1706559916978,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "r5nqU5lnHWQq"
   },
   "outputs": [],
   "source": [
    "original_text_column = 'text'\n",
    "label_column = 'classification'\n",
    "dataset_type = 'consistency' #'accuracy' 'healthy' 'consistency'\n",
    "extension = '.csv'\n",
    "\n",
    "# Colab\n",
    "path = 'drive/My Drive/TikTok/'\n",
    "results_path = path+'results/chatGPT/jul_dec_2023/'+dataset_type+'_analysis_gpt_45/'\n",
    "results_file_name = path+'codes/classification/datasets/'+dataset_type+'_analysis_jul_dec_2023.csv'\n",
    "\n",
    "# local\n",
    "path = '' \n",
    "results_path = path+'datasets/'\n",
    "results_file_name = path+'datasets/'+dataset_type+'_analysis_jul_dec_2023.csv'\n",
    "\n",
    "replace_values={\n",
    "    'accuracy': {\n",
    "        'inaccurate': 0,\n",
    "        'accurate': 1,\n",
    "        'partially accurate': 2,\n",
    "        'uncertain': 3,\n",
    "    },\n",
    "    'healthy': {\n",
    "        'unhealthy':0,\n",
    "        'healthy': 1,\n",
    "        'partially healthy': 2,\n",
    "        'uncertain': 3,\n",
    "    },\n",
    "    'consistency': {\n",
    "        'not consistent': 0,\n",
    "        'consistent': 1,\n",
    "        'partially consistent': 2,\n",
    "        'uncertain': 3,\n",
    "    }\n",
    "}\n",
    "\n",
    "new_labels = {v: k for k, v in replace_values[dataset_type].items() if v in [0, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f95d3b-aa9f-4441-a6b3-5e12677cb2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all the datasets for visualization\n",
    "\n",
    "def compare_categories(df):\n",
    "    # Comparing classification_accuracy vs classification_healthy\n",
    "    df['acc_vs_healthy'] = df['classification_accuracy'] == df['classification_healthy']\n",
    "    \n",
    "    # Comparing classification_accuracy vs classification_consistency\n",
    "    df['acc_vs_consistency'] = df['classification_accuracy'] == df['classification_consistency']\n",
    "    \n",
    "    # Comparing classification_consistency vs classification_healthy\n",
    "    df['consistency_vs_healthy'] = df['classification_consistency'] == df['classification_healthy']\n",
    "\n",
    "    return compare_three_categories(df)\n",
    "\n",
    "def compare_three_categories(df):\n",
    "    # Create a new column to store the comparison result\n",
    "    df['acc_vs_consistency_vs_healthy'] = None\n",
    "\n",
    "    # Iterate over each row\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if all three columns have the same value\n",
    "        if row['classification_accuracy'] == row['classification_healthy'] == row['classification_consistency']:\n",
    "            df.at[index, 'acc_vs_consistency_vs_healthy'] = True\n",
    "        # Check if all three columns have different values\n",
    "        elif row['classification_accuracy'] != row['classification_healthy'] != row['classification_consistency']:\n",
    "            df.at[index, 'acc_vs_consistency_vs_healthy'] = False\n",
    "        else:\n",
    "            df.at[index, 'acc_vs_consistency_vs_healthy'] = False\n",
    "    \n",
    "    return df\n",
    "    \n",
    "all_datasets_types = ['accuracy', 'healthy', 'consistency']\n",
    "\n",
    "all_datasets_df = pd.DataFrame()\n",
    "\n",
    "for type in all_datasets_types:\n",
    "    if all_datasets_df.shape[0] == 0:\n",
    "        all_datasets_df = pd.read_csv(path+'datasets/'+type+'_analysis_jul_dec_2023.csv')\n",
    "        suffix_a = type\n",
    "        all_datasets_df['classification'] = all_datasets_df['classification'].replace(replace_values[type])\n",
    "    else:\n",
    "        temp_df = pd.read_csv(path+'datasets/'+type+'_analysis_jul_dec_2023.csv')\n",
    "\n",
    "        temp_df['classification'] = temp_df['classification'].replace(replace_values[type])\n",
    "        \n",
    "        suffix_b = type\n",
    "        all_datasets_df = pd.merge(all_datasets_df, temp_df, on='text', how='left', suffixes=('_'+suffix_a,'_'+suffix_b))\n",
    "        suffix_a = suffix_b\n",
    "\n",
    "all_datasets_df = all_datasets_df.loc[:, ~all_datasets_df.columns.str.contains('^Unnamed')]\n",
    "all_datasets_df.rename(columns={\"classification\": \"classification_consistency\",  \"explanation\":\"explanation_consistency\", \"id\":\"id_consistency\"}, inplace=True)\n",
    "\n",
    "# all_datasets_df.to_csv('comparison_classifications.csv', index=False)\n",
    "\n",
    "all_datasets_df = compare_categories(all_datasets_df)\n",
    "\n",
    "print('Comparison acc_vs_healthy:', (all_datasets_df['acc_vs_healthy'] == True).sum()/all_datasets_df.shape[0])\n",
    "print('Comparison acc_vs_consistency:', (all_datasets_df['acc_vs_consistency'] == True).sum()/all_datasets_df.shape[0])\n",
    "print('Comparison consistency_vs_healthy:', (all_datasets_df['consistency_vs_healthy'] == True).sum()/all_datasets_df.shape[0])\n",
    "print('Comparison acc_vs_consistency_vs_healthy:', (all_datasets_df['acc_vs_consistency_vs_healthy'] == True).sum()/all_datasets_df.shape[0])\n",
    "all_datasets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3685f1b",
   "metadata": {
    "id": "a3685f1b"
   },
   "source": [
    "# Reading dataset\n",
    "Here you have to create a code to read your dataset as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEOelVeODVHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3777,
     "status": "ok",
     "timestamp": 1706559939354,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "lEOelVeODVHZ",
    "outputId": "a4f128b2-e523-4d24-9564-d5816fb428ca"
   },
   "outputs": [],
   "source": [
    "# print(results_path)\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# for file_name in glob.glob(results_path+'*'+extension):\n",
    "#     if 'csv' in extension:\n",
    "#         temp_df = pd.read_csv(file_name)\n",
    "#     else:\n",
    "#         temp_df = pd.read_excel(file_name)\n",
    "#     df = pd.concat([df, temp_df])\n",
    "\n",
    "# df.drop_duplicates(subset=['text'], inplace=True)\n",
    "# print(df.shape)\n",
    "# df[label_column] = df[label_column].str.lower()\n",
    "# df[label_column] = df[label_column].str.strip()\n",
    "\n",
    "\n",
    "## Necessary because the consistency dataset has more samples than the other ones. \n",
    "## So we need to merge this dataset wit the accuracy one to check the similar texts betweet them.\n",
    "# df_temp = pd.read_csv(path+'codes/classification/datasets/accuracy_analysis_jul_dec_2023.csv')\n",
    "\n",
    "# df = pd.merge(df, df_temp, on='text', how='right', suffixes=('_consistency','_accuracy'))\n",
    "\n",
    "# df.drop(columns=['Unnamed: 0', 'explanation_accuracy', 'classification_accuracy'], inplace=True)\n",
    "# df.rename(columns={\"classification_consistency\": \"classification\", \"explanation_consistency\": \"explanation\"}, inplace=True)\n",
    "\n",
    "# df.to_csv(path+'codes/classification/datasets/'+dataset_type+'_analysis_jul_dec_2023.csv', index=True)\n",
    "\n",
    "print(results_file_name)\n",
    "df = pd.read_csv(results_file_name)\n",
    "\n",
    "df.replace(replace_values[dataset_type], inplace=True)\n",
    "\n",
    "df[label_column].value_counts()\n",
    "# df.to_csv('consistency_analysis.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4a852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:40:36.059199Z",
     "start_time": "2023-10-19T11:40:35.717007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1706560024865,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "e3a4a852",
    "outputId": "9b3fa6f6-5dca-4eb2-c142-f2b5ce9d5647"
   },
   "outputs": [],
   "source": [
    "# df.dropna(subset=[original_text_column], inplace=True)\n",
    "df.dropna(subset=[label_column, original_text_column], inplace=True)\n",
    "df.drop_duplicates(subset=[original_text_column], inplace=True)\n",
    "\n",
    "df.rename(columns={original_text_column:'text', label_column: 'labels'}, inplace=True)\n",
    "\n",
    "original_text_column = 'text'\n",
    "label_column = 'labels'\n",
    "\n",
    "df[label_column] = df[label_column].replace({2:1}) # converting partially \n",
    "\n",
    "df = df[df[label_column].isin([0, 1])]\n",
    "\n",
    "df[label_column] = df[label_column].astype(int)\n",
    "\n",
    "print(df[label_column].value_counts())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27710b6",
   "metadata": {
    "id": "e27710b6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7e776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:40:41.688746Z",
     "start_time": "2023-10-19T11:40:41.527999Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1706560030812,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "95d7e776",
    "outputId": "78bd8512-0b4c-4134-e70b-21fab2545a4f"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Create a set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, df, text_column, label_column, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.processed_text_column = None\n",
    "        self.label_column = label_column\n",
    "        self.number_of_labels = len(df[label_column].value_counts())\n",
    "\n",
    "\n",
    "    def __demojize_text(self, text):\n",
    "        return emoji.demojize(text)\n",
    "\n",
    "    def __remove_words_with_euro(self, input_string):\n",
    "        # Define a regular expression pattern to match words containing 'euro'\n",
    "        pattern = r'\\b\\w*#?euro\\w*\\b'\n",
    "        # Use re.sub to replace matching words with an empty string\n",
    "        result = re.sub(pattern, '', input_string)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __remove_stop_words(self, sentence):\n",
    "        # Split the sentence into individual words\n",
    "        words = sentence.split()\n",
    "        # Use a list comprehension to remove stop words\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        # Join the filtered words back into a sentence\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def __preprocess_sentence(self, text, setup):\n",
    "\n",
    "        if setup['lower_case']:\n",
    "            text = text.lower()\n",
    "\n",
    "        if setup['remove_emojis']:\n",
    "            text = self.__demojize_text(text)\n",
    "\n",
    "        if setup['remove_stop_words']:\n",
    "            text = self.__remove_stop_words(text)\n",
    "\n",
    "        if setup['remove_numbers']:\n",
    "            text = text.replace('\\d+', '') # Removing numbers\n",
    "\n",
    "        # text = p.clean(text) #heavy cleaning\n",
    "\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            # t = remove_words_with_euro(t)\n",
    "\n",
    "            if setup['remove_users']:\n",
    "                t = '' if t.startswith('@') and len(t) > 1 else t\n",
    "                # t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            if setup['remove_urls']:\n",
    "                t = '' if t.startswith('http') else t\n",
    "                # t = 'http' if t.startswith('http') else t\n",
    "\n",
    "            new_text.append(t)\n",
    "\n",
    "        new_text = \" \".join(new_text)\n",
    "\n",
    "        if setup['lemmatize']:\n",
    "            wnl = WordNetLemmatizer()\n",
    "            list2 = nltk.word_tokenize(new_text)\n",
    "            new_text = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    def get_text_column_name(self):\n",
    "        if self.processed_text_column:\n",
    "            return self.processed_text_column\n",
    "        else:\n",
    "            return self.text_column\n",
    "\n",
    "    def get_top_words(self, n=100):\n",
    "\n",
    "        temp_text_column = self.get_text_column_name()\n",
    "\n",
    "        # Combine all tweets into a single string\n",
    "        all_tweets = \" \".join(self.df[temp_text_column])\n",
    "\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(all_tweets)\n",
    "\n",
    "        # Remove stopwords and non-alphabetic words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "        # Calculate word frequencies\n",
    "        word_freq = Counter(words)\n",
    "\n",
    "        # Get the top n words\n",
    "        top_words_and_count = word_freq.most_common(n)\n",
    "        top_words = [word for word, counter in top_words_and_count]\n",
    "        counters = [counter for word, counter in top_words_and_count]\n",
    "\n",
    "        return {'words':top_words, 'counters':counters}\n",
    "\n",
    "\n",
    "    def get_top_words_tfidf(self, n):\n",
    "\n",
    "        temp_text_column = self.get_text_column_name()\n",
    "\n",
    "        # Create a TF-IDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=n, stop_words='english')\n",
    "\n",
    "        # Fit and transform the text data\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.df[temp_text_column])\n",
    "\n",
    "        # Get feature names (words)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Sum the TF-IDF scores for each word across all tweets\n",
    "        word_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "        # Sort words by their TF-IDF scores\n",
    "        top_indices = word_scores.argsort()[0, ::-1][:n]\n",
    "\n",
    "        # Get the top n words and their TF-IDF scores\n",
    "        top_words = [(feature_names[i], word_scores[0, i]) for i in top_indices]\n",
    "\n",
    "        return top_words[0][0][0]\n",
    "\n",
    "    def preprocess(self, setup):\n",
    "\n",
    "        self.df.dropna(subset=[self.text_column], inplace=True)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.processed_text_column = 'processed_'+self.text_column\n",
    "        self.df[self.processed_text_column] = self.df.apply(lambda x: self.__preprocess_sentence(x[self.text_column], setup), axis=1)\n",
    "\n",
    "        if setup['remove_non_text_characters']:\n",
    "            pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "            self.df[self.processed_text_column] = self.df.apply(lambda x: pattern.sub('', x[self.processed_text_column]), axis=1)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def unsample(self):\n",
    "\n",
    "        # temp_text_column = self.get_text_column_name()\n",
    "        # columns = [temp_text_column, self.label_column]\n",
    "\n",
    "        columns = [self.text_column, self.processed_text_column, self.label_column]\n",
    "\n",
    "        processed_df_grouped = self.df[columns].groupby(self.label_column)\n",
    "        processed_df_grouped.groups.values()\n",
    "\n",
    "        frames_of_groups = [x.sample(processed_df_grouped.size().min(), random_state=self.random_state) for y, x in processed_df_grouped]\n",
    "        self.df = pd.concat(frames_of_groups)\n",
    "\n",
    "        self.df = shuffle(self.df, random_state=self.random_state)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def split_train_test_dataset(self, train_size=0.8):\n",
    "        # Training dataset\n",
    "        train_data = self.df[[self.get_text_column_name(), self.label_column]].sample(frac=train_size, random_state=self.random_state)\n",
    "\n",
    "        # Testing dataset\n",
    "        test_data = self.df[[self.get_text_column_name(), self.label_column]].drop(train_data.index)\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': False,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "\n",
    "data_handler = DataHandler(df=df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "# print(data_handler.get_top_words(100))\n",
    "# print(data_handler.get_top_words_tfidf(100))\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset()\n",
    "# # data_handler.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd72a0a-7fa5-4ec2-9c74-e151cce982ad",
   "metadata": {
    "id": "fbd72a0a-7fa5-4ec2-9c74-e151cce982ad",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65569ad9-fcdd-48cc-84a3-6937ae49a0e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1706560031840,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "65569ad9-fcdd-48cc-84a3-6937ae49a0e2"
   },
   "outputs": [],
   "source": [
    "class ExploratoryDataAnalysis():\n",
    "\n",
    "    def plot_text_size_distribution(self, dataframe, column_name):\n",
    "        # Extract text from the specified column\n",
    "        texts = dataframe[column_name]\n",
    "\n",
    "        # Calculate text lengths\n",
    "        text_lengths = [len(str(text)) for text in texts]\n",
    "\n",
    "        # Plot histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(text_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.title('Text Size Histogram')\n",
    "        plt.xlabel('Text Size')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def generate_word_cloud(self, dataframe, column_name):\n",
    "        # Concatenate all texts in the specified column\n",
    "        text_corpus = ' '.join(dataframe[column_name].astype(str))\n",
    "\n",
    "        # Generate the word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_corpus)\n",
    "\n",
    "        # Plot the word cloud\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c4d48-05d8-4dde-aaf0-49a0695288c8",
   "metadata": {
    "id": "501c4d48-05d8-4dde-aaf0-49a0695288c8"
   },
   "source": [
    "# Language Model (manual loop) Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d2669-a93e-40bc-8242-00b1d32a6c82",
   "metadata": {
    "id": "cc7d2669-a93e-40bc-8242-00b1d32a6c82",
    "outputId": "f8d74b68-3f3e-4adc-d7d7-91c86d6cd86f"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613\n",
    "# https://www.intodeeplearning.com/bert-multiclass-text-classification/\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    # preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    # labels_flat = labels.flatten()\n",
    "    # return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "\n",
    "def accuracy_score_func(preds, labels):\n",
    "    # preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    # labels_flat = labels.flatten()\n",
    "    # return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "class MultiClassLanguageModelHandler():\n",
    "\n",
    "    def __init__(self, model_name, new_labels, text_column, label_column, batch_size=32, text_size_limit=512):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.pipeline = None\n",
    "        self.zero_shot_pipeline = None\n",
    "        self.num_labels = 0\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.new_labels = new_labels\n",
    "        self.text_size_limit = text_size_limit\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.create_tokenizer()\n",
    "\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "\n",
    "    def test_gpu(self):\n",
    "        print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        # Storing ID of current CUDA device\n",
    "        cuda_id = torch.cuda.current_device()\n",
    "        print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def __tokenize_dataset(self, data):\n",
    "\n",
    "        '''\n",
    "        This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "        '''\n",
    "        encoded_dict = self.tokenizer.batch_encode_plus(data, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                                                   return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "    def add_new_tokens_to_tokenizer(self, new_tokens):\n",
    "        if self.tokenizer is not None:\n",
    "            number_of_tokens_added = self.tokenizer.add_tokens(new_tokens=new_tokens)\n",
    "\n",
    "            if self.model is not None:\n",
    "                print('### Resizing the model embeddings layer...')\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "            return number_of_tokens_added\n",
    "\n",
    "    def prepare_dataset(self, data):\n",
    "        self.num_labels = len(train_data[self.label_column].value_counts())\n",
    "        \n",
    "        input_ids, att_masks = self.__tokenize_dataset(data[self.text_column].to_list())     \n",
    "        y = torch.LongTensor(data[self.label_column].to_list())\n",
    "        \n",
    "        #move on device (GPU)\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        att_masks = att_masks.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        \n",
    "        dataset = TensorDataset(input_ids, att_masks, y)\n",
    "        sampler = RandomSampler(dataset)\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        print('Creating model:', self.model_name)\n",
    "        print('Device:', self.device)\n",
    "        print('Number of labels:', self.num_labels)\n",
    "\n",
    "        dropout_value = 0.1\n",
    "\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            output_attentions=False,\n",
    "                                                                            output_hidden_states=True,\n",
    "                                                                            id2label=self.new_labels,\n",
    "                                                                            hidden_dropout_prob=dropout_value)\n",
    "        except:\n",
    "            print('Error to import the model, ignore mismatched sizes')\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            output_attentions=False,\n",
    "                                                                            output_hidden_states=True,\n",
    "                                                                            ignore_mismatched_sizes=True,\n",
    "                                                                            id2label=self.new_labels,\n",
    "                                                                            hidden_dropout_prob=dropout_value)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Function to compute the metric\n",
    "    def __compute_metrics(self, eval_pred):\n",
    "        metric_accuracy = evaluate.load(\"accuracy\")\n",
    "        metric_precision = evaluate.load(\"precision\")\n",
    "        metric_recall = evaluate.load(\"recall\")\n",
    "        metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "        logits, labels = eval_pred\n",
    "        # probabilities = tf.nn.softmax(logits)\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "        results = {\n",
    "            'accuracy': metric_accuracy.compute(predictions=predictions, references=labels),\n",
    "            'precision': metric_precision.compute(predictions=predictions, references=labels),\n",
    "            'recall': metric_recall.compute(predictions=predictions, references=labels),\n",
    "            'f1': metric_f1.compute(predictions=predictions, references=labels)\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def train_evaluate_model(self, training_args, early_stopping_patience, iterations):\n",
    "        seed_val = training_args['seed']\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "        dataloader_train = self.prepare_dataset(training_args['dataset_train'])\n",
    "        dataloader_test = self.prepare_dataset(training_args['dataset_test'])\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "        epochs = training_args['epochs']\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=training_args['learning_rate']) # eps=training_args['eps']\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                     num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "        loss_function = training_args['loss_function']\n",
    "\n",
    "        train_loss_per_epoch = []\n",
    "        test_loss_per_epoch = []\n",
    "\n",
    "\n",
    "        for epoch_num in range(epochs):\n",
    "            print('Epoch: ', epoch_num + 1)\n",
    "            '''\n",
    "            Training\n",
    "            '''\n",
    "            self.model.train()\n",
    "\n",
    "            train_loss = 0\n",
    "\n",
    "            for step_num, batch_data in enumerate(tqdm(dataloader_train,desc='Training')):\n",
    "\n",
    "                input_ids, att_mask, labels = [data for data in batch_data] # data.to(self.device)\n",
    "\n",
    "                # output = self.model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "                output = self.model(input_ids=input_ids, attention_mask=att_mask)\n",
    "\n",
    "                loss = loss_function(output.logits, labels.long())\n",
    "                # loss = output.loss\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Backward pass\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # clip_grad_norm_(parameters=self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                del loss\n",
    "\n",
    "            train_loss_per_epoch.append(train_loss / (step_num + 1))\n",
    "\n",
    "            '''\n",
    "            Test\n",
    "            '''\n",
    "            self.model.eval()\n",
    "\n",
    "            test_loss = 0\n",
    "            test_pred = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for step_num_e, batch_data in enumerate(tqdm(dataloader_test,desc='Testing')):\n",
    "                    input_ids, att_mask, labels = [data.to(self.device) for data in batch_data] # moving data to GPU, when it's available\n",
    "                    output = self.model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "                    loss = output.loss\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    test_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "\n",
    "            test_loss_per_epoch.append(test_loss / (step_num_e + 1))\n",
    "            test_pred = np.concatenate(test_pred)\n",
    "\n",
    "            '''\n",
    "            Loss message\n",
    "            '''\n",
    "            print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(training_args['dataset_train']) / self.batch_size), train_loss / (step_num + 1)))\n",
    "            print(\"{0}/{1} Test loss: {2} \".format(step_num_e+1, math.ceil(len(training_args['dataset_test']) / self.batch_size), test_loss / (step_num_e + 1)))\n",
    "\n",
    "            print('F1 score:', f1_score_func(test_pred, training_args['dataset_test'][self.label_column].to_numpy()))\n",
    "            print('Accuracy:', accuracy_score_func(test_pred, training_args['dataset_test'][self.label_column].to_numpy()))\n",
    "\n",
    "            # print(classification_report(test_pred, training_args['dataset_test'][self.label_column].to_numpy(), target_names=list(self.new_labels.values())))\n",
    "\n",
    "        epochs = range(1, epochs +1 )\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(epochs, train_loss_per_epoch,label ='Training loss')\n",
    "        ax.plot(epochs, test_loss_per_epoch, label = 'Test loss' )\n",
    "        ax.set_title('Training and Test loss')\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, path, name_file):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(path+name_file)\n",
    "        # Save model\n",
    "        self.trainer.save_model(path+name_file)\n",
    "\n",
    "    def load_model(self, path, name_file):\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path+name_file)\n",
    "        # Load Model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path+name_file)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self.tokenizer, self.model\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "'''\n",
    "# Cleaning cache from GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "data_handler = DataHandler(df=df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': True,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "# exploratory_data_analysis = ExploratoryDataAnalysis()\n",
    "# exploratory_data_analysis.plot_text_size_distribution(data_handler.df, data_handler.get_text_column_name())\n",
    "# exploratory_data_analysis.generate_word_cloud(data_handler.df, data_handler.get_text_column_name())\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset(train_size=0.8)\n",
    "\n",
    "language_model_manager = MultiClassLanguageModelHandler(model_name= 'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "                                              text_column=data_handler.get_text_column_name(),\n",
    "                                              label_column=data_handler.label_column,\n",
    "                                              new_labels=new_labels)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = {\n",
    "    'learning_rate':1e-6,\n",
    "    # 'eps':1e-8,\n",
    "    'loss_function': nn.CrossEntropyLoss(),\n",
    "    'dataset_train':train_data,\n",
    "    'dataset_test':test_data,\n",
    "    'epochs':15,\n",
    "    'seed':42\n",
    "}\n",
    "\n",
    "language_model_manager.train_evaluate_model(training_args=training_args, early_stopping_patience=2, iterations=1) # '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda99fbb-7baa-47f3-8dcb-a61cf6aadf59",
   "metadata": {
    "id": "eda99fbb-7baa-47f3-8dcb-a61cf6aadf59",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Language Model (Huggingface based) Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642105b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T11:53:36.036873Z",
     "start_time": "2023-10-19T11:45:33.099759Z"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1706565112269,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "d642105b"
   },
   "outputs": [],
   "source": [
    "class HuggingfaceLanguageModelHandler():\n",
    "    def __init__(self, model_name, new_labels, text_column, label_column, text_size_limit=512):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.pipeline = None\n",
    "        self.zero_shot_pipeline = None\n",
    "        self.num_labels = 0\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.new_labels = new_labels\n",
    "        self.text_size_limit = text_size_limit\n",
    "        self.create_tokenizer()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "\n",
    "    def test_gpu(self):\n",
    "        print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        # Storing ID of current CUDA device\n",
    "        cuda_id = torch.cuda.current_device()\n",
    "        print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def __tokenize_dataset(self, data):\n",
    "        return self.tokenizer(data[self.text_column],\n",
    "                              truncation=True,\n",
    "                              # padding=True,\n",
    "                              # return_tensors='pt',\n",
    "                              max_length= 42, # 32\n",
    "                              padding=\"max_length\"\n",
    "                             )\n",
    "\n",
    "    def add_new_tokens_to_tokenizer(self, new_tokens):\n",
    "        if self.tokenizer is not None:\n",
    "            number_of_tokens_added = self.tokenizer.add_tokens(new_tokens=new_tokens)\n",
    "\n",
    "            if self.model is not None:\n",
    "                print('### Resizing the model embeddings layer...')\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "            return number_of_tokens_added\n",
    "\n",
    "    def prepare_training_testing_datasets(self, train_data, test_data):\n",
    "        self.hg_train_data = Dataset.from_pandas(train_data)\n",
    "        self.hg_test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "        self.num_labels = len(train_data[self.label_column].value_counts())\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        self.tokenized_dataset_train = self.hg_train_data.map(self.__tokenize_dataset)\n",
    "        self.tokenized_dataset_test = self.hg_test_data.map(self.__tokenize_dataset)\n",
    "\n",
    "        return self.tokenized_dataset_train, self.tokenized_dataset_test\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            id2label=self.new_labels,\n",
    "                                                                            output_hidden_states=True,\n",
    "                                                                            output_attentions=False)\n",
    "        except:\n",
    "            print('Error to import the model, ignore mismatched sizes')\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,\n",
    "                                                                            num_labels=self.num_labels,\n",
    "                                                                            ignore_mismatched_sizes=True,\n",
    "                                                                            id2label=self.new_labels,\n",
    "                                                                            output_hidden_states=True,\n",
    "                                                                            output_attentions=False)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Function to compute the metric\n",
    "    def __compute_metrics(self, eval_pred):\n",
    "        metric_accuracy = evaluate.load(\"accuracy\")\n",
    "        metric_precision = evaluate.load(\"precision\")\n",
    "        metric_recall = evaluate.load(\"recall\")\n",
    "        metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "        print(eval_pred)\n",
    "        \n",
    "        logits, labels = eval_pred\n",
    "        # probabilities = tf.nn.softmax(logits)\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "        results = {\n",
    "            'accuracy': metric_accuracy.compute(predictions=predictions, references=labels),\n",
    "            'precision': metric_precision.compute(predictions=predictions, references=labels, average='weighted'),\n",
    "            'recall': metric_recall.compute(predictions=predictions, references=labels, average='weighted'),\n",
    "            'f1': metric_f1.compute(predictions=predictions, references=labels, average='weighted')\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def train_evaluate_model(self, training_args, loss_function, early_stopping_patience, iterations):\n",
    "\n",
    "        results_summary = {}\n",
    "        detailed_metrics = ['eval_accuracy', 'eval_precision', 'eval_recall',  'eval_f1']\n",
    "\n",
    "        model = deepcopy(self.model)\n",
    "\n",
    "        self.trainer = MyTrainer( # Trainer(\n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            train_dataset = self.tokenized_dataset_train,\n",
    "            eval_dataset = self.tokenized_dataset_test,\n",
    "            compute_metrics = self.__compute_metrics,\n",
    "            loss_function = loss_function\n",
    "        )\n",
    "\n",
    "        if early_stopping_patience:\n",
    "            self.trainer.callbacks = [EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "        results = self.trainer.evaluate(self.tokenized_dataset_test)\n",
    "\n",
    "        for metric in results:\n",
    "            if metric not in results_summary:\n",
    "                if metric in detailed_metrics:\n",
    "                    results_summary[metric] = [results[metric][\"\".join(metric.split('eval_'))]]\n",
    "                else:\n",
    "                    results_summary[metric] = [results[metric]]\n",
    "            else:\n",
    "                if metric in detailed_metrics:\n",
    "                    results_summary[metric].append(results[metric][\"\".join(metric.split('eval_'))])\n",
    "                else:\n",
    "                    results_summary[metric].append(results[metric])\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        return results_summary, self.trainer\n",
    "\n",
    "    def __create_classification_column(self, df, classification_column='classification'):\n",
    "        # Add a new column 'classification' with 0 if 'non-sexist' has higher probability, else 1\n",
    "        df[classification_column] = df.apply(lambda row: 0 if row['non-'+self.dataset_type] > row[self.dataset_type] else 1, axis=1)\n",
    "        return df\n",
    "\n",
    "    def __data_loader(self, dataframe, column=1):\n",
    "        for row in dataframe.values:\n",
    "            text = row[column] # Getting the text of the tweet\n",
    "\n",
    "            if len(text.split()) > self.text_size_limit:\n",
    "                yield text.split()[:self.text_size_limit]\n",
    "            else:\n",
    "                yield text\n",
    "\n",
    "    def classify_unlabaled_datasets(self, dataset_name_file, result_file_name, batch_size_to_save, column_index=1):\n",
    "\n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = pipeline('text-classification', model=self.model,\n",
    "                                     tokenizer=self.tokenizer, device=self.device)\n",
    "\n",
    "        df = pd.read_csv(dataset_name_file)#.head(4000)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        if os.path.isfile(result_file_name): # if the results file exists\n",
    "            df_results = pd.read_csv(result_file_name)\n",
    "            df = df.tail(df.shape[0] - df_results.shape[0])\n",
    "        else:\n",
    "            df_results = pd.DataFrame(columns=list(self.model.config.id2label.values()))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for prediction in tqdm(self.pipeline(self.__data_loader(df, column_index), batch_size=32, return_all_scores=True), total=df.shape[0]):\n",
    "            result = {\n",
    "                self.text_column: [df.iloc[i]['text']],\n",
    "                prediction[0]['label']: [prediction[0]['score']],\n",
    "                prediction[1]['label']: [prediction[1]['score']]\n",
    "            }\n",
    "\n",
    "            df_results = pd.concat([df_results, pd.DataFrame.from_dict(result)])\n",
    "\n",
    "            if i % batch_size_to_save == 0 and i > 0:\n",
    "                self.__create_classification_column(df_results, self.dataset_type).to_csv(result_file_name, index=False)\n",
    "            i += 1\n",
    "\n",
    "        self.__create_classification_column(df_results, self.dataset_type).to_csv(result_file_name, index=False)#['label_match'].value_counts()\n",
    "\n",
    "    def predict_probability(self, texts_array):\n",
    "\n",
    "        if self.pipeline is None:\n",
    "            self.pipeline = pipeline('text-classification', model=self.model,\n",
    "                                     tokenizer=self.tokenizer, device=self.device)\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for predictions in tqdm(self.pipeline(self.__data_loader(pd.DataFrame(texts_array), column=0),\n",
    "                                              batch_size=32, return_all_scores=True),\n",
    "                                total=len(texts_array)):\n",
    "        #for predictions in [{'label': 'non-racism', 'score': 0.44055721163749695}, {'label': 'racism', 'score': 0.5594428181648254}]:\n",
    "            all_results.append([prediction['score'] for prediction in predictions])\n",
    "\n",
    "        return np.array(all_results)\n",
    "\n",
    "    def zero_shot_classification(self, sentence, labels, model_name=None):\n",
    "        if self.zero_shot_pipeline is None:\n",
    "\n",
    "            if model_name is None:\n",
    "                model_name = self.model_name\n",
    "\n",
    "            self.zero_shot_pipeline = pipeline(\"zero-shot-classification\", model=model_name, device=self.device)\n",
    "\n",
    "        return self.zero_shot_pipeline(sentence, labels)\n",
    "\n",
    "    def zero_shot_classification_dataframe(self, dataframe, labels, model_name=None, results_file_name=None,\n",
    "                                           batch_size=1000, column_index=1):\n",
    "\n",
    "        if results_file_name and os.path.isfile(results_file_name):\n",
    "            results_df = pd.read_csv(results_file_name)\n",
    "            dataframe = dataframe.tail(dataframe.shape[0]-results_df.shape[0]) # getting the last rows that were not collected yet\n",
    "\n",
    "        else:\n",
    "            results_df = pd.DataFrame(columns=['text']+labels)\n",
    "\n",
    "        print('Classifying', dataframe.shape[0], 'tweets')\n",
    "\n",
    "        if self.zero_shot_pipeline is None:\n",
    "            if model_name is None:\n",
    "                model_name = self.model_name\n",
    "            self.zero_shot_pipeline = pipeline(\"zero-shot-classification\", model=model_name, device=self.device)\n",
    "\n",
    "        i = results_df.shape[0]\n",
    "\n",
    "        for prediction in tqdm(self.zero_shot_pipeline(self.__data_loader(dataframe, column_index), labels,\n",
    "                                                       batch_size=32, return_all_scores=True),\n",
    "                                                       total=dataframe.shape[0]):\n",
    "            pred = {} # {'text': prediction['sequence']}\n",
    "\n",
    "            for j in range(len(prediction['labels'])):\n",
    "                pred[prediction['labels'][j]] = prediction['scores'][j]\n",
    "\n",
    "            results_df = pd.concat([results_df, pd.DataFrame([pred])], ignore_index=True)\n",
    "\n",
    "            if i % batch_size == 0 and results_file_name is not None:\n",
    "                results_df.to_csv(results_file_name, index=False)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        results_df['text'] = dataframe.iloc[:, column_index]\n",
    "\n",
    "        if results_file_name:\n",
    "            results_df.to_csv(results_file_name, index=False)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    def sentences_to_embedding_standard(self, sentences, model_names=None):\n",
    "\n",
    "        embeddings_results = {}\n",
    "\n",
    "        if model_names is None:\n",
    "            model_names = [self.model_name]\n",
    "\n",
    "        for model_name in model_names:\n",
    "\n",
    "            model = SentenceTransformer(model_name_or_path=model_name, device='cpu') #  device='gpu'\n",
    "\n",
    "            embeddings = model.encode(sentences)\n",
    "            embeddings_results[model_name] = embeddings\n",
    "\n",
    "        return embeddings_results\n",
    "\n",
    "    def sentences_to_embedding_fine_tuning(self, sentences, model_name_list, model_list, tokenizer_list):\n",
    "        # tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        # model = RobertaModel.from_pretrained('roberta-base')\n",
    "        # tweets = [\"Replace me by any text you'd like in this sentence.\",\n",
    "        #           \"Replace me by any text you'd like in this sentence2.\"]\n",
    "\n",
    "        fine_tuning_pipeline = pipeline(\"feature-extraction\", tokenizer=tokenizer_list[0], model=model_list[0])\n",
    "        column_index = 0\n",
    "\n",
    "        for prediction in tqdm(fine_tuning_pipeline(self.__data_loader(sentences, column_index), batch_size=32, return_all_scores=True),\n",
    "                               total=sentences.shape[0]):\n",
    "            pass\n",
    "\n",
    "        # embeddings_results = {}\n",
    "\n",
    "        # for model_name, model, tokenizer in zip(model_name_list, model_list, tokenizer_list):\n",
    "        #     encoded_inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        #     encoded_inputs.to(self.device)\n",
    "\n",
    "        #     with torch.no_grad():\n",
    "        #         output = model(**encoded_inputs)\n",
    "        #         embeddings = output.logits # pooler_output\n",
    "        #         reduced_embeddings = self.reduce_embeddings_dimentionality(embeddings)\n",
    "        #         embeddings_results[model_name] = (reduced_embeddings)\n",
    "\n",
    "        # return embeddings_results\n",
    "\n",
    "    def reduce_embeddings_dimentionality(self, embeddings, algorithm='PCA'):\n",
    "        n_components = 2\n",
    "\n",
    "        if algorithm == 'PCA':\n",
    "            dim_reduction_obj = PCA(n_components=n_components)\n",
    "\n",
    "        elif algorithm == 'TSNE':\n",
    "            dim_reduction_obj = TSNE(n_components=n_components, verbose=0, perplexity=40, n_iter=300)\n",
    "\n",
    "        elif algorithm == 'MDS':\n",
    "            dim_reduction_obj = MDS(n_components=n_components, metric=True, random_state=self.random_state)\n",
    "\n",
    "        return dim_reduction_obj.fit_transform(X)\n",
    "\n",
    "    def plot_embeddings(self, embeddings_results, labels, algorithm='PCA', all_together=False):\n",
    "\n",
    "        embeddings_df = pd.DataFrame()\n",
    "\n",
    "        for model_name in embeddings_results:\n",
    "            X = embeddings_results[model_name]\n",
    "\n",
    "            reduced_data =  self.reduce_embeddings_dimentionality(X, algorithm)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['model'] = [model_name]*X.shape[0]\n",
    "            df['model'] = df['model'].apply(lambda i: str(i))\n",
    "\n",
    "            df['label'] = labels\n",
    "\n",
    "            df['first_dimension'] = reduced_data[:,0]\n",
    "            df['second_dimension'] = reduced_data[:,1]\n",
    "\n",
    "            embeddings_df = pd.concat([embeddings_df, df])\n",
    "\n",
    "        if all_together:\n",
    "            self.plot_embbedings_together(embeddings_df)\n",
    "        else:\n",
    "            self.plot_embbedings_separated(embeddings_df)\n",
    "\n",
    "    def plot_embbedings_together(self, embeddings_df):\n",
    "        plt.figure(figsize=(16,10))\n",
    "\n",
    "        # Automatically assign colors and shapes\n",
    "        unique_models = embeddings_df['model'].unique()\n",
    "        unique_labels = embeddings_df['label'].unique()\n",
    "\n",
    "        color_dict = {model: plt.cm.tab10(i) for i, model in enumerate(unique_models)}\n",
    "        symbols = ['x', 'o'] #['v', '^', 's', 'D', 'o', '<', '>', 'p', '*']\n",
    "        shape_dict = {label: marker for label, marker in zip(unique_labels, symbols)}\n",
    "\n",
    "        # Scatter plot\n",
    "        for model, group_model in embeddings_df.groupby('model'):\n",
    "            for label, group_label in group_model.groupby('label'):\n",
    "                plt.scatter(group_label['first_dimension'], group_label['second_dimension'],\n",
    "                            label=f'{model} - {label}',\n",
    "                            color=color_dict.get(model, 'black'),\n",
    "                            marker=shape_dict.get(label, 'o'),\n",
    "                            alpha=0.3)\n",
    "\n",
    "        # Customize the plot\n",
    "        # plt.title('Scatter Plot with Models and Labels')\n",
    "        plt.xlabel('First Dimension')\n",
    "        plt.ylabel('Second Dimension')\n",
    "        plt.legend()\n",
    "        plt.grid(False)\n",
    "\n",
    "        # disabling xticks by Setting xticks to an empty list\n",
    "        plt.xticks([])\n",
    "\n",
    "        # disabling yticks by setting yticks to an empty list\n",
    "        plt.yticks([])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_embbedings_separated(self, embeddings_df):\n",
    "        # Automatically assign colors and shapes\n",
    "        unique_models = embeddings_df['model'].unique()\n",
    "        unique_labels = embeddings_df['label'].unique()\n",
    "\n",
    "        # color_dict = {model: plt.cm.tab10(i) for i, model in enumerate(unique_models)}\n",
    "        color_dict = {label: sns.color_palette(\"husl\", n_colors=len(unique_labels))[i] for i, label in enumerate(unique_labels)}\n",
    "        shape_dict = {label: marker for label, marker in zip(unique_labels, ['x', 'o', 's', 'D', 'v', '<', '>', 'p', '*'])}\n",
    "\n",
    "        # Set the size of each subplot\n",
    "        fig, axes = plt.subplots(1, len(embeddings_df['model'].unique()), figsize=(15, 5))  # Adjust the figsize as needed\n",
    "\n",
    "        # Create subplots for each model\n",
    "        for ax, model in zip(axes, embeddings_df['model'].unique()):\n",
    "            ax.set_title(model)\n",
    "\n",
    "            for label, group_label in embeddings_df[embeddings_df['model'] == model].groupby('label'):\n",
    "                ax.scatter(group_label['first_dimension'], group_label['second_dimension'],\n",
    "                          label=label,\n",
    "                          color=color_dict.get(label, 'black'),\n",
    "                          marker=shape_dict.get(label, 'o'))\n",
    "\n",
    "            ax.set_xlabel('First Dimension')\n",
    "            ax.set_ylabel('Second Dimension')\n",
    "            ax.legend()\n",
    "            ax.grid(False)\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # disabling xticks by Setting xticks to an empty list\n",
    "        plt.xticks([])\n",
    "\n",
    "        # disabling yticks by setting yticks to an empty list\n",
    "        plt.yticks([])\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, path, name_file):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(path+name_file)\n",
    "        # Save model\n",
    "        self.trainer.save_model(path+name_file)\n",
    "\n",
    "    def load_model(self, path, name_file):\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path+name_file)\n",
    "        # Load Model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path+name_file)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self.tokenizer, self.model\n",
    "\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, loss_function=None, **kwds):\n",
    "        super(MyTrainer, self).__init__(**kwds)\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        loss_fct = self.loss_function\n",
    "        labels = inputs.get('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# language_model_manager = HuggingfaceLanguageModelHandler(model_name= 'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "#                                               text_column=data_handler.get_text_column_name(),\n",
    "#                                               label_column=data_handler.label_column,\n",
    "#                                               new_labels=new_labels)\n",
    "\n",
    "\n",
    "# sentences = ['Impact of words for the sentence: If it werenâ€™t for the â€œniggersâ€ England wouldnâ€™t of got out of group stages. You lot are shite, be grateful \\#eng \\#Euro2020Final \\#euro2020',\n",
    "#              'Im in tears right now, we are in the final \\#EURO2020 \\#ENGDEN \\#ENG']\n",
    "\n",
    "# sentences = test_data[data_handler.get_text_column_name()].to_list()\n",
    "\n",
    "# embeddings = language_model_manager.sentences_to_embedding(sentences=sentences, model_name=None, pre_trained_model=None)\n",
    "\n",
    "# print(embeddings)\n",
    "\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=len(sentences)-1, n_iter=300)\n",
    "# tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "# print(tsne_results)\n",
    "\n",
    "# tsne_results_df = pd.DataFrame()\n",
    "\n",
    "# tsne_results_df['comp-1'] = tsne_results[:,0]\n",
    "# tsne_results_df['comp-2'] = tsne_results[:,1]\n",
    "\n",
    "# tsne_results_df['class'] = ['1', '2']\n",
    "\n",
    "# sns.scatterplot(x=\"comp-1\", y=\"comp-2\",\n",
    "#                 hue=tsne_results_df['class'].tolist(),\n",
    "#                 palette=sns.color_palette(\"hls\", 10),\n",
    "#                 data=tsne_results_df).set(title=\"Data T-SNE projection\")\n",
    "\n",
    "\n",
    "# # language_model_manager.zero_shot_classification(sentence='fuck off!', labels=['offensive', 'non-offensive'],\n",
    "# #                                                 model_name='facebook/bart-large-mnli')\n",
    "\n",
    "# language_model_manager.zero_shot_classification_dataframe(dataframe=data_handler.df.head(),\n",
    "#                                                           labels=['offensive', 'non-offensive'],\n",
    "#                                                           model_name='facebook/bart-large-mnli',\n",
    "#                                                           results_file_name='',\n",
    "#                                                           batch_size=1000)\n",
    "\n",
    "\n",
    "# print(language_model_manager.test_gpu())\n",
    "\n",
    "# language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "# language_model_manager.create_model()\n",
    "\n",
    "# # Set up training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "#     logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "#     logging_strategy='epoch',\n",
    "#     logging_steps=100,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     learning_rate=5e-6,\n",
    "#     save_strategy='epoch',\n",
    "#     save_steps=100,\n",
    "#     evaluation_strategy='epoch',\n",
    "#     eval_steps=100,\n",
    "#     load_best_model_at_end=True,\n",
    "#     num_train_epochs=10,\n",
    "#     # seed=42\n",
    "# )\n",
    "\n",
    "# results, trainer = language_model_manager.train_evaluate_model(training_args=training_args,\n",
    "#                                                                loss_function=nn.CrossEntropyLoss(), # nn.BCELoss()\n",
    "#                                                                early_stopping_patience=2,\n",
    "#                                                                iterations=1) # '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9981e",
   "metadata": {
    "id": "1da9981e"
   },
   "source": [
    "# Language Model (ML models) Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fa894-a18a-444d-a193-bb1304eb66be",
   "metadata": {
    "id": "e27fa894-a18a-444d-a193-bb1304eb66be",
    "outputId": "98be7df0-bffd-405e-e5ee-b28867352b0f"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/feature-extraction-with-bert-for-text-classification-533dde44dc2f\n",
    "# https://ddimri.medium.com/bert-for-classification-beyond-the-next-sentence-prediction-task-93acc1412749\n",
    "# https://towardsdatascience.com/a-beginners-guide-to-text-classification-with-scikit-learn-632357e16f3a#1629\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "\n",
    "class MachineLearningLanguageModelHandler():\n",
    "    def __init__(self, model_name, text_column, label_column, batch_size=32, text_size_limit=512, model=None):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.num_labels = 0\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.text_size_limit = text_size_limit\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tokenizer = self.create_tokenizer()\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def __tokenize_dataset(self, data):\n",
    "\n",
    "        '''\n",
    "        This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "        '''\n",
    "        encoded_dict = self.tokenizer.batch_encode_plus(data, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                                                   return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "        \n",
    "    # def __tokenize_dataset(self, data):\n",
    "\n",
    "    #     tokenized_data = self.tokenizer(data[self.text_column].values.tolist(), padding = True,\n",
    "    #                                     truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "    #     # move on device (GPU)\n",
    "    #     return {k:torch.tensor(v).to(self.device) for k,v in tokenized_data.items()}\n",
    "\n",
    "    def create_model(self):\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.model_name, output_hidden_states=True,\n",
    "                                                                 output_attentions=False).to(self.device)\n",
    "\n",
    "\n",
    "    def __data_loader(self, dataframe, column):\n",
    "        for row in dataframe.values:\n",
    "            text = row[column] # Getting the text of the tweet\n",
    "            \n",
    "            if len(text.split()) > self.text_size_limit:\n",
    "                yield text.split()[:self.text_size_limit]\n",
    "            else:\n",
    "                yield text    \n",
    "            yield text \n",
    "            \n",
    "    def generate_embeddings_pipeline(self, sentences_df, column_index):\n",
    "        fine_tuning_pipeline = pipeline(task=\"feature-extraction\", tokenizer=self.tokenizer, model=self.model, \n",
    "                                        device=self.device, max_length=self.text_size_limit, truncation=True, \n",
    "                                        padding=True, framework='pt')\n",
    "        \n",
    "        tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512,'return_tensors':'pt'}\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for embedding in tqdm(fine_tuning_pipeline(self.__data_loader(sentences_df, column_index), batch_size=self.batch_size), \n",
    "                              total=sentences_df.shape[0], **tokenizer_kwargs):            \n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "        return all_embeddings\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_dataset(self, data):\n",
    "\n",
    "        input_ids, att_masks = self.__tokenize_dataset(data[self.text_column].to_list())     \n",
    "        # y = torch.LongTensor(data[self.label_column].to_list())\n",
    "        \n",
    "        #move on device (GPU)\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        att_masks = att_masks.to(self.device)\n",
    "        # y = y.to(self.device)\n",
    "        \n",
    "        # dataset = TensorDataset(input_ids, att_masks, y)\n",
    "        dataset = TensorDataset(input_ids, att_masks)\n",
    "        sampler = RandomSampler(dataset)\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)\n",
    "\n",
    "        return data_loader\n",
    "         \n",
    "    def calculate_embeddings_local_model(self, data):\n",
    "        data_tokenized = self.__tokenize_dataset(data)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_states = self.model(**data_tokenized) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n",
    "\n",
    "        #get only the [CLS] hidden states\n",
    "        cls = hidden_states.last_hidden_state[:,0,:]\n",
    "\n",
    "        return cls\n",
    "\n",
    "    def calculate_embeddings_local_model_with_batches(self, original_data):\n",
    "        data_loader = self.prepare_dataset(data=original_data)\n",
    "\n",
    "        X = np.array([])\n",
    "        \n",
    "        output_class = 'hidden_states' # 'logits' \n",
    "        \n",
    "        for step_num, batch_data in enumerate(tqdm(data_loader, desc='Data')):\n",
    "\n",
    "                input_ids, att_mask = [data for data in batch_data] # data.to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model_output = self.model(input_ids=input_ids, attention_mask=att_mask)\n",
    "\n",
    "                    # Getting embeddings from the final BERT layer https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n",
    "                    \n",
    "                    # Removing the first hidden state\n",
    "                    # The first state is the input state\n",
    "                    token_embeddings = model_output[output_class][1:][-1]\n",
    "\n",
    "                    if X.shape[0] == 0:\n",
    "                        X = token_embeddings.cpu()\n",
    "                    else:\n",
    "                        X = np.concatenate((X, token_embeddings.cpu()), axis=0)\n",
    "        return X\n",
    "\n",
    "    def _reshape_dataset_for_ml(self, data):\n",
    "        nsamples, nx, ny = data.shape\n",
    "        return data.reshape((nsamples, nx*ny))\n",
    "        \n",
    "    def train_evaluate_model(self, training_args, iterations):\n",
    "        \n",
    "        X_train = self.calculate_embeddings_local_model_with_batches(training_args['dataset_train'])\n",
    "        X_test = self.calculate_embeddings_local_model_with_batches(training_args['dataset_test'])\n",
    "\n",
    "        X_train = self._reshape_dataset_for_ml(X_train)\n",
    "        X_test = self._reshape_dataset_for_ml(X_test)\n",
    "\n",
    "        print(X_train.shape, X_test.shape)\n",
    "        \n",
    "        y_train = training_args['dataset_train'][self.label_column]\n",
    "        y_test = training_args['dataset_test'][self.label_column]\n",
    "\n",
    "        if training_args['ml_model'].lower() == 'random forest':\n",
    "            ml_model = RandomForestClassifier(n_estimators=100, random_state=training_args['seed'])\n",
    "            \n",
    "        elif training_args['ml_model'].lower() == 'svm':\n",
    "            ml_model = SVC(kernel='linear')\n",
    "\n",
    "        elif training_args['ml_model'].lower() == 'decision tree':\n",
    "            ml_model = DecisionTreeClassifier()\n",
    "\n",
    "        elif training_args['ml_model'].lower() == 'naive bayes':\n",
    "            ml_model = GaussianNB()\n",
    "\n",
    "        elif training_args['ml_model'].lower() == 'logistic regression':\n",
    "            ml_model = LogisticRegression()\n",
    "\n",
    "        print('Training model...')\n",
    "        ml_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = ml_model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        # Generate a classification report\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    def load_model(self, path, name_file):\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path+name_file)\n",
    "        # Load Model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path+name_file)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "# -----------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# Cleaning cache from GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "data_handler = DataHandler(df=df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "preprocessing_setup = {\n",
    "    'lower_case': True,\n",
    "    'remove_emojis': True,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "# exploratory_data_analysis = ExploratoryDataAnalysis()\n",
    "# exploratory_data_analysis.plot_text_size_distribution(data_handler.df, data_handler.get_text_column_name())\n",
    "# exploratory_data_analysis.generate_word_cloud(data_handler.df, data_handler.get_text_column_name())\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset(train_size=0.8)\n",
    "\n",
    "\n",
    "ml_language_model_handler = MachineLearningLanguageModelHandler(model_name= 'bert-base-uncased',\n",
    "                                              text_column=data_handler.get_text_column_name(),\n",
    "                                              label_column=data_handler.label_column,\n",
    "                                              batch_size=64)\n",
    "\n",
    "ml_language_model_handler.load_model(path=path+'codes/classification/saved_models/', name_file='bert-base-uncased')\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = {\n",
    "    'dataset_train':train_data,\n",
    "    'dataset_test':test_data,\n",
    "    'ml_model': 'random forest', # 'random forest', 'svm', 'decision tree', 'naive bayes', 'logistic regression'\n",
    "    'seed':42\n",
    "}\n",
    "\n",
    "\n",
    "X_train = ml_language_model_handler.train_evaluate_model(training_args=training_args, iterations=1)\n",
    "# print(train_data.shape) '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641886a2",
   "metadata": {
    "id": "641886a2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d90a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T12:33:11.182035Z",
     "start_time": "2023-10-19T12:11:59.362544Z"
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1706565928841,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "61d90a9a"
   },
   "outputs": [],
   "source": [
    "class ExplainableTransformerPipeline():\n",
    "    \"\"\"Wrapper for Captum framework usage with Huggingface Pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, pipeline_name='text-classification'):\n",
    "\n",
    "        if 'Roberta' in model.__class__.__name__:\n",
    "            self.__name = 'roberta'\n",
    "        elif 'Bert' in model.__class__.__name__:\n",
    "            self.__name = 'bert'\n",
    "\n",
    "        self.__pipeline = pipeline(pipeline_name, model=model, tokenizer=tokenizer, device=device)\n",
    "        self.__cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "        self.__device = device\n",
    "\n",
    "    def forward_func(self, inputs, position = 0):\n",
    "        \"\"\"\n",
    "            Wrapper around prediction method of pipeline\n",
    "        \"\"\"\n",
    "        pred = self.__pipeline.model(inputs, attention_mask=torch.ones_like(inputs))\n",
    "        return pred[position]\n",
    "\n",
    "    def visualize_word_importance_in_sentence(self, text:str):\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text)\n",
    "\n",
    "        print('Prediction:', self.__cls_explainer.predicted_class_name)\n",
    "        print('Words importance:', word_attributions)\n",
    "\n",
    "        self.__cls_explainer.visualize()\n",
    "\n",
    "\n",
    "    def visualize_word_importance(self, inputs: list, attributes: list, prediction:str):\n",
    "        \"\"\"\n",
    "            Visualization method.\n",
    "            Takes list of inputs and correspondent attributs for them to visualize in a barplot\n",
    "        \"\"\"\n",
    "        attr_sum = attributes.sum(-1)\n",
    "\n",
    "        attr = attr_sum / torch.norm(attr_sum)\n",
    "\n",
    "        word_importance = pd.Series(attr.cpu().numpy()[0],\n",
    "                         index = self.__pipeline.tokenizer.convert_ids_to_tokens(inputs.detach().cpu().numpy()[0],skip_special_tokens=False))\n",
    "\n",
    "        print(word_importance)\n",
    "\n",
    "        plt.title(prediction)\n",
    "        plt.show(word_importance.plot.barh(figsize=(10,20)))\n",
    "\n",
    "        return word_importance\n",
    "\n",
    "    def __generate_inputs(self, text: str):\n",
    "        \"\"\"\n",
    "            Convenience method for generation of input ids as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.__pipeline.tokenizer.encode(text, add_special_tokens=False),\n",
    "                            device = self.__device).unsqueeze(0)\n",
    "\n",
    "    def generate_baseline(self, sequence_len: int):\n",
    "        \"\"\"\n",
    "            Convenience method for generation of baseline vector as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor([self.__pipeline.tokenizer.cls_token_id] + [self.__pipeline.tokenizer.pad_token_id] * (sequence_len - 2) + [self.__pipeline.tokenizer.sep_token_id], device = self.__device).unsqueeze(0)\n",
    "\n",
    "    def __clean_text_for_explanation(self, text):\n",
    "        text = re.sub(r'(?<=:)\\s+|\\s+(?=:)', '', text)\n",
    "        text = emoji.emojize(text)\n",
    "\n",
    "        regular_punct = list(string.punctuation) # python punctuations\n",
    "        special_punct = ['Â©', '^', 'Â®',' ','Â¾', 'Â¡','!'] # user defined special characters to remove\n",
    "\n",
    "        for punc in regular_punct:\n",
    "            if punc in text:\n",
    "                text = text.replace(punc, ' ')\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    ## LIME\n",
    "    def model_adapter(self, texts):\n",
    "\n",
    "        all_scores = []\n",
    "        batch_size = 64\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "\n",
    "            batch = texts[i:i+batch_size]\n",
    "\n",
    "            # use bert encoder to tokenize text\n",
    "            encoded_input = self.__pipeline.tokenizer(batch,\n",
    "                              return_tensors='pt',\n",
    "                              padding=True,\n",
    "                              truncation=True,\n",
    "                              max_length=self.__pipeline.model.config.max_position_embeddings-2)\n",
    "\n",
    "            for key in encoded_input:\n",
    "                encoded_input[key] = encoded_input[key].to(self.__device)\n",
    "\n",
    "            output = self.__pipeline.model(**encoded_input)\n",
    "            # by default this model gives raw logits rather\n",
    "            # than a nice smooth softmax so we apply it ourselves here\n",
    "\n",
    "            scores = output[0].softmax(1).detach().cpu().numpy()\n",
    "\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "        return np.array(all_scores)\n",
    "\n",
    "\n",
    "    def get_most_impactful_words_lime(self, text, keyword, word_importance_results):\n",
    "\n",
    "        prediction = self.__pipeline(text)[0]['label']\n",
    "\n",
    "        if prediction == keyword:\n",
    "            print(text)\n",
    "            te = TextExplainer(n_samples=500, random_state=42)\n",
    "            te.fit(text, self.model_adapter)\n",
    "\n",
    "            graphic_explanation = te.explain_prediction(target_names=list(self.__pipeline.model.config.id2label.values()))\n",
    "\n",
    "            print(graphic_explanation.targets)\n",
    "\n",
    "            for element in graphic_explanation.targets:\n",
    "                for f in element.feature_weights.pos:\n",
    "                    for word in f.feature.split():\n",
    "                        if word in word_importance_results:\n",
    "                            word_importance_results[word] += f.weight\n",
    "                        else:\n",
    "                            word_importance_results[word] = f.weight\n",
    "                return word_importance_results, graphic_explanation\n",
    "        else:\n",
    "            return word_importance_results, None\n",
    "\n",
    "\n",
    "\n",
    "    ## INTEGRATED GRADIENTS\n",
    "    def explain(self, text: str):\n",
    "        \"\"\"\n",
    "            Main entry method. Passes text through series of transformations and through the model.\n",
    "            Calls visualization method.\n",
    "        \"\"\"\n",
    "        prediction = self.__pipeline.predict(text)\n",
    "        inputs = self.__generate_inputs(text)\n",
    "        baseline = self.generate_baseline(sequence_len = inputs.shape[1])\n",
    "\n",
    "        print('inputs', len(inputs[0]))\n",
    "        # print('se liga:', self.__pipeline.model.config.label2id)\n",
    "\n",
    "        lig = LayerIntegratedGradients(self.forward_func,\n",
    "                                       getattr(self.__pipeline.model, self.__name).embeddings)\n",
    "\n",
    "        # For some reason we need to swap the label dictionary\n",
    "        labels_swaped = {v: k for k, v in self.__pipeline.model.config.id2label.items()}\n",
    "\n",
    "        attributes, delta = lig.attribute(inputs=inputs,\n",
    "                                  baselines=baseline,\n",
    "                                  target=labels_swaped[prediction[0]['label']],\n",
    "                                  return_convergence_delta=True)\n",
    "\n",
    "        self.visualize_word_importance(inputs, attributes, prediction)\n",
    "\n",
    "\n",
    "    def join_tokens_into_words(self, token_tuples):\n",
    "        self.tokens_to_exclude = ['[CLS]', '[SEP]']\n",
    "        tokens_list = []\n",
    "        scores_list = []\n",
    "\n",
    "        current_tokens_list = []\n",
    "        current_scores_list = []\n",
    "\n",
    "        for i, (token, score) in enumerate(token_tuples):\n",
    "          if token in self.tokens_to_exclude:\n",
    "              continue\n",
    "\n",
    "          if i < len(token_tuples)-1:\n",
    "            next_token = token_tuples[i+1][0]\n",
    "\n",
    "            if '##' not in next_token and len(current_tokens_list) > 0:\n",
    "              current_tokens_list.append(token)\n",
    "              current_scores_list.append(score)\n",
    "\n",
    "              tokens_list.append(current_tokens_list)\n",
    "              scores_list.append(current_scores_list)\n",
    "\n",
    "              current_tokens_list = []\n",
    "              current_scores_list = []\n",
    "\n",
    "            elif '##' not in next_token and len(current_tokens_list) == 0:\n",
    "              tokens_list.append([token])\n",
    "              scores_list.append([score])\n",
    "\n",
    "            elif '##' in next_token:\n",
    "              current_tokens_list.append(token)\n",
    "              current_scores_list.append(score)\n",
    "\n",
    "        last_token = token_tuples[-1][0]\n",
    "        last_score = token_tuples[-1][1]\n",
    "\n",
    "        if '##' in last_token and last_token not in self.tokens_to_exclude:\n",
    "          tokens_list.append(current_tokens_list+[last_token])\n",
    "          scores_list.append(current_scores_list+[last_score])\n",
    "\n",
    "        elif '##' not in last_token and last_token not in self.tokens_to_exclude:\n",
    "          tokens_list.append([last_token])\n",
    "          scores_list.append([last_score])\n",
    "\n",
    "        return tokens_list, scores_list\n",
    "\n",
    "\n",
    "    def __get_most_impactful_words_integrated_gradients(self, text_to_evaluate, threshold, keyword, results):\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text=text_to_evaluate)\n",
    "        # print(self.__cls_explainer.predicted_class_name)\n",
    "        tokens_list, scores_list = self.join_tokens_into_words(word_attributions)\n",
    "\n",
    "        new_word_attributions = []\n",
    "        for i, tokens in enumerate(tokens_list):\n",
    "            new_word_attributions.append((self.__pipeline.tokenizer.convert_tokens_to_string(tokens), np.mean(scores_list[i])))\n",
    "\n",
    "        if self.__cls_explainer.predicted_class_name == keyword:\n",
    "\n",
    "            for word in new_word_attributions:\n",
    "                if word[1] > threshold:\n",
    "                    if word[0] in results:\n",
    "                        results[word[0]] += 1\n",
    "                    else:\n",
    "                        results[word[0]] = 1\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_vertical_bar(self, text, word_scores):\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Create a vertical bar plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        bars = ax.bar(words, word_scores, color='#5B2C6F') # color='skyblue'\n",
    "\n",
    "        margin = 0.02\n",
    "\n",
    "        for word, score, bar in zip(words, word_scores, bars):\n",
    "            if score >= 0:\n",
    "                ax.text(word, score + margin, f'{score:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "            else:\n",
    "                ax.text(word, score - margin, f'{score:.2f}', ha='center', va='top', fontsize=10)\n",
    "\n",
    "        # Rotate word labels by 45 degrees for readability\n",
    "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Words')\n",
    "        ax.set_ylabel('Word Impact Scores')\n",
    "        # ax.set_title('Word Scores Vertical Bar Plot')\n",
    "\n",
    "        # Adjust the position of the y-axis labels\n",
    "        ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "        ax.set_ylim([np.min(word_scores)-0.1, np.max(word_scores)+0.1])\n",
    "\n",
    "        # plt.grid(True, color = \"grey\", linewidth = \"1\")\n",
    "        plt.axhline(y=0, color='black', linestyle='-', linewidth=\"0.5\")\n",
    "\n",
    "        # Display the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_colored_text(self, text, word_scores):\n",
    "\n",
    "        # Create a colormap based on the 'viridis' colormap\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        # cmap = plt.get_cmap('viridis')\n",
    "\n",
    "        # Normalize word scores to the range [0, 1]\n",
    "        norm = plt.Normalize(min(word_scores), max(word_scores))\n",
    "\n",
    "        # Create a color map using the normalized scores and the colormap\n",
    "        mappable = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        mappable.set_array([])\n",
    "\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Calculate the horizontal spacing between words\n",
    "        total_word_count = len(words)\n",
    "        spacing = 1.0 / total_word_count\n",
    "\n",
    "        # Create a figure and axis for the text\n",
    "        fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "        for i, (word, score) in enumerate(zip(words, word_scores)):\n",
    "            color = cmap(norm(score))\n",
    "            x_position = i * spacing\n",
    "            ax.text(x_position, 0, word, color=color, fontsize=12, ha='center', rotation=45)\n",
    "\n",
    "        # Add a color scale just above the text\n",
    "        colorbar = ColorbarBase(ax=fig.add_axes([0.2, 0.8, 0.6, 0.02]),\n",
    "                                cmap=cmap,\n",
    "                                norm=norm,\n",
    "                                orientation='horizontal')\n",
    "        colorbar.set_label('Word Impact Scores')\n",
    "\n",
    "        # Remove the axis and display the plot\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_word_importance(self, sentence, bar=True):\n",
    "\n",
    "        sentence = self.__clean_text_for_explanation(sentence)\n",
    "\n",
    "        word_attributions = self.__cls_explainer(text=sentence)\n",
    "\n",
    "        print(word_attributions)\n",
    "\n",
    "        tokens_list, scores_list = self.join_tokens_into_words(word_attributions)\n",
    "\n",
    "        scores_list = [np.mean(scores) for scores in scores_list]\n",
    "\n",
    "        print(scores_list)\n",
    "\n",
    "        if bar:\n",
    "            print(len(sentence.split()), len(scores_list))\n",
    "            self.plot_vertical_bar(sentence, scores_list)\n",
    "        else:\n",
    "            self.plot_colored_text(sentence, scores_list)\n",
    "            # self.plot_sentence(sentence)\n",
    "\n",
    "\n",
    "    def get_most_impactful_words_for_dataset(self, dataset, column_text,\n",
    "                                             threshold, keyword, method, n=20):\n",
    "        results = {}\n",
    "\n",
    "        i = 0\n",
    "        for index, row in dataset.iterrows():\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Processing:', i)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            text = self.__clean_text_for_explanation(row[column_text])\n",
    "\n",
    "            if method == 'integrated_gradients':\n",
    "                results = self.__get_most_impactful_words_integrated_gradients(text_to_evaluate=text,\n",
    "                                                            threshold=threshold,\n",
    "                                                            results=results,\n",
    "                                                            keyword=keyword)\n",
    "            elif method == 'lime':\n",
    "                results, graphic_explanation = self.get_most_impactful_words_lime(\n",
    "                                                                    text=text,\n",
    "                                                                    keyword=keyword,\n",
    "                                                                    word_importance_results=results)\n",
    "\n",
    "            # if i > 5:\n",
    "            #     break\n",
    "\n",
    "        return pd.DataFrame([(key, value) for key, value in dict(sorted(results.items(), key=lambda item: item[1], reverse=True)).items()], columns=['word','frequency']).head(n)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------\n",
    "# exp_model = ExplainableTransformerPipeline(model=language_model_manager.trainer.model,\n",
    "#                                            tokenizer=language_model_manager.tokenizer,\n",
    "#                                            device=language_model_manager.device,\n",
    "#                                            pipeline_name='text-classification')\n",
    "\n",
    "\n",
    "# ####### LIME\n",
    "# # results_most_important_words = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "# #                                                                               column_text=data_handler.get_text_column_name(),\n",
    "# #                                                                               threshold=0, keyword='racism',\n",
    "# #                                                                               method='lime', n=100)\n",
    "\n",
    "\n",
    "# ## Using lime to plot the word importance for few samples\n",
    "# # samples = test_data[test_data[label_column]==1].sample(n=3, random_state=42)\n",
    "\n",
    "# # for sample in samples[data_handler.get_text_column_name()]:\n",
    "# #     print('*** Sample:',sample)\n",
    "# #     word_importance_results, graphic_explanation = exp_model.get_most_impactful_words_lime(sample, 'racism', {})\n",
    "# #     print(word_importance_results)\n",
    "# #     graphic_explanation\n",
    "\n",
    "\n",
    "# ####### INTEGRATED GRADIENTS\n",
    "\n",
    "# # Using integrated gradients to plot the word importance for few samples\n",
    "# samples = test_data[test_data[label_column]==1].sample(n=3,random_state=42)\n",
    "\n",
    "# for sample in samples[data_handler.get_text_column_name()]:\n",
    "# # for sample in samples[data_handler.text_column]:\n",
    "#     print(sample)\n",
    "#     # exp_model.explain(sample)\n",
    "#     # exp_model.visualize_word_importance_in_sentence(sample)\n",
    "#     exp_model.plot_word_importance(sample, bar=True)\n",
    "\n",
    "\n",
    "# results = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "#                                                column_text=data_handler.get_text_column_name(),\n",
    "#                                                threshold=0.1,\n",
    "#                                                keyword=dataset_type,\n",
    "#                                                method='integrated_gradients',\n",
    "#                                                n=50)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b0f97",
   "metadata": {
    "heading_collapsed": true,
    "id": "199b0f97",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf39ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T14:12:13.071533Z",
     "start_time": "2023-10-16T14:11:34.390225Z"
    },
    "hidden": true,
    "id": "0fcf39ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ExperimentManager():\n",
    "    def __init__(self, data_handler, dataset_type):\n",
    "        self.data_handler = data_handler\n",
    "        self.dataset_type = dataset_type\n",
    "        self.metrics = ('eval_accuracy','eval_precision','eval_recall', 'eval_f1')\n",
    "\n",
    "    def start_experiment(self, experiment_design, preprocessing_setup):\n",
    "        data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "        train_data, test_data = self.data_handler.split_train_test_dataset()\n",
    "\n",
    "        if experiment_design['unsample']:\n",
    "            data_handler.unsample()\n",
    "\n",
    "        experiment_results = {}\n",
    "\n",
    "        for model_name in experiment_design['model_list']:\n",
    "\n",
    "            print('----------------------------------------')\n",
    "            print('Training:', model_name)\n",
    "\n",
    "            language_model_manager = LanguageModelHandler(model_name=model_name,\n",
    "                                              dataset_type=self.dataset_type,\n",
    "                                              text_column=self.data_handler.get_text_column_name(),\n",
    "                                              label_column=self.data_handler.label_column)\n",
    "\n",
    "            language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "            language_model_manager.create_model()\n",
    "\n",
    "            results, trainer = language_model_manager.train_evaluate_model(training_args=experiment_design['training_args'],\n",
    "                                                               early_stopping_patience=experiment_design['early_stopping_patience'],\n",
    "                                                               iterations=experiment_design['iterations'])\n",
    "\n",
    "            df_results = pd.DataFrame()\n",
    "            df_results['Dataset'] = [self.dataset_type] * len(self.metrics)\n",
    "            df_results['Model'] = [model_name] * len(self.metrics)\n",
    "            df_results['Metric'] = [metric.replace('eval_', '').capitalize() for metric in self.metrics]\n",
    "            df_results['Value'] = [np.mean(results[k]) for k in self.metrics if k in results]\n",
    "\n",
    "            experiment_results[model_name] = {'results':df_results, 'model':language_model_manager}\n",
    "\n",
    "        return experiment_results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# preprocessing_setup = {\n",
    "#     'lower_case': True,\n",
    "#     'remove_emojis': False,\n",
    "#     'remove_stop_words': True,\n",
    "#     'remove_numbers': False,\n",
    "#     'remove_users': True,\n",
    "#     'remove_urls': True,\n",
    "#     'remove_non_text_characters': True,\n",
    "#     'lemmatize': False\n",
    "# }\n",
    "\n",
    "# # No preprocessing\n",
    "# # preprocessing_setup = {key: False for key in preprocessing_setup}\n",
    "\n",
    "# # Set up training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "#     logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "#     logging_strategy='epoch',\n",
    "#     logging_steps=100,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     learning_rate=5e-6,\n",
    "#     save_strategy='epoch',\n",
    "#     save_steps=100,\n",
    "#     evaluation_strategy='epoch',\n",
    "#     eval_steps=100,\n",
    "#     load_best_model_at_end=True,\n",
    "#     num_train_epochs=10,\n",
    "#     # seed=42\n",
    "# )\n",
    "\n",
    "# experiment_design = {\n",
    "#     'model_list': [\n",
    "#         # 'bert-base-uncased',\n",
    "#         # 'vinai/bertweet-base',\n",
    "#         # 'cardiffnlp/twitter-roberta-base-offensive', # Offensive speech Roberta\n",
    "#         # 'Hate-speech-CNERG/dehatebert-mono-english' # Hate speech Roberta\n",
    "\n",
    "#         # 'Pablo94/racism-finetuned-detests-29-10-2022', ## Racism model\n",
    "#         'bitsanlp/Homophobia-Transphobia-v2-mBERT-EDA' ## Homophobia model\n",
    "#     ],\n",
    "#     'unsample': True,\n",
    "#     'early_stopping_patience': 2,\n",
    "#     'training_args': training_args,\n",
    "#     'iterations': 1\n",
    "# }\n",
    "\n",
    "# data_handler = DataHandler(df=hate_speech_df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "# experiment_manager = ExperimentManager(data_handler, dataset_type=dataset_type)\n",
    "# results = experiment_manager.start_experiment(experiment_design, preprocessing_setup)\n",
    "\n",
    "# results[experiment_design['model_list'][0]]['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21f6cc",
   "metadata": {
    "id": "6d21f6cc"
   },
   "source": [
    "# Starting the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbc5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T15:59:28.403915Z",
     "start_time": "2023-10-16T15:59:11.039468Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8f6b8462a92c46719fca37ea797e1a6e",
      "3beddb2505214571906342f86a4f2c31",
      "01191e6924074d50ab31a115d4d00725",
      "29e588ccd788493d902ec82b14c29d6f",
      "9702ff03a2aa43ac8de8b142eb10829c",
      "dfb979be7b154f7ba9d86b90a1d3da7a",
      "d5cd74e22aa14bca863d65a8e65a170f",
      "240c88c8d1c3443d9c5f08276f0520a0",
      "ea08c5b585ac4f95ac6a9d125525c0af",
      "ac7fe66be5cc44709cd33731597e047e",
      "027f75ad20b941c9a41e014fef8491c8",
      "270190e61d9c491d9634477c9562fb46",
      "6ef54acb0bb047fcb476c41f8a1febd2",
      "82ef318ca9854893a71c4f4cf694a21e",
      "c4112fc53e204047bbd9b1035f281a67",
      "918fdf95c77d4e59a4945e00001e27a2",
      "afa2336628bc485693b40506beb54b5f",
      "f8777307ee2849428929aeb749ec53a5",
      "92b0784e08114b028f6535dc22e20bc6",
      "4850905ac5c84579bca8e75d1c523870",
      "cc7e99709ccb4000a5dfc8b2e84d02e8",
      "88a3072fad0e46c5b2933291963885b3",
      "1bdc2c6a152a4b9babb01e190954d46e",
      "d84f8a5b080a481eb9c428dd58d37963",
      "aed6f9da358346d7aaba26754391b50e",
      "9bb79b9797924b468182b6dd98073c43",
      "0d5d3906e1e14c8380c4df09fbb517e3",
      "760ba7d6ebc544dfabdcee913300f363",
      "4486c2753bc042f8add62e6770aea5eb",
      "902133b09008442d83000637e5e48483",
      "72a02fe2e6b344489edaca34bb609f91",
      "4dcf20473bed4fadb9d4cdf2fb769f29",
      "04d06da0ca164b988bb69497a35e8f90",
      "06426f374ada423782f9d533cd9efed8",
      "bbf02b8c1d79482c99c0d45af36d53ad",
      "22cba3d483374ce1b1d0df30ee36655b",
      "0c3742de71c646388d2111b90c2ddbf6",
      "dac62e5b9b08421283d1d2a9def525e0",
      "aa99317f99f1435784239a844ba6997a",
      "bba64555be7140ea83f06291377a00e3",
      "46dd385929c44483a715afd1434b2fc9",
      "cc0667791e754667a68bb284ad918c75",
      "86c6d04b972b4ef788bf39f76d61dc3f",
      "2b8ba5dbe5d4426693cd433ad9dbc8eb",
      "fd7ce92728054cfca73e0ef8e5040a23",
      "d3a808b7223640a3b18f6e22e6f623bb",
      "2057509e605b44ee8a1f203e01efaf29",
      "ea98c73e8dfa41b4b628ae06db86191e",
      "709cc420a0454d449f54753b971cef1f",
      "d656f07f068e4c61af247d378c0c06d8",
      "4f925c06f1fa48e2bf2956fda13fc89a",
      "3c2dc7c7fd9a4b5bb69254f8e61fdc20",
      "0334d45a4ccc45faad1d16b0f3c660a7",
      "e5f14721349e47b7b9687654727866d3",
      "8f912b17204d47c5b9f4ca0ffd47a351",
      "dc7b2ead1fed4515812890b999d008b8",
      "b74794cebe224896b968a60fd0bcba05",
      "c596e6cc78fa4bfc928133134174f753",
      "7ee9a337d4a143bfb54b4ab8ca19d946",
      "3b72554a308c4c0db7dc5fd3397f10ab",
      "d50cdfc404d746988b59669fbd50ca2c",
      "939cd4e59da9449c9453d632bc236203",
      "cdb1e0b9e7d24945a0e1fccd5b215bf7",
      "d4cfaa0801214d3ea2509f6e377ff37d",
      "6575ff61b07f48eca49a0a76cc57b21d",
      "2094ff8a190b4aa8ac0cb8bdba396bdf",
      "587f67ac4e474f89aa06fae1f85054c4",
      "9b858a3cdd91447c9ceaf16805b8c7ad",
      "edf41e16b2b24a0fbaff14d6a8cd1358",
      "90b196f5357647699f9c05f0f2c34e54",
      "8254f8df51ce4bdb8e5a306624904beb",
      "f9cb919a848340fda38ad0364c8efd0d",
      "c6ec37e1cce14b6499b6e0a0d06fbdca",
      "78e22750547945bf84d0097a4a65af64",
      "55f17de2e2fc483881747457031c8141",
      "0ca197276027490ba8e71efb6ad62fd9",
      "0cb21c220c844262ab27285f05de7734"
     ]
    },
    "executionInfo": {
     "elapsed": 1708229,
     "status": "ok",
     "timestamp": 1706568289260,
     "user": {
      "displayName": "Guto Santos",
      "userId": "11859335695763588451"
     },
     "user_tz": 0
    },
    "id": "d6bbc5a0",
    "outputId": "e0d22313-cc7a-4928-b4dc-f34332cbb8a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning cache from GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#### Defining Data Handler object and processing the dataset\n",
    "preprocessing_setup = {\n",
    "    'lower_case': False,\n",
    "    'remove_emojis': True,\n",
    "    'remove_stop_words': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_users': True,\n",
    "    'remove_urls': True,\n",
    "    'remove_non_text_characters': True,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "\n",
    "data_handler = DataHandler(df=df, text_column=original_text_column, label_column=label_column)\n",
    "\n",
    "data_handler.preprocess(setup=preprocessing_setup)\n",
    "\n",
    "# exploratory_data_analysis = ExploratoryDataAnalysis()\n",
    "# exploratory_data_analysis.plot_text_size_distribution(data_handler.df, data_handler.get_text_column_name())\n",
    "# exploratory_data_analysis.generate_word_cloud(data_handler.df, data_handler.get_text_column_name())\n",
    "\n",
    "data_handler.unsample()\n",
    "\n",
    "# print(data_handler.get_top_words(100))\n",
    "# print(data_handler.get_top_words_tfidf(100))\n",
    "\n",
    "train_data, test_data = data_handler.split_train_test_dataset(train_size=0.8)\n",
    "print('Number of training samples:', train_data.shape[0])\n",
    "print('Number of testing samples:', test_data.shape[0])\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Defining the Languager Model Manager object and creating the model\n",
    "\n",
    "# 'bert-base-uncased' 'roberta-base'\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "language_model_manager = HuggingfaceLanguageModelHandler(model_name=model_name, #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "                                              text_column=data_handler.get_text_column_name(),\n",
    "                                              label_column=data_handler.label_column,\n",
    "                                              new_labels=new_labels)\n",
    "\n",
    "language_model_manager.prepare_training_testing_datasets(train_data, test_data)\n",
    "\n",
    "language_model_manager.create_model()\n",
    "\n",
    "#### Training the model\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",\n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    # these are important:\n",
    "    learning_rate= 5e-6,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=10, # most important\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "results, trainer = language_model_manager.train_evaluate_model(training_args=training_args,\n",
    "                                                               loss_function=nn.CrossEntropyLoss(), # nn.BCELoss()\n",
    "                                                               early_stopping_patience=2,\n",
    "                                                               iterations=1)\n",
    "for metric in results:\n",
    "    print(metric, results[metric])\n",
    "\n",
    "# language_model_manager.save_model(path='saved_models/', name_file=model_name.replace('/','-'))  #local\n",
    "language_model_manager.save_model(path=path+'codes/classification/saved_models/', name_file=model_name.replace('/','-')) # ''' #colab\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ##### Generating the Embeddings\n",
    "# sentences = test_data[data_handler.get_text_column_name()].to_list() #+train_data[data_handler.get_text_column_name()].to_list()\n",
    "# labels = []\n",
    "\n",
    "# for label in test_data[data_handler.label_column].to_list():\n",
    "#     if label == 1:\n",
    "#         labels.append(dataset_type)\n",
    "#     else:\n",
    "#         labels.append('non '+dataset_type)\n",
    "\n",
    "# print('Calculating Embeddings...')\n",
    "\n",
    "# models_to_test = [\n",
    "#                     # 'bert-base-uncased',\n",
    "#                     'roberta-base'\n",
    "#                   ]\n",
    "\n",
    "# tokenizer_list = [] # [language_model_manager.tokenizer]\n",
    "# model_list = [] # [trainer.model]\n",
    "\n",
    "# for model_name_to_load in models_to_test:\n",
    "#     tokenizer, model = language_model_manager.load_model(path=path+'codes/classification/saved_models/',\n",
    "#                                                          name_file=model_name_to_load.replace('/','-'))\n",
    "\n",
    "#     tokenizer_list.append(tokenizer)\n",
    "#     model_list.append(model)\n",
    "\n",
    "# dimention_reduction_algorithm = 'TSNE'\n",
    "\n",
    "# # embeddings = language_model_manager.sentences_to_embedding_standard(sentences=sentences, model_names=models_to_test)\n",
    "\n",
    "# # language_model_manager.plot_embeddings(embeddings_results=embeddings, labels=labels,\n",
    "# #                                        algorithm=dimention_reduction_algorithm, all_together=False)\n",
    "\n",
    "# embeddings = language_model_manager.sentences_to_embedding_fine_tuning(sentences=test_data,\n",
    "#                                                                        model_name_list=models_to_test,\n",
    "#                                                                        tokenizer_list=tokenizer_list,\n",
    "#                                                                        model_list=model_list)\n",
    "\n",
    "# language_model_manager.plot_embeddings(embeddings_results=embeddings, labels=labels,\n",
    "#                                        algorithm=dimention_reduction_algorithm, all_together=False) # '''\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "#### Classifying big unlabled datasets with the trained model\n",
    "\n",
    "# year = '2008'\n",
    "# dataset_path = 'dataset/euros_second/'\n",
    "# dataset_name_file = dataset_path+year+'.csv'\n",
    "# result_file_name = dataset_path+language_model_manager.dataset_type\n",
    "# result_file_name += '/'+year+'_'+language_model_manager.model_name.split('/')[-1]+'.csv'\n",
    "\n",
    "# language_model_manager.classify_unlabaled_datasets(dataset_name_file=dataset_name_file,\n",
    "#                                                    result_file_name=result_file_name,\n",
    "#                                                    batch_size_to_save=100)\n",
    "\n",
    "### Defining the Explainable AI object\n",
    "# trained_model = language_model_manager.trainer.model\n",
    "# trained_tokenizer = language_model_manager.tokenizer\n",
    "trained_tokenizer, trained_model = language_model_manager.load_model(path=path+'codes/classification/saved_models/',\n",
    "                                                         name_file=model_name.replace('/','-'))\n",
    "\n",
    "print('model_max_length:',trained_tokenizer.model_max_length)\n",
    "exp_model = ExplainableTransformerPipeline(model=trained_model,\n",
    "                                           tokenizer=trained_tokenizer,\n",
    "                                           device=language_model_manager.device,\n",
    "                                           pipeline_name='text-classification')\n",
    "\n",
    "\n",
    "####### LIME\n",
    "# results_most_important_words = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "#                                                                               column_text=data_handler.get_text_column_name(),\n",
    "#                                                                               threshold=0, keyword='racism',\n",
    "#                                                                               method='lime', n=100)\n",
    "\n",
    "\n",
    "## Using lime to plot the word importance for few samples\n",
    "# samples = test_data[test_data[label_column]==1].sample(n=3, random_state=42)\n",
    "\n",
    "# for sample in samples[data_handler.get_text_column_name()]:\n",
    "#     print('*** Sample:',sample)\n",
    "#     word_importance_results, graphic_explanation = exp_model.get_most_impactful_words_lime(sample, 'racism', {})\n",
    "#     print(word_importance_results)\n",
    "#     graphic_explanation\n",
    "\n",
    "\n",
    "####### INTEGRATED GRADIENTS\n",
    "\n",
    "# Using integrated gradients to plot the word importance for few samples\n",
    "samples = test_data[test_data[label_column]==0].sample(n=3,random_state=10)\n",
    "\n",
    "print(data_handler.df.iloc[samples.index][data_handler.text_column].values)\n",
    "\n",
    "for sample in samples[data_handler.get_text_column_name()]:\n",
    "# for sample in samples[data_handler.text_column]:\n",
    "    print(sample)\n",
    "    # exp_model.explain(sample)\n",
    "    exp_model.visualize_word_importance_in_sentence(sample)\n",
    "    # exp_model.plot_word_importance(sample, bar=True)\n",
    "\n",
    "\n",
    "# results = exp_model.get_most_impactful_words_for_dataset(dataset=test_data,\n",
    "#                                                column_text=data_handler.get_text_column_name(),\n",
    "#                                                threshold=0.1,\n",
    "#                                                keyword=dataset_type,\n",
    "#                                                method='integrated_gradients',\n",
    "#                                                n=50)\n",
    "# results\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efd365-020c-47f7-b0d2-80927f4751fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T16:45:30.048389Z",
     "start_time": "2023-10-16T16:45:30.035532Z"
    },
    "id": "29efd365-020c-47f7-b0d2-80927f4751fc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Using Pipeline for zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a2b72-1b90-4938-8348-22c46ee681a9",
   "metadata": {
    "id": "a78a2b72-1b90-4938-8348-22c46ee681a9"
   },
   "outputs": [],
   "source": [
    "language_model_manager = LanguageModelHandler(model_name= 'facebook/bart-large-mnli', #'bert-base-uncased', #'cardiffnlp/twitter-roberta-base-offensive'\n",
    "                                              dataset_type=dataset_type,\n",
    "                                              text_column='text', #data_handler.get_text_column_name(),\n",
    "                                              label_column='label' #data_handler.label_column\n",
    "                                             )\n",
    "year = '2016'\n",
    "dataset_path = 'dataset/euros_second/'\n",
    "dataset_name_file = dataset_path+year+'.csv'\n",
    "result_file_name = dataset_path+'zero_shot/'\n",
    "result_file_name += year+'_'+language_model_manager.model_name.split('/')[-1]+'.csv'\n",
    "\n",
    "df_to_classify_zero_shot = pd.read_csv(dataset_name_file)\n",
    "\n",
    "# language_model_manager.zero_shot_classification(sentence='fuck off!', labels=['offensive', 'non-offensive'],\n",
    "#                                                 model_name='facebook/bart-large-mnli')\n",
    "\n",
    "language_model_manager.zero_shot_classification_dataframe(dataframe=df_to_classify_zero_shot,\n",
    "                                                          labels=['neutral', 'racism','homophobia', 'sexism'],\n",
    "                                                          results_file_name=result_file_name,\n",
    "                                                          column_index=7, # you have to check the text column index in your dataset\n",
    "                                                          batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ec6a9a7f",
    "e27710b6",
    "fbd72a0a-7fa5-4ec2-9c74-e151cce982ad",
    "501c4d48-05d8-4dde-aaf0-49a0695288c8",
    "eda99fbb-7baa-47f3-8dcb-a61cf6aadf59",
    "1da9981e",
    "641886a2",
    "199b0f97",
    "29efd365-020c-47f7-b0d2-80927f4751fc"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01191e6924074d50ab31a115d4d00725": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_240c88c8d1c3443d9c5f08276f0520a0",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea08c5b585ac4f95ac6a9d125525c0af",
      "value": 28
     }
    },
    "027f75ad20b941c9a41e014fef8491c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0334d45a4ccc45faad1d16b0f3c660a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "04d06da0ca164b988bb69497a35e8f90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06426f374ada423782f9d533cd9efed8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbf02b8c1d79482c99c0d45af36d53ad",
       "IPY_MODEL_22cba3d483374ce1b1d0df30ee36655b",
       "IPY_MODEL_0c3742de71c646388d2111b90c2ddbf6"
      ],
      "layout": "IPY_MODEL_dac62e5b9b08421283d1d2a9def525e0"
     }
    },
    "0c3742de71c646388d2111b90c2ddbf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86c6d04b972b4ef788bf39f76d61dc3f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2b8ba5dbe5d4426693cd433ad9dbc8eb",
      "value": " 466k/466k [00:00&lt;00:00, 1.03MB/s]"
     }
    },
    "0ca197276027490ba8e71efb6ad62fd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cb21c220c844262ab27285f05de7734": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d5d3906e1e14c8380c4df09fbb517e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bdc2c6a152a4b9babb01e190954d46e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d84f8a5b080a481eb9c428dd58d37963",
       "IPY_MODEL_aed6f9da358346d7aaba26754391b50e",
       "IPY_MODEL_9bb79b9797924b468182b6dd98073c43"
      ],
      "layout": "IPY_MODEL_0d5d3906e1e14c8380c4df09fbb517e3"
     }
    },
    "2057509e605b44ee8a1f203e01efaf29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c2dc7c7fd9a4b5bb69254f8e61fdc20",
      "max": 6646,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0334d45a4ccc45faad1d16b0f3c660a7",
      "value": 6646
     }
    },
    "2094ff8a190b4aa8ac0cb8bdba396bdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22cba3d483374ce1b1d0df30ee36655b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46dd385929c44483a715afd1434b2fc9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc0667791e754667a68bb284ad918c75",
      "value": 466062
     }
    },
    "240c88c8d1c3443d9c5f08276f0520a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "270190e61d9c491d9634477c9562fb46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ef54acb0bb047fcb476c41f8a1febd2",
       "IPY_MODEL_82ef318ca9854893a71c4f4cf694a21e",
       "IPY_MODEL_c4112fc53e204047bbd9b1035f281a67"
      ],
      "layout": "IPY_MODEL_918fdf95c77d4e59a4945e00001e27a2"
     }
    },
    "29e588ccd788493d902ec82b14c29d6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac7fe66be5cc44709cd33731597e047e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_027f75ad20b941c9a41e014fef8491c8",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.56kB/s]"
     }
    },
    "2b8ba5dbe5d4426693cd433ad9dbc8eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b72554a308c4c0db7dc5fd3397f10ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3beddb2505214571906342f86a4f2c31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfb979be7b154f7ba9d86b90a1d3da7a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d5cd74e22aa14bca863d65a8e65a170f",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "3c2dc7c7fd9a4b5bb69254f8e61fdc20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4486c2753bc042f8add62e6770aea5eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46dd385929c44483a715afd1434b2fc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4850905ac5c84579bca8e75d1c523870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4dcf20473bed4fadb9d4cdf2fb769f29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f925c06f1fa48e2bf2956fda13fc89a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55f17de2e2fc483881747457031c8141": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "587f67ac4e474f89aa06fae1f85054c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b858a3cdd91447c9ceaf16805b8c7ad",
       "IPY_MODEL_edf41e16b2b24a0fbaff14d6a8cd1358",
       "IPY_MODEL_90b196f5357647699f9c05f0f2c34e54"
      ],
      "layout": "IPY_MODEL_8254f8df51ce4bdb8e5a306624904beb"
     }
    },
    "6575ff61b07f48eca49a0a76cc57b21d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ef54acb0bb047fcb476c41f8a1febd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afa2336628bc485693b40506beb54b5f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f8777307ee2849428929aeb749ec53a5",
      "value": "config.json: 100%"
     }
    },
    "709cc420a0454d449f54753b971cef1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72a02fe2e6b344489edaca34bb609f91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "760ba7d6ebc544dfabdcee913300f363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78e22750547945bf84d0097a4a65af64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ee9a337d4a143bfb54b4ab8ca19d946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6575ff61b07f48eca49a0a76cc57b21d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2094ff8a190b4aa8ac0cb8bdba396bdf",
      "value": " 1662/1662 [00:02&lt;00:00, 768.59 examples/s]"
     }
    },
    "8254f8df51ce4bdb8e5a306624904beb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82ef318ca9854893a71c4f4cf694a21e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92b0784e08114b028f6535dc22e20bc6",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4850905ac5c84579bca8e75d1c523870",
      "value": 570
     }
    },
    "86c6d04b972b4ef788bf39f76d61dc3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a3072fad0e46c5b2933291963885b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f6b8462a92c46719fca37ea797e1a6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3beddb2505214571906342f86a4f2c31",
       "IPY_MODEL_01191e6924074d50ab31a115d4d00725",
       "IPY_MODEL_29e588ccd788493d902ec82b14c29d6f"
      ],
      "layout": "IPY_MODEL_9702ff03a2aa43ac8de8b142eb10829c"
     }
    },
    "8f912b17204d47c5b9f4ca0ffd47a351": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "902133b09008442d83000637e5e48483": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90b196f5357647699f9c05f0f2c34e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ca197276027490ba8e71efb6ad62fd9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0cb21c220c844262ab27285f05de7734",
      "value": " 440M/440M [00:02&lt;00:00, 119MB/s]"
     }
    },
    "918fdf95c77d4e59a4945e00001e27a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92b0784e08114b028f6535dc22e20bc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "939cd4e59da9449c9453d632bc236203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9702ff03a2aa43ac8de8b142eb10829c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b858a3cdd91447c9ceaf16805b8c7ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9cb919a848340fda38ad0364c8efd0d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c6ec37e1cce14b6499b6e0a0d06fbdca",
      "value": "model.safetensors: 100%"
     }
    },
    "9bb79b9797924b468182b6dd98073c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dcf20473bed4fadb9d4cdf2fb769f29",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_04d06da0ca164b988bb69497a35e8f90",
      "value": " 232k/232k [00:00&lt;00:00, 11.0MB/s]"
     }
    },
    "aa99317f99f1435784239a844ba6997a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac7fe66be5cc44709cd33731597e047e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aed6f9da358346d7aaba26754391b50e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_902133b09008442d83000637e5e48483",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_72a02fe2e6b344489edaca34bb609f91",
      "value": 231508
     }
    },
    "afa2336628bc485693b40506beb54b5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b74794cebe224896b968a60fd0bcba05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d50cdfc404d746988b59669fbd50ca2c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_939cd4e59da9449c9453d632bc236203",
      "value": "Map: 100%"
     }
    },
    "bba64555be7140ea83f06291377a00e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbf02b8c1d79482c99c0d45af36d53ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa99317f99f1435784239a844ba6997a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bba64555be7140ea83f06291377a00e3",
      "value": "tokenizer.json: 100%"
     }
    },
    "c4112fc53e204047bbd9b1035f281a67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc7e99709ccb4000a5dfc8b2e84d02e8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_88a3072fad0e46c5b2933291963885b3",
      "value": " 570/570 [00:00&lt;00:00, 40.2kB/s]"
     }
    },
    "c596e6cc78fa4bfc928133134174f753": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdb1e0b9e7d24945a0e1fccd5b215bf7",
      "max": 1662,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4cfaa0801214d3ea2509f6e377ff37d",
      "value": 1662
     }
    },
    "c6ec37e1cce14b6499b6e0a0d06fbdca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc0667791e754667a68bb284ad918c75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc7e99709ccb4000a5dfc8b2e84d02e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdb1e0b9e7d24945a0e1fccd5b215bf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3a808b7223640a3b18f6e22e6f623bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d656f07f068e4c61af247d378c0c06d8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4f925c06f1fa48e2bf2956fda13fc89a",
      "value": "Map: 100%"
     }
    },
    "d4cfaa0801214d3ea2509f6e377ff37d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d50cdfc404d746988b59669fbd50ca2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5cd74e22aa14bca863d65a8e65a170f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d656f07f068e4c61af247d378c0c06d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d84f8a5b080a481eb9c428dd58d37963": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_760ba7d6ebc544dfabdcee913300f363",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4486c2753bc042f8add62e6770aea5eb",
      "value": "vocab.txt: 100%"
     }
    },
    "dac62e5b9b08421283d1d2a9def525e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc7b2ead1fed4515812890b999d008b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b74794cebe224896b968a60fd0bcba05",
       "IPY_MODEL_c596e6cc78fa4bfc928133134174f753",
       "IPY_MODEL_7ee9a337d4a143bfb54b4ab8ca19d946"
      ],
      "layout": "IPY_MODEL_3b72554a308c4c0db7dc5fd3397f10ab"
     }
    },
    "dfb979be7b154f7ba9d86b90a1d3da7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5f14721349e47b7b9687654727866d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea08c5b585ac4f95ac6a9d125525c0af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ea98c73e8dfa41b4b628ae06db86191e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5f14721349e47b7b9687654727866d3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8f912b17204d47c5b9f4ca0ffd47a351",
      "value": " 6646/6646 [00:07&lt;00:00, 1222.86 examples/s]"
     }
    },
    "edf41e16b2b24a0fbaff14d6a8cd1358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78e22750547945bf84d0097a4a65af64",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55f17de2e2fc483881747457031c8141",
      "value": 440449768
     }
    },
    "f8777307ee2849428929aeb749ec53a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9cb919a848340fda38ad0364c8efd0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd7ce92728054cfca73e0ef8e5040a23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d3a808b7223640a3b18f6e22e6f623bb",
       "IPY_MODEL_2057509e605b44ee8a1f203e01efaf29",
       "IPY_MODEL_ea98c73e8dfa41b4b628ae06db86191e"
      ],
      "layout": "IPY_MODEL_709cc420a0454d449f54753b971cef1f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
